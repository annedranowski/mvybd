%!TEX program = lualatex

\documentclass[draft]{article}
\usepackage{basic}
\usepackage{emoji}

% \setemojifont{Apple Color Emoji}
% Working title: 
\title{How to compute the fusion product of MV cycles in type A}
\author{Roger Bai, Anne Dranowski, Joel Kamnitzer} 
% \date{October 2020}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
% 
% The Mirkovi\'c--Vybornov isomorphism
In \cite{mirkovic2007quiver} (see also the recent sequel \cite{mirkovic2019comparison}) Mirkovi\'c and Vybornov provide a geometric version of symmetric and skew Howe $(\GL_m,\GL_n)$ dualities by relating Kazhdan--Lusztig slices in the affine Grassmannian of $\GL_m$ on the one hand to slices in $n\times n$ nilpotent orbits 
% conjugacy classes of nilpotent matrices 
and on the other hand to Nakajima quiver varieties. 
This article generalizes the first of these relations (an isomorphism) to families by way of the Beilinson--Drinfeld Grassmannian. Essential to \cite{mirkovic2007geometric}, this space is used to define a fusion (aka convolution) product on sheaves (representations) but can also be used to define a fusion product of MV cycles (bases). 
% 
This product structure makes the vector space spanned by the MV cycles into a commutative algebra isomorphic to the ring of functions on the unipotent subgroup (conjectured in \cite{anderson2003polytope} and proved in \cite{baumann2019mirkovic}). % conjectured by JEA and proved by BKK
% 
To quote from Mirkovic and Vilonen:
\begin{quotation}
    However, we make crucial use of an idea of Drinfeld, going back to around 1990. He discovered an elegant way of obtaining the commutativity constraint by interpreting the convolution product of sheaves as a ``fusion'' product.
\end{quotation}
% 
[Liaison]

The purpose of this paper is to give a conceptually elementary way to compute this product in type $A$. We do not know how fruitful this work might be in the larger landscape of geometric representation theory as it is plagued by two major limitations: it does not readily generalize outside of type $A$, and it is as yet computationally expensive. 
% 
Moreover, it is not clear why one should care about the fusion product on bases. \emoji{imp} \emoji{grin} \emoji{worried} \emoji{monocle-face} \emoji{skull}

\section{Players}

\subsection{Rings and discs}

Set $\cO = \cO_0 = \CC\xt$ and $\cK = \cK_0 = \CC\xT$.
% TODO: I rearranged this sentence, because starting it with 'for any other' presupposes we have introduced s 
%  For any other 
We will also consider $\cO_s = \CC\xt[t-s]$ and its fraction field $\cK_s=\CC\xT[t-s]$, for any 
$ s \in \CC\setminus\{0\} $, as well as $ \Oinf = \CC\xt[t^{-1}] $ and $\Kinf = \CC\xT[t^{-1}]$. For any $ s \in \PP = \PP^1$, 
% TODO: Identifying $s\in\CC$ and the line $s\in\PP$ containing it right?
$ \cO_s$ is the completion of the local ring $ \mathcal O_{\PP, s} $ and thus the formal spectrum of $ \cO_s$ is the formal neighbourhood $ D_s$ of $ s $ (also called the formal disc centered at $ s$).  Similarly the formal spectrum of the field $\cK_s$ is the deleted formal neighbourhood (or punctured disc), denoted $ D_s^\times$.

Note that for $ s \in \CC $, we have an obvious isomorphisms  $\cO_s \cong \cO$ and $\cK_s\cong\cK$ taking $ t-s $ to $ t$. 

\subsection{Groups}
Let $H $ be an algebraic group over $ \CC $.  We will be interested in $ H(R)$ where $ R $ is a $\CC$-algebra, for example $R = \CC[t], \cO_s$, etc.  
\acom{random question, I know $H(R)$ is supposed to mean functor of points of the scheme $H$ but can it also be viewed as a base change from $\CC$ to $\RR$?}
Note that evaluation at $ t = s$ provides a group homomorphism $ H(\cO_s) \rightarrow H$.  We denote the kernel of this map by $ H_1(\cO_s)$.  We will be particularly interested in this construction in the case $ s = \infty$, which gives us the group $ H_1(\Oinf)$ which is often called the first congruence subgroup.
% \acom{Add $G_1$}
%\acom{TODO: Algebraic groups over rings, i.e.\ the meaning of $G(K)$. The affine space of $m\times m$ matrices which we denote $ M_m$. The first congruence subgroup $G_1$ and why we take $\Oinf$ points instead of $\CC[t]$ points as some of the lit does.}

Fix $G = \GL_m$ and let $T\subset G$ be the maximal torus of diagonal matrices. 
% are there other possibilities?? :S 
Write $P$ for the coweight lattice of $T$, so we have $P\cong \ZZ^m$.  
% TODO: Thank you - resolved!
% \acom{This identification assumes we are taking $\CC$ points of $G$ right? i.e.\ it's really $P(T(\CC)) = \Hom(\CC^\times,T(\CC))$ that's $\cong \ZZ^m$ and for instance $P(T(\CC[x]/(x^2))$ will give sth else?} 
% \jcom{No, the coweight lattice is the set of homomorphisms of algebraic groups from $ \mathbb G_m$ to $ GL_m$.  It is independent of the ring/field of definition.}

Under this isomorphism the dominant coweights of $ G $ 
% is
are identified with the set of $ \lambda = (\lambda_1, \dots, \lambda_m) $ such that $ \lambda_1 \ge \cdots \ge \lambda_m$. 
We also need the notion of effective dominant coweight, 
% \acom{can we conflate or should we be precise and say coweight?} \jcom{Corrected, let's stick to coweights.}
which means those $ \lambda $ as above with $ \lambda_m \ge 0$.  So an effective dominant coweight $ \lambda $ is a partition of $|\lambda| = \lambda_1 + \cdots + \lambda_m$, which we call the size of $ \lambda$. We write $P_+$ for the set of dominant coweights, and $P_{++}$ for the set of effective dominant coweights. 

% nicematrix test
% $\begin{bNiceArray}{ccc|c}[margin]
%     \Block{3-3}<\Large>{A} & & & 0 \\
%     & \hspace*{1cm} & & \Vdots \\
%     & & & 0 \\
%     \hline
%     0 & \Cdots& 0 & 0
%     \end{bNiceArray}$
% 
% $\begin{pNiceMatrix}
%     \frac12 & -\frac12 \\
%     \frac13 & \frac14 \\
%     \end{pNiceMatrix}$

% Set $\AA = \AA^1_x$ (the subscript denoting the chosen local coordinate) and fix $s\in \AA - \{0\}$. 


Given a coweight $\mu \in P$ and a point $ s\in \CC$, we define $ (t-s)^\mu$ to be the diagonal matrix 
% with entries $ (t-s)^{\mu_1}, \dots, (t-s)^{\mu_m}$
% \[
%     \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
%         (t-s)^{\mu_1} & 0             & \Cdots & 0 \\
%         0             & (t-s)^{\mu_2} & \Ddots & \Vdots \\
%         % 0 & b & a & \Ddots & & \\
%         \Vdots              & \Ddots        & \Ddots & 0 \\
%         % \Vdots & & & & \\
%         0             & \Cdots        &   0    & (t-s)^{\mu_m}
%     \end{pNiceMatrix}
% \]
\[
\begin{bmatrix}
    (t-s)^{\mu_1} \\
    & (t-s)^{\mu_2} \\ 
    & & \ddots \\
    & & & (t-s)^{\mu_m}
\end{bmatrix} 
\]
which we will view in $\GL_m(K) $ for any ring $ K $ containing $\CC[t]$ in which $t-s$ is invertible (for example $ K = \Ks, \CC(t)$). % A: Think R sb K; changed it 
\acom{1. Add warning that we will also write $(t-s)^\mu$ for its image in $\GL_m(K)/\GL_m(R)$. We hope it will be evident? 2. Change notation $\mu = (\mu_1,\dots,\mu_m)$ since we also have $\mu = \mu_1 + \mu_2$. We could do $\mu = \mu' + \mu''$ or $\mu = a + b$? The notation $\mu^{(i)}$ is already taken to mean sth. Ofc all can be changed. Can also do $\mu = {}_{1}\mu + {}_{2}\mu$. \emoji{sweat-smile}}


We will also be interested in the affine space $ M_m$ of $m\times m$ matrices.  Note that for any $ \CC$-algebra $ R $, $\GL_m(R) $ consists of those matrices $ g \in M_m(R) $ whose determinant is invertible in $ R$. Thus $ t^\lambda \in M_m(\CC[t])$ for all $ \lambda \in P_{++}$ but $ t^\lambda \in \GL_m(\CC[t]) $ if and only if $ \lambda = 0 $.

\subsection{Lattices}
\label{ss:lat}
We will use the lattice model \acom{for...?}, so it is useful to recall the following definition. Let $ R \subset K$ be two $\CC$-algebras (usually, but not always, $K$ will be a field). 
% \acom{When is $K$ not a field?}). Let $ m \in \mathbb N$ \acom{we have already fixed $m$?} and; e.g. K = C[t,t^{-1}] is not a field!!! 
Consider $ K^m $ as a $K$-module. By restriction $ K^m$ can be viewed as an $R$-module.  An $R$-lattice in $K^m$ 
% \acom{what is $M$? should be $K^m$?}; bc Joel was thinking about arbitrary free modules 
is an $R$-submodule $ L \subset K^m$ such that $ L $ is a free rank $ m $ $R$-module and $ L \otimes_R K = K^m $.  Equivalently, $ L = \Sp_R(v_1, \dots, v_m)$ where $v_1, \dots, v_m$ are free generators of $K^m$. 
% 
\begin{comment}
\jcom{Maybe need that both $K$ and $R$ are integral domains and have the same field of fractions?} 
\acom{Roger has proved that what is written is ok.}
\rcom{Here's my argument: If $L \subset K^m$ is a free rank $m$ $R$-module and $L \otimes K = K^m$, then it's clear we that $L$ is generated by $m$ generators $v_1,\dots,v_m$ over $R$. Since $L\otimes K = K^m$, these also generate $K^m$ over $K$. Then we have a surjective map $K^m \twoheadrightarrow K^m$ given by sending a basis of $K^m$ to the generators $v_1,\dots,v_m$. Since any surjective endomorphism of finite $K$-modules is injective, the $v_1,\dots,v_m$ form a basis. I think the only thing that is needed is that the rings are commutative.}
\jcom{Great.}
\end{comment} 

% The trivial lattice is 
% No more: L_0(R) = 
$ R^m \subset K^m $ is called the standard lattice. 
% We will abuse notation and denote $L_0(R)$ simply $L_0$ in the hope that the ring $R$ will be understood from the context. 
The group $\GL_m(K) $ acts transitively on the set of $R$-lattices in $K^m$, thus giving a bijection between 
% the set of such lattices 
this set and $\GL_m(K)/\GL_m(R)$, since $ GL_m(R) $ is the stabilizer of the standard lattice. 
%\acom{For example, we will frequently make use of the lattice $L_\lambda$ corresponding to the point $t^\lambda\in\Gr$ given $\lambda\in P$. If $e_1,\dots,e_m$ denote the standard basis elements of 
% $L_0(\cO)$ then 
%$\cO^m$ then 
%$$ 
%L_\lambda = \Sp_\cO(t^{\lambda_1}e_1, \dots, t^{\lambda_m}e_m)\,. 
%$$}


We will be particularly interested in $\CC[t]$-lattices in $ \CC(t)^m$.  Given such a lattice $ L $ and a point $ a \in \CC$, the specialization of $ L $ at $ a $ is defined as
$$
% L(a) := L \otimes_{\CC[t]} \CC\xt[t-a] \subset \CC\xT[t-a]^m\,.
L(a) := L \otimes_{\CC[t]} \cO_a, \text{ a lattice in } \cK_a.  %\,. % The containment is not part of the definition so I comment it out \subset \cK_a^m\,.
$$
If 
% $ L(a) = L_0(\cO_a)$ 
$L(a) = \cO_a^m$ then $L$ is said to be trivial at $a$. 
% We say that $ L $ is trivial at $ a $ if $ L(a) = \cO_a^m$. 
% \acom{Do we want to employ the notation $\cO_a$ and $\cK_a$ here?}
For example, the lattice $(t-s)^{-1}\CC[t] \subset \CC(t)$ is trivial at any $a\ne s$. 
% \acom{it is a little bit uneven to have notation for $\cO^m$ but not for $\cO_a^m$?}

% \acom{Should we also mention our interest in $\cO$-lattices in $\cK^m$?}
% \jcom{I don't think that it is necessary.  I just put the part about $ \CC(t)$ to set up the notation for this specialization.}


% \jcom{I wasn't sure of the notation to use here, but I wanted something which didn't conflict with $ L_0$.}

% \acom{I think that MVy use this notation also: }

%\includegraphics[width=\textwidth]{img/Capture d’écran, le 2021-02-17 à 13.03.10.png}}

\subsection{Affine Grassmannians} %  actors
% To map thick Gr to C is to map a lattice to the points where it isn't trivialzable; doesn't make sense to ask for poles 
% G(C(t))/G(C[t]) for example lives over the Ran space, finitely many eigenvalues; positive part of thick contains this guy as image 
Set $\AA = \AA^1$ and let $\Vtriv$ denote the trivial rank $ m $ vector bundle. 

We now define various version of the affine Grassmannian which will play an important role in this paper.

\begin{definition}
     The \textbf{ordinary affine Grassmannian} $\Gr = G(\cK)/G(\cO)$.
\end{definition}    
    It is described in modular terms by 
    $$
     \Gr = 
    \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_0 $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_0^\times $} \right\} \,. 
    $$

    We also have a lattice description 
    $$ 
    \Gr = \left\{ L \subset \cK^m : \text{ $L$ is a $\cO$-lattice} \right\}\,.
    $$
    We obtain the lattice from $ (V,\varphi) $ by setting $ L = \Gamma(D_0, V)$ which is embedded into $ \cK^m = \Gamma(D_0^\times, \Vtriv)$ using $ \varphi$.  On the other hand, we obtain the lattice from the group-theoretic description $ G(\cK) / G(\cO) $ by setting $ L = g \cO^m$ for $ g \in G(\cK)$.
    % \emoji{clap} 
    % \acom{Change $V_0$}
    % 
\begin{definition}
 For any $ s \in \CC $, 
    % \acom{should it be $\AA$ instead of $\CC$} 
    the \textbf{ordinary affine Grassmannian} $\Gr_s = G(\cK_s)/G(\cO_s)$ at $ s $. 
\end{definition}    
As above, we have modular and lattice descriptions
\begin{gather*}
    \Gr_s = 
    \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_s $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D^\times_s $} \right\}\,. \\
    \Gr_s = \left\{ L \subset \cK_s^m : \text{ $L$ is a $\cO_s$-lattice} \right\} \,. 
\end{gather*}

\begin{definition}
The \textbf{thick affine Grassmannian} $\Grth = G(\Kinf)/G(\CC[t])$.
\end{definition}
    Again, we have the following modular and lattice descriptions
    \begin{gather*}
         \Grth = 
         \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ \PP $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_\infty $ } \right\}\,. \\
    \Grth = \left\{ L \subset  \Kinf^m : \text{ $L$ is a $\CC[t]$-lattice} \right\}\,.
     % \CC\xT[t^{-1}]^m 
    \end{gather*}
    % \jcom{Maybe write $\Kinf$ for $ \CC\xT[t^{-1}] $?  Also maybe change the ``thick'' notation''.} \acom{Since $\sf th$ is also the first two letters of ``thin''?} \jcom{That's a good point!  Maybe we could use some kind of bold  Gr (``thick'' letters) or maybe that would be too confusing.} \acom{Ok, it's tentative.}

    \begin{definition} 
        The (two point) \textbf{Beilinson--Drinfeld Grassmannian} $\pi : \Grbd\to \AA$ (with one point fixed at 0, and the second point $s\in\AA$ varying).
    \end{definition}
    It is described in modular terms by 
    % {\small
    $$
     \Grbd = 
    % \begin{aligned}
        \left\{ 
            (V,\varphi,s) : V\text{ is a rank $m$ vector bundle on }\PP, \varphi : V \xrightarrow{\sim} \Vtriv \text{ on } \PP \setminus \{0, s\}  
        \right\} 
    % \end{aligned}
    $$%}
    % 
    The fibre of $\Grbd \to \AA$ over $ s \in \AA $ will be denoted $ \Gr_{0,s} $ and is given by
    % \rcom{why do we have the $s\ne 0$ restriction here} \jcom{it would work when $ s = 0$, but it looks a bit strange there.}
    % 
    $$ 
    % \Gr_{0,s} = 
    G(\CC[t, t^{-1}, (t-s)^{-1}]/G(\CC[t])\,. % := \pi^{-1}(s) 
    $$
    % \acom{0 in notation is kinda redundant? But we can't take it away because we already have a $\Gr_s$ and it is something else?}
    % \jcom{Yes, that is one reason.  But also, there was a comment of yours before in the file, saying that you sometimes got confused that one point was fixed at 0, so this is a good notation to remind us (and the reader).  For the same reason, maybe we should write $ \Gr_{0, \AA}$ instead of $ \Gr_\AA$.}
    % \acom{lol}
    We also have the lattice descriptions 
    % {\small
    \begin{gather*}
    \Grbd = 
    \{ (L,s) :
    % s\in\AA,\,
    % L \subset  \CC(t)^m, s \in \AA 
    \text{$L \subset  \CC(t)^m$ is a $\CC[t]$-lattice trivial at any $ a \ne 0, s$} \}\,, \\
    % , $L$ is 
     \Gr_{0,s} = 
    \{ L \subset  \CC[t,t^{-1},(t-s)^{-1}]^m : \text{ $L$ is a $\CC[t]$-lattice} \}\,.
    \end{gather*}%}
    % \acom{We do not consistently specify where $s$ is in these definitions}
    % \acom{$s\in\CC$ should be $s\in\AA$?}
    % \jcom{Sorry I was very consistent about $ \CC$ vs $\AA$.  Which do you prefer?}
    % 
    \begin{definition} 
    The \textbf{positive parts} of $\Gr $ and $\Grth$ are defined by 
    $$
        \Gr^+ = \left(M_m(\cO) \cap G(\cK)\right) / G(\cO) \quad        
        {\Grth}^+ = \left(M_m(\CC[t]) \cap G(\Kinf)\right) / G(\CC[t])\,. 
    $$
    \end{definition}
    In modular terms, $\Gr^+$ (resp. ${\Grth}^+$) is the set of those $ (V, \varphi)$ where $ \varphi : V \rightarrow \Vtriv $ extends to an inclusion of coherent sheaves over $ D_0 $ (resp. over $ \PP$).
    
    In lattice terms, $ \Gr^+$ (resp. ${\Grth}^+$) contains those lattices $L$ which are contained in the relevant standard lattice. 
    % \acom{previously called ``trivial''} \jcom{I changed it so that $ R^m $ is called the standard lattice.  But I would like to leave the terminology ``trivial at $ a$'' is that ok, or should we write ``standard at $a$''.} 
    % \acom{Oh, I don't know.. is trivialized at $a$? is standardized at $a$? Maybe standard at $a$ is better.. Man I never thought about this interchangeable lingo.} 
    % lattice $ L_0(\cO)$ (resp. $L_0(\CC[t])$). % , i.e. $ L \subseteq L_0$.

% \acom{enumerate the points for now}
\subsection{Relations between different affine Grassmannians}
These different versions of the affine Grassmannian are related as follows.  

\begin{enumerate}
    \item There is an isomorphism $ \Gr_s \cong \Gr $ coming from the isomorphism $ \Ks \cong \cK$.
    \item We have a map from the two point Beilinson--Drinfeld affine Grassmannian to the thick affine Grassmannian $ \Grbd \rightarrow \Grth $
    % $$ \Grbd \rightarrow \Grth $$ 
    which is given in modular terms by restricting the trivialization to $ D_\infty$.  It is given in group-theoretic terms on the fibre over $ s $ by the inclusion
    $$
    G(\CC[t, t^{-1}, (t-s)^{-1}])/ G(\CC[t]) \rightarrow G(\Kinf)/G(\CC[t])\,. 
    $$
    % \acom{Permission to update notation and replace $\CC\xT[t^{-1}]$ by $\Kinf$ as per your suggestion Joel?}
    % \jcom{Permission granted!}
    % \acom{replaced}
    
    Finally, it is given in lattice terms as the identity on the lattice and using the inclusion $\CC[t, t^{-1}, (t-s)^{-1}]^m \rightarrow \Kinf^m$ on the ambient spaces.
    \item The fibres $ \Gr_{0,s}$ of $ \Grbd \rightarrow \AA$ can be described as % are given as follows
    $$
    \Gr_{0,s} \cong 
    \begin{cases} 
        \Gr \times \Gr_s & s \ne 0 \\
        \Gr              & s = 0\,.
    \end{cases}
    $$
    In the $s\ne 0$ case, the isomorphism is given in the modular realization by restricting the vector bundle and trivializing to the appropriate discs.  
    In the lattice realization, it is given by forming tensor products 
    $$
    L \mapsto (L(0), L(s))\,.
    $$
    % in the $ s \ne 0 $ case.
    % \jcom{Maybe we should introduce some notation for this isomorphism.}
    % \acom{Ok. Question: how is the isomorphism given at $s=0$?}
    % \acom{Ok. TODO. Also write this in group theoretic terms. Probably $[g] \mapsto ([g],[g])$ is ok. Since $g\CC[t]\otimes_{\CC[t]}\cO_s \cong g\cO_s $ for some reason, where $g\in\CC[t,t^{-1},(t-s)^{-1}]$ can be viewed as $g\in\cK_s$.}
    % 
    This is compatible with the lattice description since for $ g \in G(\CC[t, t^{-1}, (t-s)^{-1}]) $, we have
    $$
    \left( g \CC[t]^m \right) \otimes_{\CC[t]} \cO = g \cO^m
    $$
    and similarly for $ L(s)$.
    
    In the $ s = 0 $ case, the isomorphism is described in the same way, except that we just need to form $ L(0)$.
    
    
%    \jcom{I tried to answer your question, but didn't explain it well.  When $ s = 0$, the map is the same except that $ L(0) = L(s) $ , so we only need $L(0)$.}
\end{enumerate}

\subsection{The fusion construction}
The following construction will be very important in this paper.
First, we recall that we have an isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $, 
for each $ s \ne 0$.  In turn, we have the isomorphism $ \Gr_s \cong \Gr$.  

Gluing these together, we obtain
$$
\tau : \Gr_{0, \CC^\times} \rightarrow \Gr \times \Gr_{\CC^\times} \rightarrow \Gr \times \Gr
$$
where the first morphism is an isomorphism and the second map uses $ \Gr_s \cong \Gr $ on each fibre.  (This $\tau$ matches the map $ \tau $ of \cite{mirkovic2007geometric}, except that we are working over $ \{0 \} \times \CC^\times$ instead of the complement of the diagonal in $ \CC^2 $.)

Let $ X_1, X_2 \subset \Gr$ be two subschemes.
We define the subscheme $ \tau^{-1}(X_1 \times X_2) \subset \Gr_{0, \CC^\times} $.

\jcom{I changed this a bit in order to match Mirkovic--Vilonen better.}
\acom{We say ``we define the subscheme..'' but what is there to define? Should it say ``we consider'' maybe?}
% Via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $,  for each $ s \ne 0$, we obtain a subscheme $ X_1 \times X_2 \times \AA^\times $  of $\Grbd$.  

We define $ X_1 \ast_{\AA} X_2 $ to be the scheme-theoretic closure of $  \tau^{-1}(X_1 \times X_2) $ inside of $ \Grbd $.  
Next, we consider the projection $ \pi: \Grbd \rightarrow \AA $, and define 
% $ X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) \subset \Gr_{0,0} $, 
\begin{equation}
    \label{deservesdisplay}
    X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) 
\end{equation}
which we call the fusion of $ X_1 $ and $X_2$. It is contained in $\Gr_{0,0}$ but regarded as a subscheme of $ \Gr $ using {the isomorphism} $\Gr_{0,0} \cong \Gr $. 

By this construction, \acom{technically, there were two given above, so *these* constructions?} $X_1 \ast_{\AA} X_2 $ is a flat family over $\CC$ whose fibre away from $0$ is $X_1 \times X_2$ and whose fibre over $ 0$ is $ X_1 \circ X_2$.

% \acom{Repeat question: what is this iso? Roger says maybe it's in MV?}
% \jcom{Now answered above.}

%\jcom{I thought that it would be good to introduce this operation now, since we will use it often in the paper and it is the most convenient way to define $\Gr^{\lambda_1, \lambda_2}$. I don't know what notation we should use.}

%\acom{Roger and I have been using $\ast$ to denote fusion. Do we need a notation for the scheme-theoretic closure of $X_1\times X_2\times\AA^\times$? If not then I like $X_1\ast X_2 = \overline{X_1 \times X_2 \times \AA^\times}\cap \pi^{-1}(0)$.}
%\jcom{I think that we will need notations for both things, but I'm happy to use $ \ast$ for the central fibre and use something else for the whole family.} \acom{Ok, I have changed it to notation from Roger's thesis/AK. Only now I am confused as to what is being called fusion---why do we not take the top dimensional subscheme?}
%\jcom{I guess that there are two things that one could mean by fusion of MV cycles.  This scheme $ X_1 \circ X_2$ or the expression which appear in Theorem 7.11 of mvbasis, which is a linear combination of the top-dimensional components of this scheme with multiplicites.  In the general context of this section ($X_1, X_2$ are not necessarily MV cycles), then I guess it is best to talk about the subscheme.  Maybe we can call the expression appearing Theorem 7.11 the ``numerical fusion'' of MV cycles.  By the way, somewhere in this paper we should explain Theorem 7.11 and say that this gives us motivation to study fusion of MV cycles.}
%\acom{Sure! It is quoted at the end, in \Cref{s:denouement} as an application. Addendum: for the numerical fusion I suggest simply $[Z_1][Z_2]$. This is what I will use in my talk. }

\subsection{Some subvarieties of affine Grassmannians}
Now fix effective dominant coweights $\lambda', \lambda'' \in P_{++}$ of sizes $N',N''$  and their sums $ \lambda = \lambda' + \lambda'' $ of size $ N = N'+N''$. 
% Coweights define elements in the various affine Grassmannians which we consider. Warning: We will write only coset representatives for elements of the various Grassmannians; we hope that it is evident which right cosets they represent. 
% the right cosets which they represent are clear from the context.
We consider the following subschemes of the affine Grassmannians.
% 
\begin{definition}
The \textbf{spherical Schubert cell} $\Gr^\lambda = G(\cO) t^\lambda$ and its closure 
    $ \overline\Gr^\lambda = \bigcup_{\lambda' \le \lambda} \Gr^{\lambda'} $.  
\end{definition}
    % \acom{do we prefer $\overline{\Gr}^\lambda$ to $\overline{\Gr^\lambda}$? I am for the former! Though it is less accurate??} \jcom{I'm not sure which is better.  I was ``testing'' it out.}
    Since $ \lambda $ effective, $ t^\lambda \in \Gr^+ $ and so the spherical Schubert variety $\overline{\Gr}^\lambda $ lies inside the positive part $ \Gr^+$ of the affine Grassmannian.
    % 
\begin{definition} 
    The \textbf{fusion of two spherical Schubert varieties} $$ \overline{\Gr}^{\lambda', \lambda''}_{0, \AA} = \overline{\Gr}^{\lambda'} \ast_\AA \overline{\Gr}^{\lambda''} \rightarrow \AA $$
\end{definition}
    % \acom{I guess that it is good to have the notation here, in place of $\overline{\overline\Gr^{\lambda'}\times\overline\Gr^{\lambda''}\times\AA^\times}$..}
    %\acom{should we also subsript this family with $0,\AA$?}
    By construction the fibre $ \overline{\Gr}_{0,s}^{\lambda', \lambda''} $ over $ s \ne 0 $ is $ \overline{\Gr}^{\lambda'} \times \overline{\Gr}^{\lambda''}$.  
    % By \cite[Proposition 3.1.14]{zhu2016introduction},
    % Zhu's theorem (\jcom{need ref} \acom{pretty sure it's }), 
    % the fibre over $ 0 $ is $ \overline{\Gr}^{\lambda' + \lambda''}$.
    Over $s=0$ the fibre is $ \overline{\Gr}^{\lambda' + \lambda''}$ (and reduced) by 
    % a theorem of Zhu 
    \cite[Proposition 3.1.14]{zhu2016introduction}. 

    Suppose that $ s \ne 0$.  In the fibre $\overline{\Gr}_{0,s}^{\lambda', \lambda''}$ we have the open locus $ \Gr_{0,s}^{\lambda', \lambda''}$ coming from transporting $ \Gr^{\lambda'} \times \Gr^{\lambda''} $ via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $.  This open locus is a $ G(\CC[t])$-orbit
    % .  The lattice description of $ \Gr_{0,s}^{\lambda', \lambda''}$ 
    whose lattice description is given in \Cref{le:Grl1l2} below.
    % \acom{Why $G(\CC[t])$. Because we are on the left side!}
    
As above, since $ \lambda', \lambda'' $ are effective, the map $ \Gr_{0, \AA} \rightarrow \Grth$ restricts to a map $ \Gr^{\lambda', \lambda''}_{0, \AA} \rightarrow \Grth^+$.

Now fix two effective coweights $ \mu', \mu'' \in \mathbb N^m$ such that $ \mu_i$ has size $ N_i$ and such that $ \mu = \mu' + \mu'' $ is dominant.  (We do not require $ \mu', \mu'' $ to be effective.) Using $ \mu, \mu', \mu''$, we define the following subschemes.

\begin{definition}
    The \textbf{Kazhdan--Lusztig slice} $\cW_\mu = G_1(\Oinf) t^\mu \subset \Grth $.  
\end{definition}
Since $\mu $ is effective, we see that $ \cW_\mu $ lies inside the positive thick Grassmannian $ \Grth^+$.
    % 
    
    In modular terms, this corresponds to the locus of those $ (V, \varphi)$ such that $ V $ is isomorphic to the trivial vector bundle on $ \PP$ and such that $ \varphi$ preserves the {Harder--Narasimhan filtration of $V$ at $ \infty$}.  The lattice description of $ \cW_\mu $ is given in Lemma \ref{le:Wmu} below. 
    % \acom{que est-ce?} \jcom{It's not very relevant for this paper.  Here is an explanation: every vector bundle on $\PP$ (or maybe any curve) has a special filtration called the HN--filtration; it is a partial flag of subbundles of type determined by $ \mu$.  We require that $ \varphi$ takes this filtration to the standard partial flag at the fibre over $ \infty$.  Maybe ``preserves'' is not the best word above.}
    % A: Ok thanks!
    % 
\begin{definition} The \textbf{semi-infinite orbit} 
$$ S^\mu = N_-(\cK)t^\mu \subset \Gr $$ 
and the \textbf{fusion of  semi-infinite orbits} 
$$ S^{\mu', \mu''}_{0,\AA} := S^{\mu'} \ast_\AA S^{\mu''} \,.$$  
\end{definition}
If $ s \ne 0 $, then the fibre $S^{\mu', \mu''}_{0,s} $ is given by
    $$
    % S^{\mu', \mu''}_{0,s} = 
    N_-(\CC[t, t^{-1}, (t-s)^{-1}])t^{\mu'} (t-s)^{\mu''} G(\CC[t])\subset G(\CC[t,t^{-1}, (t-s)^{-1}]) / G(\CC[t]) \,. % \G(\CC[t])
    $$
    % TODO: resolve
    % \jcom{Maybe we should introduce the $ L_{\mu', \mu''} $ notation, but I'm not sure it is necessary.}
    On the other hand, if $ s = 0$, then the fibre $S^{\mu', \mu''}_{0,0} $ equals $ S^{\mu' + \mu''}$. 
    % TODO: resolve
    % \acom{changed $\mu$ to $\mu' + \mu''$ since we wrote it out for $\overline{\Gr}_{0,\AA}^{\lambda', \lambda''} $}
    
    \jcom{Still need a reference for why this central fibre is reduced.}
    

\subsection{Matrices}
 We now consider some subvarieties of the space of $ N\times N$ matrices, again using the $ \lambda, \lambda', \lambda'', \mu, \mu', \mu''$ defined in the previous section.  Recall that $\lambda, \mu $ are partitions of $ N$.

\begin{itemize}
    \item For $ s \in \CC$ 
    % and $\lambda\in P_{++}$
    we have a Jordan form matrix $ J_{s,\lambda}$ with eigenvalue $ s$ and Jordan blocks of sizes $ \lambda_1, \dots, \lambda_m$. 
    % \acom{$s\in\AA$?}
    % 
    \item The adjoint orbit $ \OO^\lambda \subset M_N(\CC)$ of matrices conjugate to $ J_{0,\lambda}$.  Its closure is $ \overline{\OO}^\lambda = \bigcup_{\gamma \le \lambda} \OO^{\gamma}$.
    % 
    \item For $ s \in \CC, s \ne 0$, the adjoint orbit $ \OO^{\lambda', \lambda''}_{0,s}$ of matrices conjugate to $ J_{0,\lambda'} \oplus J_{s,\lambda''}$.  Its closure is $ \overline{\OO}^{\lambda', \lambda''}_{0,s} = \bigcup_{\gamma' \le \lambda', \gamma'' \le \lambda''} \OO^{\gamma', \gamma''}_{0,s}$. 
    % \acom{$s\in\AA$?}
    % 
    \item A linear operator $ T : V \rightarrow V $ on an $N$-dimensional vector space $V$ is said to have Jordan type $((0,\lambda'), (s,\lambda''))$ if its matrix representatives lie in $ \OO^{\lambda', \lambda''}_{0,s}$.
    % 
    \item The family of adjoint orbits $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow \AA$ whose fibre over $ s \ne 0 $ is $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ and whose fibre over 0 is $\overline{\OO}^\lambda$ a consequence of Eisenbud and Salten's description/characterization(?) of ``rank varieties'' as proved in \Cref{cor:es}.
    
   % \jcom{We should justify why the 0 fibre is correct scheme-theoretically.} 
    %\acom{does it follow because ``$s$ divides $\det g$'' is a closed condition 
    %in $\AA_s\times \AA_{\det}$? 
    %so the set of $A$ which are conjugate to
    %$J_{0,\lambda'} \oplus J_{s,\lambda''}$
    %by a matrix whose determinant is divisible by $s$ is also closed?}
    %\jcom{I didn't follow your argument. One approach would be to write down the generators for the ideal of $\overline{\OO}^{\lambda', \lambda''}_{0,s} $ and then set $ s =0$.  For example, take $ \lambda' = (1) = \lambda'' $ (I know it is a small example!), then the ideal is generated by $ tr(A) = s$, $det(A) = 0$, so when we take $ s =0$, we get just $ tr(A) = 0, det(A) = 0$.  I am hoping that we can find a reference for this result somewhere.}
    %\acom{I thought that the point would be to show that a dense subset of matrices which are conjugate to $J_{0,\lambda'}\oplus J_{s,\lambda''}$ limit to matrices which are conjugate to $J_\lambda$. In particular, that the conjugating matrix remains invertible in the $s\to0$ limit.}
    
% TODO: delete commented out remark and subcomment!
% \begin{remark}
\begin{comment}
        Base case $m = 1$: by a slight abuse of notation let $\lambda', \lambda_2 \in \NN$. Claim: $J=J_{0,\lambda'} \oplus J_{s,\lambda_2}$ is conjugate to 
        
% $\begin{pNiceArray}{CC|CC}[first-row,last-row=5,first-col,last-col,nullify-dots]
% & C_1 & \Cdots & & C_4 & \\
% L_1 & a_{11} & a_{12} & a_{13} & a_{14} & L_1 \\
% \Vdots & a_{21} & a_{22} & a_{23} & a_{24} & \Vdots \\
% \hline
% & a_{31} & a_{32} & a_{33} & a_{34} & \\
% L_4 & a_{41} & a_{42} & a_{43} & a_{44} & L_4 \\
% & C_1 & \Cdots & & C_4 &
% \end{pNiceArray}$
\[
        A = \begin{bNiceArray}{ccc|ccc}[columns-width = auto,first-row,last-col,code-for-first-row = \color{blue}\scriptstyle\rotate,code-for-last-col = \color{blue}\scriptstyle,nullify-dots,xdots/line-style=loosely dotted]
        1 & \Cdots & \lambda_1 & \lambda_1 + 1 & \Cdots & \lambda_1 + \lambda_2 \\
        0 & 1 & 0 & 0 & 0 & 0 & 1\\
        0 & \Ddots & 1 & 0 & 0 & 0 & \Vdots \\
        0 & 0 & 0 & 1 & 0 & 0 & \lambda_1 \\
        \hline
         & & & s & 1 & 0 & \lambda_1 + 1 \\
         & & & 0 & \Ddots & 1 & \Vdots \\
         & & & 0 & 0 & s & \lambda_1 + \lambda_2 
        \end{bNiceArray} 
\]
\end{comment}
% Denote by $R_i$ the $i$th row of $J$ 
%Let $g = E_{\lambda_2}\cdots E_1$ be the product of elementary matrices $E_i$ which correspond to the elementary row operations 
%$$R_{\lambda_1 + i - 1} \leftarrow R_{\lambda_1 + i-1} + \frac 1s R_{\lambda_1 + i}$$ 
%for $i = 1,\dots,\lambda_2$. Then $gJg^{-1} = A$. For the next part we introduce the notation $A_{\lambda_1+\lambda_2}$ for this matrix $A$. 

%Now let $\lambda_1,\lambda_2$ be arbitrary. Clearly $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to 
%$$\bigoplus_{i=1}^m (J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}})\,.$$ 
%Moreover each block $J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}}$ is conjugate to $A_{\lambda_{1,i}+\lambda_{2,i}}$. Altogether $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to $\bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$ and this latter matrix specializes to the Jordan normal form $J_{0,\lambda_1 + \lambda_2}$ at $s=0$.

% By Tanisaki $A\in \OO^\lambda$ iff the coefficients of $z^m$ in $k\times k$ minors of $zI - A$ for any $m\le p_\lambda(k) - 1$ and for any $k = 1\dots |\lambda|$ vanish. Here $p_\lambda(k) = \lambda_{n-k+1} + \cdots + b_n$ and $k = 1,\dots,n$. So $p_\lambda(k) = \#$ boxes in the last $k$ rows of $\lambda$.

% In particular this result describes the ideal $I_i$ of $N_i\times N_i$ matrices $A_i$ such that $A_i - \delta_{i,2}s$ is conjugate to $J_{\delta_{i,2} s, \lambda_i}$ for $i = 1,2$. Let $A_1,A_2$ be such a pair of matrices. Clearly $A_1 \oplus A_2$ is conjugate to $J_{0,\lambda_1} \oplus J_{s,\lambda_2}$. Let $g$ be such that $g(J_{0,\lambda_1} \oplus J_{s,\lambda_2})g^{-1} = \bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$. 

% If we apply the change of basis $g$ to the ideal $I=I_1 + I_2$ and set $s = 0$ (if possible) do we recover the ideal of $\OO^{\lambda_1 + \lambda_2}$? Let $A^g = g(A_1 \oplus A_2) g^{-1}$...
% \end{remark}
    
    \item Fix the ``$\mu$-numeration'' \((e^1_1,\ldots,e^{\mu_1}_1,\ldots,e^1_m,\ldots,e^{\mu_m}_m)\) of the standard basis of $\CC^N$. The Mirkovic--Vybornov slice $\TT_\mu$ is defined as the set of $A\in M_N(\CC)$ such that 
    \[
        \begin{aligned}
            &\text{for all } 1 \le a,s\le m\,,
            \text{for all } 1\le b\le \mu_a\,, 1\le t\le \mu_s\,, \\
            &\text{if } 1\le t < \mu_s \text{ or } t = \mu_s < b \le \mu_a \\
            &\text{then } (e^t_s)' (A-J_\mu) e^b_a = 0 \,.
        \end{aligned}    
    \]
    In words, $A$ is a $\mu$ Jordan form matrix plus a $\mu\times\mu$ block matrix with possibly nonzero entries occuring in the first $\min(\mu_i,\mu_j)$ columns of the last row of each $\mu_i\times\mu_j$ block. 
    % $\dim \TT_\mu = N^2 - \dim \OO_\mu$. 
    In pictures, aka by example, the elements $A$ for $\mu=(3,2,2,1)$ look like 
    \[
        \left[\begin{BMAT}(e){ccc;cc;c;c}{ccc;cc;c;c}
             & 1 & & & & & \\
             &  & 1 & & & & \\
            A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
              &  & &  & 1 & & \\
              A_{21}^1 & A_{21}^2 & & A_{22}^1 & A_{22}^2 & A_{23}^1 & A_{24}^1 \\
             A_{31}^1 & & & A_{32}^1 & & A_{33}^1 & A_{34}^1 \\
             A_{41}^1 & & & A_{42}^1 & & A_{43}^1 & A_{43}^1
        \end{BMAT}
        \right]
    \]
    Smaller: 
    \[
        \left[\begin{BMAT}(e){ccc;cc}{ccc;cc} 
            0 & 1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0 & 0\\
            A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2\\
            0 & 0 & 0 & 0 & 1\\
            A_{21}^1 & A_{21}^2 & 0 & A_{22}^1 & A_{22}^2
            \end{BMAT}\right]    
            \mapsto 
            \left[\begin{array}{rr}
                t^{3} - A_{11}^3 t^2 - A_{11}^2 t - A_{11}^1 & -A_{21}^2 - A_{21}^1  \\
                -A_{12}^2 t - A_{12}^1 & t^{2} - A_{22}^2 t - A_{22}^1
            \end{array}\right]
    \]
 
    
    \item To each $ A \in \TT_\mu$, we will associate the $m\times m$ matrix 
    of polynomials 
    $ g(A) = \left( g(A)_{ij} \right)$ in $ M_m(\CC[t]) $ as follows.
    \begin{equation}
        \label{eq:mvyofa}
        g(A)_{ij} = 
    \begin{cases} t^{\mu_i} - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i = j \\
            % \text{ if $ i = j$} \\
            - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i \ne j
        %  \text{ if $ i \ne j$ }
    \end{cases}
    \end{equation}
    where $A^k_{ji}$ is the $k$th entry from the left of the last row of the $\mu_j\times\mu_i$ block of $A$. 
    % For example, the $A$ above will define 
    % \acom{the transpose of! TODO. Had script?}
    % \[
    % \begin{bmatrix}
    %     t^3 - A_{11}^1 - A_{11}^2 t - A_{11}^3 t^2 & - A_{12}^1 - A_{12}^2 t & -A_{13}^1 & -A_{14}^1 \\
    %     & t^2 \\
    %     & & t \\
    %     & & & t 
    % \end{bmatrix}    
    % \]

    % \jcom{I moved the definition here since it will be useful for defining the upper triangular stuff.}
    % \acom{Ok.}
    
    \item The ``upper-triangular'' \mvy slice $\UU^{\mu', \mu''}_{0,\AA}\rightarrow \AA $, defined by
    $$
    \UU^{\mu', \mu''}_{0,\AA} := \{ (A,s) \in \TT_\mu \times \AA : g(A)_{ii} = t^{\mu_{1,i}} (t-s)^{\mu_{2,i}}, g(A)_{ij} = 0 \text{ for $ j < i $ }\}
    $$
    So a matrix in $\UU^{\mu', \mu''}_{0,\AA}$ is weakly block upper-triangular and its diagonal blocks are given by the companion matrices for the polynomials $t^{\mu_{1,i}} (t-s)^{\mu_{2,i}}$ ($i=1,\dots,m$).
    
    Note that the fibre $  \UU^{\mu', \mu''}_{0,0}$ is the same as the intersection of $ \TT_\mu $ with the set of strictly upper-triangular matrices.

    $\UU_{0,0}$ entries look like 
    \[
        \left[\begin{BMAT}(e){ccc;cc}{ccc;cc} 
            0 & 1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0 & 0\\
            0 & 0 & 0 & A_{12}^1 & A_{12}^2\\
            0 & 0 & 0 & 0 & 1\\
            0 & 0 & 0 & 0 & 0
            \end{BMAT}\right]    
            % \mapsto 
            % \left[\begin{array}{rr}
            %     t^{3} & 0 \\
            %     -A_{12}^2 t - A_{12}^1 & t^{2}
            % \end{array}\right]
    \]
    $\UU^{\mu', \mu''}_{0,\AA}$ entries look like 
    \[
        \left[\begin{BMAT}(e){ccc;cc;c}{ccc;cc;c} 
            0 & 1 & 0 & 0 & 0 & 0\\
            0 & 0 & 1 & 0 & 0 & 0\\
            0 & -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 \\
            0 & 0 & 0 & 0 & 1 & 0\\
            0 & 0 & 0 & 0 & s & A_{23}^1\\
            0 & 0 & 0 & 0 & 0 & s
            \end{BMAT}\right]    \in \UU_{0,s}^{(1,1,0),(2,1,1)}
    \]
    
    % \jcom{Instead of $\TT^+$ maybe a different letter would be best? I'm moved all $\lambda, \mu$ to upper indices for consistency (except for $ \cW_\mu, \TT_\mu$) and also to let us put $ s $ or $ \AA$ in the lower index.  I haven't been very consistent about $ \AA $ vs $ \CC$.  Which do you prefer?}
    % 
    % \acom{Maybe $\UU$ for a different letter? And, I guess we do not need $\AA$ but it is a nice letter to have around. It also serves to distinguish the $\CC$ over which $R,K$ are defined from the $\AA$ over which $\Grbd$ is defined?}
    % \jcom{Sure $\UU$ is good.}
    % \acom{And, sorry, ignore my other comment; it's the same $\CC$ (or $\AA$) nothing to distinguish.}
\end{itemize}

% TODO: place this better? 
% \acom{The following is a rough note.} 

\begin{theorem}(\cite{eisenbud1989rank})
    \label{thm:es}
Let $r$ be a decreasing concave non-negative integral function with $r(0) = N$ (the dimension of the ambient vector space). Such a function is called a rank function. Then, in the space of $N\times N$ matrices, 
% The relevant result from Eisenbud--Saltman seems to be that 
$$X_r = X_{r,0} = \{A:\rk A^i \le r(i) \}$$ 
is the flat limit of 
% $$X_{r,\lambda} = \{A : \rk (A-\lambda_i)^j\le r(i,j)\}$$
$$X_{r,z} = \{A : \rk (A-z_i)^j\le r(i,j)\}$$
with $r(i,j)$ determined by $r(i)$.  
\end{theorem}

\acom{Don't know how much of this setup to recall but here are the parts that will appear in the quoted theorem.}
% 
ES show that 
\begin{enumerate}
    \item $X_r$ is a normal variety
    \item $X_r$ is Gorenstein with rational singularities
    \item $X_r$ fits into a flat family over $\AA^m$ of normal varieties whose fibre over a point $(z_1,\dots,z_m)$ such that the $z_i$ are all distinct is 
\end{enumerate}
\[
    X_{r;z_1,\dots,z_m} = \left\{
        A \in \End(V) : \cork(A - z_i) \ge r(i-1) - r(i) , i = 1,\dots,m 
        \right\}
\]
They construct a simultaneous resolution of singularities and use the resolution of $X_r$ to describe the tangent space to $X_r$ at a point corresponding to an endomorphism $A$ such that $\rk A^i = r_i$ for all $i$. As an application of the tangent space computation they show that the general fiber of this family can be written as the scheme-theoretic intersection of varieties like $X_r$. 

To construct the resolution, they take $m$ to be the largest number such that $r(m) \ne r(m+1)$.
% Fact: rank functions stabilize
Let $(x_1,\dots, x_m)$ be coordinates on $\AA^m$ and let $W\subset \AA^m$ be a linear subvariety (translate of subvectorspace). 
They introduce the flag variety $F = \{(V = V_0 \supset V_1 \supset \cdots \supset V_m \supset 0 ) : \dim V_i = r(i)\}$ and define $\mathcal X_{r,W}\subset\End(V) \times W \times F$ to be the subvariety of triples $(A,\vec z,\{V_i\})$ such that $(A-z_i) V_{i-1} \subset V_i$ for $z_i = x_i(\vec z)$ and $i = 1,\dots,m$. The (reduced image of) projection of $\mathcal X_{r,W}$ down to $\End(V) \times W$ is denoted $X_{r,W}$ a closed affine subvariety. 

\begin{theorem}
    (\cite[Theorem~2.1 iii)]{eisenbud1989rank})
    $X_{r,W}$ is the restriction of $X_{r,\AA^m}$ to $W$ and it is flat over $W$. 
\end{theorem}

They apply this result to study the fibers of the family $X_{r,\AA^m}$ over points $\vec z \in \AA^m$ for $z_i$ not necessarily distinct. They partition the coordinates $z_i$ of $\vec z$ by equality with the help of a permutation $p$ of $\{1,\dots,m\}$ such that 
\begin{align*}
    z_{p(1)} &= \cdots = z_{p(m_1)} , \\
    z_{p(m_1 + 1)} &= \cdots = z_{p(m_2)} , \\ 
    \dots \\
    z_{p(m_s + 1)} &= \cdots = z_{p(m_s + 1)}
\end{align*}
and $p$ preserves the order within each interval $m_i + 1,\dots,m_{i+1}$. \acom{Not sure what this means. Order of coordinates?} With this notation in place they define 
\begin{gather*}
    r(i,j):= \dim V - a_{p(m_i + 1)} - \cdots - a_{p(m_i + j )} \\
    \qquad \qquad \qquad \qquad \qquad \text{ for } i = 1,\dots,s \text{ and } j = 1,\dots , m_{i + 1 } - m_i 
\end{gather*}
where $a_i = r(i-1) - r(i)$ which characterizes the Jordan types of the generalized eigenspaces associated to the eigenvalues $z_i$. Finally they state the following corollary. 
\begin{corollary}
    (\cite[Corollary~2.2]{eisenbud1989rank})
    The scheme-theoretic fibre of $X_{r,W}$ over $\vec z\in W$ is 
    \[
        X_{r,\vec z} = \{A \in \End(V) : \rk (A-z_i)^j\le r(i,j) \text{ for all } i , j \}
    \]
\end{corollary}

\begin{corollary}
    \label{cor:es}
    The family $\OO^{\lambda',\lambda''}_{0,\AA}$ defined by \Cref{above} has zero fibre $\OO^\lambda$. 
\end{corollary}

\begin{lemma}
    \label{lem:es}
    Let $\lambda = \lambda' + \lambda''$ be permutations. There exists a permutation of the columns of $\lambda$ such that the first $\lambda'_1$ columns are precisely the columns of $\lambda'$ and the next $\lambda''_1$ columns are precisely the columns of $\lambda''$. 
\end{lemma}

\begin{lemma}
   $r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$ is a rank function. 
\end{lemma}

The case that we are interested in is the case of two generalized/repeated eigenvalues, i.e.\ $\vec z = (z_1,z_2) = (0,s)$. \acom{Does order matter?} 
% Let me change notation so that $\lambda_i$ are shapes and 
We begin by considering the open loci where the rank conditions are strict. 

Recall that $\lambda' + \lambda'' = \lambda$ is a partition of $N$. 
Let $m_i = \lambda_{i,1}$ ($i = 1,2$), $m = m_1 + m_2$, and define
$$r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$$
on $j = 1,\dots,m$. Note that $r(j-1) - r(j)$ is the length of the $j$th column of $\lambda$. As in \cite{eisenbud1989rank} we will denote this quantity $a_j$. 
% 

Next, take $p$ to be a permutation of the (lengths of) columns $a_j$ of $\lambda$ such that $a_{p(1)},\dots,a_{p(m_1)}$ are the (lengths of) columns of $\lambda'$ and $a_{p(m_1 + 1)},\dots, a_{p(m_1 + m_2)}$ are the (lengths of) columns of $\lambda''$. 
Then the functions 
$$
\begin{aligned}
r(1,j) &= N - a_{p(1)} - \cdots - a_{p(j)} \\
% \text{ and } 
r(2,j) &= N - a_{p(m_1 + 1)} - \cdots - a_{p(m_1 + j)}
\end{aligned}
$$
are such that 
\begin{align*}
    \OO^{\lambda',\lambda''}_{0,s} = X_{r,(0,s)} = \{A:\rk A^j &= r(1,j) \text{ for } i = 1,\dots, m_1 \\ \text{ and } \rk (A-s)^j &= r(2,j) \text{ for } i = m_1 + 1 ,\dots, m \}
\end{align*}
and according to \cite[Theorem 2.1(iii)]{eisenbud1989rank} the family $X_{r,\AA^m}$ is flat over $\AA^m$ with zero fibre $X_{r,0} = \OO^\lambda$ as desired. I guess we need to carefully choose the $W \subset \AA^m$ defining our family, which is smaller than the family $X_{r,\AA^m}$. Well, $W$ is just cut out by $0 = x_1 = x_2 = \cdots = x_{m_1}$ and $ x_{m_1 + 1} = \cdots = x_m$, or some permutation thereof to match the permutation $p$ above.
 
% On the one hand, we know that $r(i,j)$ should be equal to $N$ minus the number of boxes in the first $j$ columns of $\lambda_i$. On the other hand, ES define $r(i,j)$ to be $N$ minus the $i$th chunk of $j$ columns determined by the rank function $r(i)$. Also, we would like for $r(i)$ to determine the shape $\lambda = \lambda' + \lambda''$. Take for example $\lambda' = \lambda'' = (2,1)$. Then $\lambda = (4,2)$ and so $r\{0,1,2,3,4\}=\{6,4,2,1,0\}$ and 
% $$\{a_1,a_2,a_3,a_4\} = \{6-4,4-2,2-1,1-0\}=\{2,2,1,1\}\,.$$

% For another example take $\lambda' = (1)$ and $\lambda'' = (1,1)$. Then $\lambda = (2,1)$ and $r\{0,1,2\} = \{3,1,0\}$ so $\{a_1,a_2\} = \{2,1\}$.

% Again too small. The problem is I don't think that it is in general true that some permutation of the list $r(i-1) - r(i)$ will be the list of lengths of columns of $\lambda'$ followed by the list of lengths of columns of $\lambda''$ which is what we are looking for. For one thing there's in general more than one way to write a partition as a sum of two partitions. Ok, concretely, a third example. Take $\lambda' = (a,b)$ and $\lambda'' = (c,d)$ then $\lambda = (a + c, b + d)$ and 
% $$r\{0,1,\dots,a + c\} = \{N, N-2, \dots, N-2(b+d), N-2(b+d) - 1,\dots, 0\}$$
% so 
% $$\{a_1,a_2,\dots\} = \{2,2,\dots,2,1,1,\dots,1\}$$
% with $b+d$ 2s and $a+c - b - d$ 1s. Wait a minute, nevermind, of course this is true. We have two partitions $\lambda'$ and $\lambda''$ and one way to form the partition $\lambda = \lambda' + \lambda''$ is to break $\lambda_i$ down into columns, order the columns by length, and recombine. Cool. I think it's fine. 
% \acom{That's it for the note.}

\section{Exposition}

Anderson and Kogan conjectured in \cite{anderson2006algebra} \acom{or earlier?} that those MV polynomials which are cluster monomials for a Fomin--Zelevinsky cluster algebra structure on $\CC[N]$ are naturally expressible as determinants\dots
% and they conjectured a formula for many of them.

It's not clear how this work helps with/relates to AK/their conjectures.

The generalized \mvy is interesting in its own right. \acom{is it?}

Computing fusion still hard but at least boiled down to linear algebra. Cf.\ fusion product as it appears in \cite{beilinson1991quantization,feigin2generalized,mirkovic2007geometric,anderson2006algebra,bezrukavnikov2005equivariant}.

Interesting combinatorics? Presumably it is related to Feigin and Loktev's generalized Kostka polynomials? Though Joel says that the product on SSYT that we are witnessing is unlikely to have a combinatorial description. 

Exchange relations only work on cluster modules where one is a mutation of the other (i.e.\ those corresponding to cluster monomials which are related by mutation). Of course this gives me everything. Up to $A_4$ as in type $A_5$ there exist indecomposable modules which are not cluster, so exchange relations do not apply. 

The hope (conjecture) is that this paper gives a way to compute on such modules. Cf.\ counterexample satisfying $\barD(c_Y) = \barD(b_Z) + 2\barD(b)$, where $b$ is a cluster monomial belonging to both bases and $c_Y$ is the square of a cluster monomial ($b'$ say) belonging to both bases. The $\barD$ equation suggests that $(b')^2 = b_Z + 2b$. We can try to check this using Roger's script. Such calculations bring us closer to answering two questions. First, how forgetful is $\barD$. Second, on the level of modules, are the terms appearing in the fusion product determined by $\Ext^1$ of the factors. By Roger's thesis this is true if the factors are cluster modules related by a mutation. 

% No; $x*y=\sum z\Rightarrow y = \frac 1 x \sum z$. 

Representation theory? 

\section{Rising Action} % Problem

\begin{lemma} \label{le:Grl1l2}
    Let $ L \in \Grth^+ $.  Let $ s \in \AA, s \ne 0 $.  The following are equivalent:
    \begin{enumerate}
        \item $ L $ is in the image of the map $ \Gr^{\lambda', \lambda''}_{0,s} \rightarrow \Grth^+$. \acom{Should we describe this map somewhere? Earlier?}
        \item The linear operator $ t $ on $ \CC[t]^m/L$ has Jordan type $(0,\lambda'), (s,\lambda'')$.
        \item $ L \in G(\CC[t]) t^{\lambda'} (t-s)^{\lambda''}$.
        % \acom{why no $G(\CC[t])$-coset?}
    \end{enumerate}
\end{lemma}

\begin{proof}
First we recall that for $ L \in \Gr^+$, $ L \in \Gr^{\lambda} = G(\cO)t^\lambda $ if and only if $ t |_{L_0/L} $ has Jordan type $ \lambda$. \acom{Change $L_0$}

    Now assume that $ (L, s) \in \Gr^{\lambda', \lambda''}_{0,s}$. We know that \acom{Rather than we know, it is by definition?} $ L(0) \in \Gr^{\lambda'}$ and $L(s) \in \Gr^{\lambda''} $.  This means that $t $ acting on 
    % $ \CC\xt^m / L(0)$ 
    $\cO^m/L(0)$ has Jordan type $ \lambda'$ and $ t$ acting on 
    % $ \CC\xt[t-s]^m / L(s) $ 
    $\cO_s^m/L(s)$ has Jordan type $ \lambda''$.  Now, the map 
    % $ L_0 / L \rightarrow \CC\xt^m / L(0)$ 
    $\CC[t]^m/L\to \cO^m/L(0)$ induces an isomorphism between the $0$-generalized eigenspace of $ t$ and $ \cO^m / L(0)$.   The same thing holds for the $s$-generalized eigenspace of $t $ and $ \cO_s^m/L(s)$. This shows that (i) implies (ii) and the logic can be reversed to see that (ii) implies (i). \acom{is this map $v + L \mapsto v + L(0)$?} \marginpar[]{*}
    % todo
    
    On the other hand, if $ L = g t^{\lambda'} (t-s)^{\lambda''} \CC[t]^m$ for some $ g \in G(\CC[t])$, then $ L(0) = g t^{\lambda'}\cO^m $ since 
    % $ (t-s)^{\lambda''} \in G(\CC\xt)$. 
    $ (t-s)^{\lambda''} \in G(\cO)$. In this way, we see that (iii) implies (ii) and the logic can be reversed to get the equivalence.
\end{proof}

\begin{lemma} \label{le:Wmu}
    Let $ L \in \Grth^+$.  The following are equivalent:
    \begin{enumerate}
        \item $ L \in \cW_\mu$.
                \item $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ for some $ v_i $ of the form $ v_i = t^{\mu_i} e_i + \sum_{j=1}^m p_{ij} e_j $ where $ p_{ij} \in \CC[t] $ has degree less than $ \min(\mu_i, \mu_j)$.
        \item  For all $ i $, 
        $$ t^{\mu_i} e_i \in \Sp_\CC(\{t^k e_j : 0 \le k < \min(\mu_i, \mu_j), 1 \le j \le m \}) + L. $$
    \end{enumerate}
    Moreover, for such $L $, $ \beta_\mu := \{ [t^k e_i] : 0 \le k < \mu_i, 1 \le i \le m\}$ forms a basis for $ L_0/L$. \acom{Change $L_0$}
\end{lemma}

\jcom{I realized the meaning of the ``extra'' condition from the paper with Sabin and I added it into point 3.}
\acom{Technically though the extra condition in that paper is weaker? $W_{\mu_i}$ being the span of $e_j,te_j,\dots,t^{\mu_i-1}e_j$ or the complement of $t^{\mu_i}\cO^m$ in $\cO^m$.}

\begin{proof}
    Let $ L \in \cW_\mu$.  Then $ L = \Sp_{\CC[t]}(v_1, \dots, v_m) $ for some $ v_i $ with $ v_i = t^{\mu_i} e_i + \sum_{j=1} q_{ij}t^{\mu_i} e_j $ and $ q_{ij} \in t^{-1} \Oinf$.  Since $ L \in \Grth^+ $, we see that $ v_i \in \CC[t]^m$ which means that $ p_{ij} := q_{ij}t^{\mu_i} $ lies in $ \CC[t]$.  By construction, the polynomial $ p_{ij}$ has degree less than $ \mu_i$.
    
    Fix $ i$ and suppose that for some $ j$, $ \mu_j < \mu_i$.  In this case, we can alter our basis to $ v'_i = v_i - r v_j$ for some polynomial $r \in \CC[t]$.  This gives us new polynomials $ p'_{ij} = p_{ij} - r (t^{\mu_j} + p_{jj}) $.  In this way, we can ensure that $ p_{ij} $ has degree less than $ \min(\mu_i, \mu_j)$.  Thus (i) implies (ii).
    
    Suppose that $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ as in (ii).  Then
    $$t^{\mu_i} e_i - v_i \in \Sp_\CC(\{t^k e_j :  k < \min(\mu_i, \mu_j), 1 \le j \le m \})  \,. $$
    Hence (ii) implies (iii).  

    Finally, given (iii), then we can see $ v_i := t^{\mu_i} e_i - \sum_{j=1}^m p_{ij} e_j \in L $ for some $ p_{ij} \in \CC[t]$ of degree less than $ \min(\mu_i,\mu_j) $.  It is easy to see that $ L = \Sp_\cO(v_1, \dots, v_m) $ and so $ L \in \cW_\mu$.  
    
    Finally to show that $ \beta_\mu$ forms a basis for $ \CC[t]^m/L$, it suffices to show that for each $ i$, $ t^{\mu_i} e_i  \in \Sp_\CC(\beta_\mu) + L$. \rcom{Preimage of $\beta_\mu$}  This follows immediately from (iii).
    % 
    \end{proof}
\acom{note-to-self, skipped this; return to it.}

\begin{lemma}
    Let $\mu',\mu'' \in P$ and $\mu = \mu' + \mu''$. Under the map $ \Grbd \rightarrow \Grth$, the image of $ S^{\mu', \mu''}_{0,\AA}$ lands in $ \cW_\mu$.
\end{lemma}
\acom{Probably it is worth writing down the map $ \Grbd \rightarrow \Grth$ just before this first occurence? \textit{is it $(L,s)\mapsto L$?}}
% Resolved? the map $ \Grbd \rightarrow \Grth$ caused some confusion in my talk as on the left we have a family and on the right we have the thick affine Grassmannian --- Jiuzu kept thinking aloud that we can make $\Grth$ a family too.. Anyway I think that you address this below Joel. 

\begin{proof}
    Let $ L $ lie in this image.  Then $ L = g t^{\mu'} (t-s)^{\mu''}\CC[t]^m$ for some $ g \in N(\CC[t, t^{-1}, (t-s)^{-1}]) $.  Let $ h =(t-s)^{\mu''} t^{-\mu''}  $.  Note that $ h \in T_1(\Oinf)$. 
    Moreover,
    $$ L = h (h^{-1} g h) t^{\mu}\CC[t]^m\,. $$
    Note that 
    % $ h^{-1} g h \in N(\CC\xT[t^{-1}])$ 
    $h^{-1} g h \in N(\cK_\infty)$ and so we can factor $ h^{-1} g h = n_1 n_2$ for $ n_1 \in N_1(\CC\xt[t^{-1}]), n_2 \in N(\CC[t]) $.  Since $ \mu $ is dominant, we see that $ t^{-\mu} n_2 t^\mu \in N(\CC[t]) $ \rcom{I think we want $\mu$ antidominant here or should the $N$'s be $N_-$'s?}.  Thus, $ L = h n_1 t^\mu \CC[t]^m$.  Since $ hn_1 \in G_1(\CC\xt[t^{-1}])$, the result follows.
    \end{proof}

\section{Climax}
Given $ A \in \TT_\mu$, recall the definition of $ g(A)$ given in \Cref{eq:mvyofa}.
% section ??.  
Since $ g(A) \in M_n(\CC[t]) \cap G_1(\Oinf)$, we will regard $ g(A)$ as giving an element of $ \Grth^+ \cap \cW_\mu$.

The following result is \cite[Theorem 3.2]{cautis2018categorical}.

\begin{theorem} \label{th:TmuWmu}
The map $ \TT_\mu \rightarrow \Grth^+ \cap \cW_\mu $ given by $ A \mapsto g(A)\CC[t]^m $ is an isomorphism with inverse given by
$$ L \mapsto [t|_{\CC[t]^m/L} ]_{\beta_\mu} \,. $$
\acom{Up to transpose?!}
\end{theorem}


For the next result, we will consider the ``intersection'' of $ \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ with $\cW_\mu$.  
As $  \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ is not a subscheme of $ \Grth$, by this intersection, we really mean the preimage of $ \cW_\mu$ under the map 
$$ 
\overline{\Gr}^{\lambda', \lambda''}_{0,\AA}  \hookrightarrow \Grbd \rightarrow \Grth\,.
$$
This is not a very serious abuse of notation, since the map $ \Grbd \rightarrow \Grth $ is almost injective. 

In a similar way, we will write $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu$ using the non-injective map $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow M_N(\CC)$ (for example the fibre of this map over $ 0 $ is $ \CC $). \acom{Grammar of `write using the map' is iffy. Also, is the fibre over 0 just a copy of the zero matrix in every $s$-fibre?}

\begin{theorem} \label{th:OGrl}
    There is an isomorphism
    $$\overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \cong \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu $$
    given by $ (A,s) \mapsto (g(A)\CC[t]^m, s)$.
\end{theorem}
% \acom{Joel, you stopped writing $X_{0,\AA}$ in favour of $X_\AA$ --- should we do this everywhere?}
% \jcom{Oops, no I think that we should stick with $ X_{0, \AA}$.}
% TODO: comb the doc at the end for this notation 
\begin{proof}
Since we already have the isomorphism from Theorem \ref{th:TmuWmu}, it suffices to show that for any $ A \in \TT_\mu$, $$ (A,s) \in \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \text{ if and only if } (g(A)\CC[t]^m, s) \in \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu $$
This follows immediately from Lemma \ref{le:Grl1l2}.
\end{proof}

\begin{theorem}
    The isomorphism from Theorem \ref{th:OGrl} restricts to an isomorphism
    $$ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \UU^{\mu', \mu''}_{0,\AA} \cong \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} \cap S^{\mu', \mu''}_{0,\AA}$$
\end{theorem}

%\jcom{I was thinking about the proof of this result.  It is enough to prove that for $ A \in \OO^{\lambda', \lambda''}_s \cap \TT_\mu $, $g(A) \in S^{\mu', \mu''}_s $ if and only if $ A \in T^+$.  It is easy to show that if $ A \in \UU$, then $ g(A) \in S^{\mu', \mu''}_s $.  But the converse is not so obvious.  (I think Anne's paper/thesis might be incomplete on this point.)  I thought of two approaches: first to write down the lattice interpretation of $S^{\mu', \mu''}$ (similar to what is in my thesis in the Anderson-Kogan comparison section) or think about both $S^{\mu', \mu''}$ and $\UU$ as attracting sets for a $\CC^\times$ action.  Which do you prefer?}
% 
%\acom{Assuming Theorem 2 we think this follows by application of Roger's lemma. Also, I think my thesis is complete on this point.}
% 
%\jcom{I thought about this and I agree with you now.  I will write this up soon.} 
% 
%\acom{But I am also curious about the attracting set approach.}

\begin{proof}
We could prove this by observing the both sides are the attracting locus of an appropriate $ \CC^\times$ action. However, we will give the following more algebraic proof.


 Let $ A \in \TT_\mu$ and $ s \in \CC $. We must show that  $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $ if and only if $ (g(A),s) \in S^{\mu', \mu''}_{0,\AA} $.
 
 On the one hand, if $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $, then $ g(A)$ is upper triangular with diagonal $ t^{\mu'} (t-s)^{\mu''}$, and so $ g(A) \in N_-[t, t^{-1}, (t-s)^{-1}] t^{\mu'} (t-s)^{\mu''}$.
 
 On the other hand, if $ (g(A), s) \in S^{\mu', \mu''}_{0,\AA}$, then we can write 
 $$
 g t^\mu r= n t^{\mu'} (t-s)^{\mu''}
$$
for some $ r \in G(\CC[t]), n \in N_-(\CC[t, t^{-1}, (t-s)^{-1}]) $ and $ g = g(A)t^{-\mu}$.  Let $ h = (t-s)^{\mu''} t^{-\mu''}$ which lies in $ T_1(\CC\xt[t^{-1}]) $.
Note that $ h^{-1}n h \in N(\Kinf)$, 
% ((t^{-1}))
so we can factor it as $ h^{-1} n h  = n_1 n_2 $, where $ n_1 \in N_{-,1}(\Oinf), n_2 \in N_-(\CC[t])$.  So then after doing a bit of algebra, we reach
$$
t^\mu r (t^{-\mu} n_2^{-1} t^\mu) t^{-\mu} = g^{-1} h n_1.
$$
The right hand side $ g^{-1} h n_1 $ lies in $ G_1(\Oinf) $.  Since $ \mu $ is dominant, $ t^{-\mu} n_2 t^\mu \in N(\CC[t])$, and so the left hand side lies in $t^\mu G(\CC[t]) t^{-\mu}$.

Moreover since $ \mu $ is dominant, we know that 
$$t^\mu G(\CC[t]) t^{-\mu} \cap G_1(\Oinf) = N_1(\Oinf)\,.$$
% $$t^\mu G(\cO) t^{-\mu} \cap G_1(\CC\xt[t^{-1}]) = N_1(\CC\xt[t^{-1}]\,.$$
Thus, we deduce that $ g^{-1} h n_1 \in N_1(\Oinf)$ 
% $ g^{-1} h n_1 \in N_1(\CC\xt[t^{-1}])$ 
and hence $ g(A) \in t^{\mu'} (t-s)^{\mu''} N_1(\Oinf) $.  Given that we know that $ A \in \TT_\mu $, this implies that $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA}$ as desired.
\end{proof}

%\jcom{In this proof, I made some statements about $\mu$ dominant, which are incorrect (as Anne knows). To make them correct we should either switch to having $ \mu$ being antidominant or switch $ N$ to $ N_-$.  In this proof $ g(A)$ generally denotes a group element, rather than an affine Grassmannian element which conflicts a bit with earlier usage.}

\section{Falling Action} % Resolution
\begin{comment}
\begin{theorem}
    Let $\lambda_i\ge\mu_i$ be dominant ($i=1,2$), $\mu = \mu' +\mu''$, and $\lambda =\lambda'+\lambda''$. 
    % I have an irrational dislike of ending sentences with subscripts --- can we fix this?
    There is an isomorphism 
    \begin{equation}
        \overline{\OO}_{\lambda',\lambda''}\cap\TT_{\mu',\mu''} \to \overline\Gr_\AA^{\lambda',\lambda''}\cap \cW_{\mu',\mu''}
    \end{equation}
    got by taking a $\mu\times\mu$ block matrix $A$ in the $s$-fibre $\overline{\OO_{\lambda',\lambda''}^s}\cap\TT_{\mu',\mu''}^s$ on the left to the representative of the $s$-fibre on the right defined by  
    \begin{equation}
        \begin{split}
            g &= t^{\mu'} (t-s)^{\mu''} + a(t) \\
            a_{ij}(t) &= - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1}
        \end{split}
    \end{equation}
    where $A^k_{ji}$ is the $k$th entry from the left of the last row of the $\mu_j\times\mu_i$ block of $A$. 
\end{theorem}

Let's call this the MVyBD isomorphism.

\begin{proof}
    The proof is fibre by fibre, so fix $s\ne 0$. \acom{Emphasize in the intro later (because this always confuses me) that by the $s$-fibre we really mean the $(0,s)$-fibre; i.e.\ its the BD Grassmannian over the second symmetric power of $C = \AA^1$; better just replace $s$-fibre by $(0,s)$-fibre everywhere it occurs.}
    \begin{enumerate}
        \item The map is well defined. In particular, it defines $\CC[t]$-lattices in $\CC(t)^m$. Moreover, these lattices break down to give pairs of lattices upon inverting $t$ or $t-s$ that have the right properties. [Copy Roger's proof]
        \item The inverse map is got by taking the matrix of multiplication by $t$.  More precisely, let $ L \in Gr^{BD} \cap \cW_\mu$.  We work with the quotient $\CC[t]^m/L$ just as in the ordinary MVy isomorphism---the only difference being $\CC\xt$ is replaced by $\CC[t]$.
\begin{enumerate}
    \item 
    We claim that 
    \begin{equation}
        \{[e_i],[te_i],\dots,[t^{\mu_{i}-1}e_i] : 1\le i \le m\}
    \end{equation}
    is a $\CC$-basis of $\CC[t]^m/L$.
    
    To see this, we use that $ L $ 
 has a $\CC[t]$-basis of the form 
    \begin{equation}
        v_i = t^{\mu_i} + \sum_{j>i} p_{ij}(t) e_j 
    \end{equation}
    with $\deg p_{ij}(t) < \mu_i = \mu_{1,i} + \mu_{2,i}$ ($1\le i\le m$).
    \acom{I don't know why this should be true. We might have to just define fibres of $\cW_{\mu',\mu''}$ in this way?}
    \item $t\big|_{\CC[t]^m/L}$ will have two eigenvalues, 0 and $s$, and its generalized 0-eigenspace will have block type $\le \lambda'$ while its generalized $s$-eigenspace will have block type $\le \lambda''$. 
    To see this, note that there is a natural isomorphism
    $$\CC\xt^m/(L \otimes_{\CC[t]} \CC\xt) = \text{generalized $0$ eigenspace of $t$ on } \CC[t]^m/L$$
    carrying the action of $t $ to the action of $t$.
    
    The left hand side is the same thing as
    $$ \CC\xt^m / (L \otimes_{\CC[t]} \CC\xt) = (\CC[t]^m/L) \otimes_{\CC[t]} \CC\xt $$
    
    the defining fact that lattices satisfying Equation~\ref{eq:defGrBDlambda} equivalently satisfy 
    \begin{equation}
        \begin{split}
            t\big|_{\CC\xt^m/L_1}\text{ has Jordan type} \le \lambda' \\
            t\big|_{\CC\xt^m/L_2}\text{ has Jordan type} \le \lambda'' 
        \end{split}
    \end{equation}
    where recall 
    $L_i = L\otimes \CC\xt$ ??? 
    % $L_i = L\otimes \CC[(t-p_i)^{-1}]$ 
    and $p_1 = s$ while $p_2 = 0$. 
    
    $$\CC\xt^m/(L \otimes_{\CC[t]} \CC\xt) = \text{generalized $0$ eigenspace of $t$ on } \CC[t]^m/L$$
    carrying the action of $t $ to the action of $t$.
    
    The left hand side is the same thing as
    $$ \CC\xt^m / (L \otimes_{\CC[t]} \CC\xt) = (\CC[t]^m/L) \otimes_{\CC[t]} \CC\xt $$
    \acom{Somehow, restricting to an eigenspace is like inverting/forgetting the action of $t$ by any other generalized eigenvalue? Basic linear algebra? Joel?}
\end{enumerate}
    \end{enumerate}
\end{proof}



\begin{theorem}[Theorem 1 version 2]
    Let $\lambda_1,\lambda_2$ and $\mu$ 
    %be dominant
    be arbitrary, such that $\lambda = \lambda_1 + \lambda_2 \ge \mu$. Then there is an isomorphism 
    \begin{equation}
        \overline{\OO_{\lambda_1,\lambda_2}} \cap \TT_\mu \to \overline{\Grbd[2]^{\lambda_2,\lambda_2}}\cap\cW_\mu 
    \end{equation}
    defined by the same map as in Theorem 1.
\end{theorem}

\jcom{This is true as stated with the ``larger'' definition of $ \cW_\mu $.  In fact, for any $\lambda_1, \lambda_2$, it is true $ \overline{\Grbd[2]^{\lambda_2,\lambda_2}}\cap\cW_\mu $ is contained in a subset that we could call $ \cW_\mu^s$ which we could define as
$$
\cW_\mu^s = G_1\xt[t^{-1}]\TT_\mu \cap G[t,t^{-1}, (t-s)^{-1}] / G[t]
$$
where we regard $G[t,t^{-1}, (t-s)^{-1}] / G[t] \subset G\xT[t^{-1}]/G[t] $

The way to think about this is as follows: inside the thick affine Grassmannian we can consider the $G$-bundles trivialized away from just 0, $s$, or equivalently those lattices which become the standard lattice after tensoring with $ \CC[[t-a]] $ for any $ a \ne 0, s$.
}
\begin{corollary}
    The MVyBD isomorphism restricts to an isomorphism of subfamilies 
    \begin{equation}
        \overline{\OO_{\lambda_1,\lambda_2}}\cap\TT_{\mu',\mu''}^+ \to \overline{(\Grbd[2])^{\lambda_1,\lambda_2}}\cap S_{\mu_1,\mu''}\,. 
    \end{equation}
\end{corollary}

\begin{proof}
    Let $ A \in \overline{\OO_{\lambda_1,\lambda_2}}\cap\TT_{\mu_1,\mu_2}^+$ and let $ g $ be the polynomial matrix formed by the Mirkovic-Vybornov isomorphism.  Then the diagonal entries of $ g $ are $ t^{\mu_{1,k}} (t-s)^{\mu_{2,k}}$ and we can factor
    $$ g = (g t^{-\mu_1}(t-s)^{-\mu_2}) t^{\mu_1}(t-s)^{\mu_2} \in N[t, t^{-1}, (t-s)^{-1}] t^{\mu_1} (t-s)^{\mu_2}$$
    So we get containment in one direction.
    
    For the reverse containment, we choose $ [g] \in \overline{(\Grbd[2])^{\lambda_1,\lambda_2}}\cap S_{\mu_1,\mu_2}$.  By the lemma below, $[g] \in \cW_\mu$ and thus it lies in the image of our map and we are done.
\end{proof}

% Define $S_{\mu_1,\mu_2}^s = N_-\xT[t^{-1}] t^{\mu_1} (t-s)^{\mu_2}$. 

\acom{is it a fibre of $S_{\mu_1,\mu_2}$ defined above?}

We could also make the following claim. 


\begin{lemma}[KWWY14]
    Let $\mu$ be dominant. Then 
    \begin{equation}
        N_{-}\xT[t^{-1}] L_\mu = N_1\xt[t^{-1}]L_\mu
    \end{equation}
    \acom{where I am not sure about the double brackets.}
\end{lemma}

\begin{lemma}%[Roger's lemma]
    Let $\mu_1,\mu_2$ be dominant and let $s\in \AA^1 - \{0\}$. Then 
    \begin{equation}
        S_{\mu_1,\mu_2}^s \subset \cW_\mu 
    \end{equation}
    where $\mu = \mu_1 + \mu_2$.
\end{lemma}

\begin{proof}
    % Copy Roger's proof.
    We have
\[
\begin{split}
    S_{\mu_1, \mu_2} & = N\xT[t^{-1}]t^{\mu_1}(t-s)^{\mu_2} \\
     & \subset T_1\xt[t^{-1}] N\xT[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \\
     & = T_1\xt[t^{-1}] N_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \qquad \text{\cite[Lemma 2.3]{kamnitzer2014yangians}}\\
     & = B_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \\
     & = B_1\xt[t^{-1}] t^{\mu_1 + \mu_2} \\
     & \subset G_1\xt[t^{-1}] t^{\mu_1 + \mu_2} \\
     & = W_{\mu_1 + \mu_2}
\end{split}
\]
where $B_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} = B_1\xt[t^{-1}] t^{\mu_1 + \mu_2}$ since 
\[
\frac{t}{t-s} = 
1 + \frac{s}{t} + \frac{s^2}{t^2} + \cdots 
\in B_1\xt[t^{-1}].
\]
\end{proof}
\end{comment}

\section{Denouement}
\label{s:denouement}

As an application we can compute fusion of stable MV cycles of type $\alpha_i$ for any $i \in I$. What about more general weights? Having $\kpf > 1$.

\begin{proposition}(\cite{baumann2019mirkovic})
    % Given two MV cycles $Z_\tau$ and $Z_\sigma$ of type\dots 
    Let $Z_{i} = Z_{\tau_i}$  be an MV cycle of type $\lambda_i\in P_{++}$ and weight $\mu_i\in P$ ($i = 1,2$). Then 
    \begin{equation}
        % Z_\tau\ast Z_\sigma 
        m_{\lambda_1\lambda_2} ([Z_1]\otimes[Z_2]) 
        = 
        % \sum_{Z\in\cZ(\lambda)_\mu} 
        \sum_{\tau\in S(\lambda)_\mu}
        i\left(
            Z_\tau, Z_1\circ Z_2 
            % \pi^{-1}(0)\cdot\overline{Z_1\times Z_2\times U}
        \right) [Z_\tau]
    \end{equation}
    and the intersection multiplicities on the right are found by the algorithm described in APX.
    In particular, 
    the multiplicity of $Z_\tau$
    % in the product of the zero fiber 
    % (divisor class) 
    % and the 
    % (Zariski) 
    % closure of the family $Z_1 \times Z_2 \times U$ 
    is equal to the multiplicity of its generalized orbital variety $X_\tau$ among the set 
    \begin{equation}\label{eq:tabmult}
        \irr\lim_{s\to 0} \overline{\OO}^{\lambda_1,\lambda_2}_{0,s}\cap\UU^{\mu_1,\mu_2}_{0,s} \,. 
        % \irr \lim_{s\to 0} \overline\OO^s_{\lambda_1,\lambda_2} \cap \TT_{\mu_1,\mu_2}^{+,s}
    \end{equation}
    Since stable MV cycles can be represented by ordinary ones the multiplicities in 
    $$
    b_{Z_1}b_{Z_2} = \sum_{Z\in\cZ(\infty)_{-\nu_1 - \nu_2}} i \left(
        Z, Z_1 \circ Z_2 
        % \pi^{-1}(0) \cdot \overline{Z_1 \times Z_2 \times U}
    \right) b_Z 
    $$
    can also be deduced from \cref{eq:tabmult} for appropriate choices of $\lambda_i$ and $\mu_i$ ($i = 1,2$).
\end{proposition}
\acom{or Corollary?}

\begin{conjecture}
    % Let $Z_i \subset \overline{S^{\nu_i}\cap S^0_-}$ be an MV cycle of weight $\nu_i$ ($i = 1,2$) and put $\nu = \nu_1 + \nu_2$. 
    Let $\tau$ be the tableau of shape $\lambda$ and weight $\mu$ whose Lusztig datum is equal to the sum of the Lusztig data of $\tau_1$ and $\tau_2$. 
    % Then $i(\tau, \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U})$ is equal to 1. 
    Then 
    \begin{equation}
        i(\tau, 
        % \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U}
        % Z_1 \circ Z_2 
        \tau_1 \circ \tau_2 
        ) = 1 \,. 
    \end{equation}
\end{conjecture}

% TODO: un-appendicize 
% \appendix

\subsection{Generalized orbital varieties?}
TODO: recall $\tau\mapsto X_\tau \mapsto Z_\tau$

\begin{question}
    Smallest $\mu_1, \mu_2$ such that\dots?
\end{question}

\subsection{Tableaux and families}

TODO: $Z_1\ast_\AA Z_2$ is described by $\tau_1\ast\tau_2$ or $\tau_1\sqcup\tau_2$

\begin{question}
    How do we know that our algorithm produces MV cycles? 
\end{question}

\begin{proposition}
    Let $\tau_1,\tau_2$\dots 
    \[
    \{A\in\UU_{0,s}^{\mu_1,\mu_2} : A \big|_{\mu^1 + \cdots + \mu^i} \in \OO_{0,s}^{\lambda_{1}^{(i)},\lambda_2^{(i)}} \text{ all } i\} = Z_{\tau_1}\times Z_{\tau_2}
    \]
\end{proposition}

\begin{proof}
    Want $L(0) \in Z(\tau_1)$ and $L(s) = Z(\tau_2)$. \acom{Another notational possibility is $\mu = \mu^0 + \mu^s$ etc., or $\mu(0) + \mu(s)$ or some variant with the eigenvalues being the indices.}

    Fix $i \in \{1,\dots,m\}$ and let $p = \mu^1 + \cdots + \mu^i$. 
    % Define $\lambda^{(i)}(\tau_1 \circ \tau_2)$ so that the 
    Jordan type of $t\big|_{\CC[t]^p/L(0)\cap\CC[t]^p}$ is $\lambda_1^{(i)}$ and Jordan type of $t-s\big|_{\CC[t]^p/L(s)\cap \CC[t]^p}$ is $\lambda_2^{(i)}$. 

    Need: operations of taking quotients and taking generalized eigenspaces (i.e.\ $-\otimes\CC\xt[t-a]$) commute.
    See $(*)$ above. Also 
    % 
    $$\CC\xt^m/(L \otimes_{\CC[t]} \CC\xt) = \text{generalized $0$ eigenspace of $t$ on } \CC[t]^m/L$$
    carrying the action of $t $ to the action of $t$.
    
    The left hand side is the same thing as
    $$ \CC\xt^m / (L \otimes_{\CC[t]} \CC\xt) = (\CC[t]^m/L) \otimes_{\CC[t]} \CC\xt $$
    \acom{Restricting to an eigenspace is like inverting (forgetting) the action of $t$ by any other generalized eigenvalue. Does this have to do with the fact that a functor $F: \sch \to \set$ commutes with projective limit if and only if it's representable?}

    Exercise 2.20 from \cite{atiyah2018introduction}: If $f : A \to B$ is a ring homomorphism and $M$ is a flat $A$-module, then $M_B = B\otimes_A M$ is a flat $B$-module. Take $A=\CC[t]$ and $B=\CC\xt$. \marginpar[]{*}

    This exercise is irrelevant. We want to say something about two different modules over the same base ring. As Roger reminded the relevant fact is \cite[Proposition~10.14]{atiyah2018introduction}: 
    % 
    \begin{quotation}
        If $A$ is a Noetherian ring, $\fa$ an ideal, $\hat A$ the $\fa$-adic completion of $A$, then $\hat A$ is a flat $A$-algebra.
    \end{quotation}
    applied to $A = \CC[t]$ and $\fa = (t)$. 
\end{proof}

\begin{question}[Related question]
    If $\mu_1,\mu_2$ are not dominant, the LHS is smaller so the map $?$ has no chance of being an isomorphism. In general the map $?$ has to do with producing a matrix from a linear operator. 
    \[
    \begin{tikzcd}
        \overline\Gr^{\lambda_1,\lambda_2}_{0,s}\cap S^{\mu_1,\mu_2}_{0,s} \ar[r,equals]\ar[d] & \overline\Gr^{\lambda_1} \cap S^{\mu_1} \times \overline\Gr^{\lambda_2} \cap S^{\mu_2} \supset Z_1 \times Z_2 \ar[d]\\
        \overline\OO^{\lambda_1,\lambda_2}_{0,s} \cap \UU^{\mu_1,\mu_2}_{0,s} \ar[r,"?"] & \overline\OO^{\lambda_1}\cap \TT_{\mu_1}\cap \n \times \overline\OO^{\lambda_2}\cap \TT_{\mu_2}\cap \n
    \end{tikzcd}    
    \]
\end{question}

\subsection{Examples}
\rcom{I have 3 examples in mind right now: fusion product producing 1 term (maybe $1 \leftarrow 2 * 1 \rightarrow 2$), fusion product producing more than 1 term $(S_1 * S_2)$, and fusion product with multiplicities $((S_1 * S_2)^2)$}

\acom{Plus fusion product of fake MV cycles for a fourth!}

We view MV cycles as semi-standard young tableaux. Suppose we have two tableaux $\tau_1$ and $\tau_2$ with respective weights $\lambda_i$ and $\mu_i$ and we wished to take their fusion product. We consider a generic matrix $X \in \UU^{\lambda_1,\lambda_2}_{0,s}$. 

Consider the subtableaux $\tau_{1,i}$ of $\tau_1$ and $\tau_{2,i}$ of $\tau_2$ where we take the boxes of $\tau_1$ and $\tau_2$ containing the numbers $1,\dots,i$. Let $\lambda_{1,j}$ and $\lambda_{2,j}$ be the corresponding dominant weights. For each $i$, we obtain a corresponding submatrix $X_i \in \UU^{\lambda_{1,i}, \lambda_{2,i}}_{0,s}$ of $X$. These subtableaux impose rank conditions, equivalently vanishing minors, on each $X_i$ which allows us to deduce relations on the variables in $X$. The relations must also include the new variables in the latest block and cannot have $s$ as a factor.

To find the MV cycles, equivalently tableaux, in the $0$-fibre of the fusion, we consider the ring generated by all the variables of $X$ and quotient out the ideal generated by the relations from the vanishing minors as well as the minimal polynomial of $X$. We then add $s$ into the ideal. Each piece in the primary decomposition of this ideal will correspond to a tableaux, and subsequently a MV cycle.

\begin{example}
Let $G = SL_3$ and consider the tableaux
$$\tau_1 = \young(12,3) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau_2=\young(13,2).$$
The MV cycles corresponding to $\tau_1$ and $\tau_2$ are both isomorphic to $\PP^2$.
The weights are $\lambda_1 = \lambda_2 = (2,1,0)$ and $\mu_1 = \mu_2 = (1,1,1)$. Hence the matrix in consideration is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
    0 & 1 & & & & \\
     & s & a_1 & a_2 & b_1 & b_2 \\
     & & 0 & 1 & & \\
     & & & s & b_3 & b_4 \\
     & & & & 0 & 1 \\
     & & & & & s
\end{BMAT}
\right].
\]

For the first entry of each $\mu_i$, we have the following subtableaux to consider:
$$\young(1) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1).$$
Let $X_1$ be the top-left $2 \times 2$ submatrix of $X$. The subtableaux imposes the conditions $\dim \ker X_1 = 1$ and $\dim \ker (X_1-s) = 1$. We see that we do not obtain any relations from this.

For the first two entries of each $\mu_i$, we have the following subtableaux:
$$\young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1,2).$$
Let $X_2$ be the top-left $4 \times 4$ submatrix of $X$. The left subtableau imposes the conditions $\dim \ker X_2 = 1$ and $\dim \ker X_2^2 = 2$ while the right subtableau imposes the condition that $\dim \ker (X_2 -s) = 2$. These conditions force the relation $$a_1 + sa_2 = 0.$$

The original tableaux for $X$ imposes the conditions that $\dim \ker X = 2$, $\dim \ker X^2 = 3$, $\dim \ker (X-s) = 2$, and $\dim \ker (X-s)^2 = 3$. These force the relation $$b_3 = 0.$$

Hence we consider the quotient
$$\frac{\CC[a_1,a_2,b_1,b_2,b_3,b_4,s]}{\langle a_1+sa_2,b_3,s \rangle} \cong
\frac{\CC[a_1,a_2,b_1,b_2,b_3,b_4]}{\langle a_1,b_3 \rangle}.$$
We see already that the $0$-fibre is irreducible. The tableau corresponding to the ideal $\langle a_1, b_3 \rangle$ is 
$$\young(1123,23)$$
which corresponds to the MV cycle that is the product of the MV cycles corresponding to $\tau_1$ and $\tau_2$.
\end{example}

\begin{example}
Let $G = SL_3$ and consider the tableaux
$$\tau_1 = \young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau_2 = \young(11,23).$$
The MV cycles corresponding to the above tableaux are each isomorphic to $\PP^1$.
We have the weights to be $\lambda_1 = (2,0,0)$, $\lambda_2 = (2,2,0)$, $\mu_1 = (1,1,0)$, and $\mu_2 = (2,1,1)$. The matrix under consideration is therefore
\[
X = \left[\begin{BMAT}(e){ccc;cc;c}{ccc;cc;c}
    0 & 1 & & & & \\
     & 0 & 1 & & & \\
     & -s^2 & 2s & a_1 & a_2 & b_1 \\
     & & & 0 & 1 & \\
     & & & & s & b_2 \\
     & & & & & s
\end{BMAT}
\right].
\]
There are no relations from looking at the submatrix $X_1$. From the submatrix $X_2$, we require $$a_1 + sa_2 = 0.$$
From $X$, we further obtain the relations $$a_2b_2 + sb_1 = 0, a_1b_2 + s^2b_1 = 0.$$
Then the quotient ring we find is
$$\frac{\CC[a_1,a_2,b_1,b_2,s]}{\langle a_1+sa_2, a_2b_2+sb_1, a_1b_2+s^2b_1, s\rangle} \cong \frac{\CC[a_1,a_2,b_1,b_3,s]}{\langle a_1, a_2\rangle \cap \langle a_1, b_2 \rangle}.$$
We see that the 0-fibre is reducible with two components. The tableau corresponding to each ideal is
$$\begin{array}{cc} \vspace{1mm}
    \young(1113,22) & \text{for } \langle a_1, a_2 \rangle \\ 
    \young(1112,23) & \text{for } \langle a_1, b_2 \rangle.
\end{array}$$
Each of these tableaux corresponds to a MV cycle isomorphic to $\PP^2$ so the generic fibre $\PP^1 \times \PP^1$ degenerates to two copies of $\PP^2$.
\end{example}

\begin{example}
Let $G = SL_3$ and consider the following tableaux:
$$\tau_1 = \young(1122) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau_2 = \young(1111,2233).$$
The MV cycles corresponding to these tableaux are each isomorphic to $\PP^1 \times \PP^1$. 
The weights are then $\lambda_1 = (4,0,0)$, $\lambda_2 = (4,4,0)$, $\mu_1 = (2,2,0)$, and $\mu_2 = (4,2,2)$.
Then we are considering the following matrix:
\[
X = \left[\begin{BMAT}(e){cccccc;cccc;cc}{cccccc;cccc;cc}
    0 & 1 & & & & & & & & & & \\
     & 0 & 1 & & & & & & & & & \\
     & & 0 & 1 & & & & & & & & \\
     & & & 0 & 1 & & & & & & & \\
     & & & & 0 & 1 & & & & & & \\
     & & -s^4 & 3s^3 & -6s^2 & 4s & a_1 & a_2 & a_3 & a_4 & b_1 & b_2 \\
     & & & & & & 0 & 1 & & & & \\
     & & & & & & & 0 & 1 & & & \\
     & & & & & & & & 0 & 1 & & \\
     & & & & & & & & -s^2 & 2s & b_3 & b_4 \\
     & & & & & & & & & & 0 & 1 \\
     & & & & & & & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
By computer, it can be checked that we obtain the following 3 ideals:
$$
\begin{array}{cl}
    I_1 = & \langle a_1, a_2, a_3, a_4, s \rangle \\
    I_2 = & \langle a_1, a_2, b_3, b_4, s \rangle \\
    I_3 = & \langle a_1, a_2, b_3^2, a_4b_3 + a_3b_4, a_3b_3, a_3^2, s \rangle.
\end{array}
$$
It can be checked that $I_3$ has multiplicity 2 \rcom{Not sure how to make this precise and how to check it.} so the 0-fibre has 4 components, two of which are the same. The tableau corresponding to each ideal is
$$
\begin{array}{cc}\vspace{1mm}
    \young(11111133,2222) & \text{for } I_1 \\ \vspace{1mm}
    \young(11111122,2233) & \text{for } I_2 \\ \vspace{1mm}
    \young(11111123,2223) & \text{for } I_3.
\end{array}
$$
The MV cycle for each ideal is isomorphic to $\PP^2 \times \PP^2$.
\end{example}

\begin{example}
Let us redo example 3 but with non-dominant weights $\mu_i$ such that $\mu_1 + \mu_2$ is still dominant. Then consider the tableaux
$$\tau_1 = \young(22) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau_2 = \young(11,33).$$
Then the weights are $\lambda_1 = (2,0,0)$, $\lambda_2 = (2,2,0)$, $\mu_1 = (0,2,0)$, and $\mu_2 = (2,0,2)$. Our matrix $X$ is now
\[
X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
    0 & 1 & & & & \\
    -s^2 & 2s & a_1 & a_2 & b_1 & b_2 \\
     & & 0 & 1 & & \\
     & & & 0 & b_3 & b_4 \\
     & & & & 0 & 1 \\
     & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
Using a computer, the ideals we now get are
$$
\begin{array}{cl}
    I_1 = & \langle a_1, a_2 \rangle \\
    I_2 = & \langle b_3, b_4 \rangle \\
    I_3 = & \langle b_3^2, a_2b_3 + a_1b_4, a_1b_3, a_1^2 \rangle. 
\end{array}
$$
The tableaux corresponding to each ideal is
$$
\begin{array}{cc}\vspace{1mm}
    \young(1133,22) & \text{for } I_1 \\ \vspace{1mm}
    \young(1122,33) & \text{for } I_2 \\ \vspace{1mm}
    \young(1123,23) & \text{for } I_3
\end{array}
$$
and the MV cycles corresponding to each are the same as before.
\[
{\lambda'}^{(i)}    
\]  
\end{example}

\bibliographystyle{alpha}
\bibliography{mvybd}

\end{document}