%!TEX program = lualatex

\documentclass[draft]{article}
\usepackage{basic}
\usepackage{emoji}

% \setemojifont{Apple Color Emoji}
% Working title: 
\title{How to compute the fusion product of MV cycles in type A}
\author{Roger Bai, Anne Dranowski, Joel Kamnitzer} 
% \date{October 2020}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
% 
% The Mirkovi\'c--Vybornov isomorphism
In \cite{mirkovic2007quiver} (see also the recent sequel \cite{mirkovic2019comparison}) Mirkovi\'c and Vybornov provide a geometric version of symmetric and skew Howe $(\GL_m,\GL_n)$ dualities by relating Kazhdan--Lusztig slices in the affine Grassmannian of $\GL_m$ to slices in $n\times n$ nilpotent orbits on the one hand 
% conjugacy classes of nilpotent matrices 
and to Nakajima quiver varieties on the other hand. 
This paper generalizes the first of these relations (an isomorphism) to families of slices by way of the Beilinson--Drinfeld Grassmannian. 
Essential to \cite{mirkovic2007geometric}, this space is used to define a fusion (aka convolution) product on sheaves (representations) but can also be used to define a fusion product of MV cycles (bases). 
% 
This product structure makes the vector space spanned by the MV cycles into a commutative algebra isomorphic to the ring of functions on the unipotent subgroup (conjectured in \cite{anderson2003polytope} and proved in \cite{baumann2019mirkovic}). % conjectured by JEA and proved by BKK
% 
To quote from Mirkovi\'c and Vilonen:
\begin{quotation}
    However, we make crucial use of an idea of Drinfeld, going back to around 1990. He discovered an elegant way of obtaining the commutativity constraint by interpreting the convolution product of sheaves as a ``fusion'' product.
\end{quotation}
% 
[Liaison]

The purpose of this paper is to give a conceptually elementary way to compute this product in type $A$. We do not know how fruitful this work might be in the larger landscape of geometric representation theory as it is plagued by two major limitations: it does not readily generalize outside of type $A$, and it is as yet computationally expensive. 
% 
Moreover, it is not clear why one should care about the fusion product on bases. \emoji{imp} \emoji{grin} \emoji{worried} \emoji{monocle-face} \emoji{skull}

\section{Players}

\subsection{Rings and discs}

Set $\cO = \cO_0 = \CC\xt$ and $\cK = \cK_0 = \CC\xT$.
% TODO: I rearranged this sentence, because starting it with 'for any other' presupposes we have introduced s 
%  For any other 
We will also consider $\cO_s = \CC\xt[t-s]$ and its fraction field $\cK_s=\CC\xT[t-s]$, for any 
$ s \in \CC\setminus\{0\} $, as well as $ \Oinf = \CC\xt[t^{-1}] $ and $\Kinf = \CC\xT[t^{-1}]$. For any $ s \in \PP = \PP^1$, 
% TODO: Identifying $s\in\CC$ and the line $s\in\PP$ containing it right?
$ \cO_s$ is the completion of the local ring $ \mathcal O_{\PP, s} $ and thus the formal spectrum of $ \cO_s$ is the formal neighbourhood $ D_s$ of $ s $ (also called the formal disc centered at $ s$).  Similarly the formal spectrum of the field $\cK_s$ is the deleted formal neighbourhood (or punctured disc), denoted $ D_s^\times$.

Note that for $ s \in \CC $, we have an obvious isomorphisms  $\cO_s \cong \cO$ and $\cK_s\cong\cK$ taking $ t-s $ to $ t$. 

\subsection{Groups}
Let $H $ be an algebraic group over $ \CC $.  We will be interested in $ H(R)$ where $ R $ is a $\CC$-algebra, for example $R = \CC[t], \cO_s$, etc.  
% 
% \acom{random question, I know $H(R)$ is supposed to mean functor of points of the scheme $H$ but can it also be viewed as a base change from $\CC$ to $\RR$?}
% \jcom{Usually $ H_R$ is the notation used for the base change, so using $ H(R)$ here is a bit of an abuse, but it is common and harmless.}
% 
Note that evaluation at $ t = s$ provides a group homomorphism $ H(\cO_s) \rightarrow H$.  We denote the kernel of this map by $ H_1(\cO_s)$, often called the  first congruence subgroup.  We will be particularly interested in this construction in the case $ s = \infty$, which gives us the group $ H_1(\Oinf)$.
% \acom{Add $G_1$}
%\acom{TODO: Algebraic groups over rings, i.e.\ the meaning of $G(K)$. The affine space of $m\times m$ matrices which we denote $ M_m$. The first congruence subgroup $G_1$ and why we take $\Oinf$ points instead of $\CC[t]$ points as some of the lit does.}

Fix $G = \GL_m$ and let $T\subset G$ be the maximal torus of diagonal matrices. 
% are there other possibilities?? :S 
The coweight lattice of $ G $ is $ \ZZ^m = \Hom(\CC^\times,T)$.
% TODO: Thank you - resolved!
% \acom{This identification assumes we are taking $\CC$ points of $G$ right? i.e.\ it's really $P(T(\CC)) = \Hom(\CC^\times,T(\CC))$ that's $\cong \ZZ^m$ and for instance $P(T(\CC[x]/(x^2))$ will give sth else?} 
% \jcom{No, the coweight lattice is the set of homomorphisms of algebraic groups from $ \mathbb G_m$ to $ GL_m$.  It is independent of the ring/field of definition.}
% 
The \textbf{dominant coweights} of $ G $ 
% is
are the set of $ \lambda = (\lambda_1, \dots, \lambda_m) $ such that $ \lambda_1 \ge \cdots \ge \lambda_m$. 
% 
A coweight $ \mu = (\mu_1, \dots, \mu_m)$ is called \textbf{effective}, if $ \mu_j \ge 0 $ for all $ j$.
% We also need the notion of effective dominant coweight, 
% \acom{can we conflate or should we be precise and say coweight?} \jcom{Corrected, let's stick to coweights.}
%which means those $ \lambda $ as above with $ \lambda_m \ge 0$.  
% \acom{1. No boldface on this new term. Proceed with selective boldfacing? 2. Since we sometimes need $\Peff$ which contains $P_{++} = \Peff\cap\Pdom$, how about just using the notation $\Peff,\Pdom$ instead of $P_+, P_{++}$ for a change?}
% \jcom{I added the definition of effective coweight.  I actually don't think that we need any special notation $ P, P_+, P_{dom}$ etc !  Later we can just write ``$ \lambda \in \ZZ^m$ a dominant or effective coweight'' etc}
% TODO: it's settled then, no special notation fro coweights!
% 
So an effective dominant coweight $ \lambda $ is a partition of $|\lambda| = \lambda_1 + \cdots + \lambda_m\in\NN$, which we call the size of $ \lambda$.
%We write $P_+$ for the set of dominant coweights, and $P_{++}$ for the set of effective dominant coweights. 
% 
% nicematrix test
% $\begin{bNiceArray}{ccc|c}[margin]
%     \Block{3-3}<\Large>{A} & & & 0 \\
%     & \hspace*{1cm} & & \Vdots \\
%     & & & 0 \\
%     \hline
%     0 & \Cdots& 0 & 0
%     \end{bNiceArray}$
% 
% $\begin{pNiceMatrix}
%     \frac12 & -\frac12 \\
%     \frac13 & \frac14 \\
%     \end{pNiceMatrix}$
% 
% Set $\AA = \AA^1_x$ (the subscript denoting the chosen local coordinate) and fix $s\in \AA - \{0\}$. 
% 

Given a coweight $\mu \in \ZZ^m$ and a point $ s\in \CC$, we define $ (t-s)^\mu$ to be the diagonal matrix 
% with entries $ (t-s)^{\mu_1}, \dots, (t-s)^{\mu_m}$
% \[
%     \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
%         (t-s)^{\mu_1} & 0             & \Cdots & 0 \\
%         0             & (t-s)^{\mu_2} & \Ddots & \Vdots \\
%         % 0 & b & a & \Ddots & & \\
%         \Vdots              & \Ddots        & \Ddots & 0 \\
%         % \Vdots & & & & \\
%         0             & \Cdots        &   0    & (t-s)^{\mu_m}
%     \end{pNiceMatrix}
% \]
\[
\begin{bmatrix}
    (t-s)^{\mu_1} \\
    & (t-s)^{\mu_2} \\ 
    & & \ddots \\
    & & & (t-s)^{\mu_m}
\end{bmatrix} 
\]
which we will view in 
% $\GL_m(K) $ 
$G(K)$ for any ring $ K $ containing $\CC[t]$ in which $t-s$ is invertible (for example $ K = \Ks, \CC(t)$). % A: Think R sb K; changed it 
\acom{Add warning that we will also write $(t-s)^\mu$ for its image in 
% $\GL_m(K)/\GL_m(R)$ 
$G(K)/G(R)$ and we hope it will be clear? Only we haven't introduced this space yet.}
% Resolved
% 2. Change notation $\mu = (\mu_1,\dots,\mu_m)$ since we also have $\mu = \mu_1 + \mu_2$. We could do $\mu = \mu' + \mu''$ or $\mu = a + b$? The notation $\mu^{(i)}$ is already taken to mean sth. Ofc all can be changed. Can also do $\mu = {}_{1}\mu + {}_{2}\mu$. \emoji{sweat-smile}}


We will also be interested in the affine space $ M_m$ of $m\times m$ matrices.  Note that for any $ \CC$-algebra $ R $, 
% $\GL_m(R) $ 
$G(R)$ consists of those matrices $ g \in M_m(R) $ whose determinant is invertible in $ R$. Thus for example $ (t-s)^\mu\in M_m(\CC[t])$ for all effective $ \mu \in \ZZ^m$ but 
% $ t^\lambda \in \GL_m(\CC[t]) $ 
$(t-s)^\mu\in G(\CC[t])$ if and only if $ \mu = 0 $.
% 
\acom{maybe add that it's the same for $G(\cO)$.}

\subsection{Lattices}
\label{ss:lat}
We will use the lattice model for the affine Grassmannian, so it is useful to recall the following definition. Let $ R \subset K$ be two $\CC$-algebras (usually, but not always, $K$ will be a field). 
% \acom{When is $K$ not a field?}). Let $ m \in \mathbb N$ \acom{we have already fixed $m$?} and; e.g. K = C[t,t^{-1}] is not a field!!! 
Consider $ K^m $ as a $K$-module. 
% \acom{left or right, doesn't matter? I think we are in a situation where they are the same, i.e.\ our rings are commutative.} 
By restriction $ K^m$ can be viewed as an $R$-module.  An \textbf{$R$-lattice} in $K^m$ 
% \acom{what is $M$? should be $K^m$?}; bc Joel was thinking about arbitrary free modules 
is an $R$-submodule $ L \subset K^m$ 
% such that $ L $ 
which is a free $R$-module of rank $ m $ and satisfies $ L \otimes_R K = K^m $. Equivalently, $ L = \Sp_R(v_1, \dots, v_m)$ where $v_1, \dots, v_m$ are free generators of $K^m$. 
% 
\begin{comment}
\jcom{Maybe need that both $K$ and $R$ are integral domains and have the same field of fractions?} 
\acom{Roger has proved that what is written is ok.}
\rcom{Here's my argument: If $L \subset K^m$ is a free rank $m$ $R$-module and $L \otimes K = K^m$, then it's clear we that $L$ is generated by $m$ generators $v_1,\dots,v_m$ over $R$. Since $L\otimes K = K^m$, these also generate $K^m$ over $K$. Then we have a surjective map $K^m \twoheadrightarrow K^m$ given by sending a basis of $K^m$ to the generators $v_1,\dots,v_m$. Since any surjective endomorphism of finite $K$-modules is injective, the $v_1,\dots,v_m$ form a basis. I think the only thing that is needed is that the rings are commutative.}
\jcom{Great.}
\end{comment} 

% The trivial lattice is 
% No more: L_0(R) = 
$ R^m \subset K^m $ is called the standard lattice. 
% We will abuse notation and denote $L_0(R)$ simply $L_0$ in the hope that the ring $R$ will be understood from the context. 
The group $\GL_m(K) $ acts transitively on the set of $R$-lattices in $K^m$, thus giving a bijection between 
% the set of such lattices 
this set and $\GL_m(K)/\GL_m(R)$, since $ \GL_m(R) $ is the stabilizer of the standard lattice. 
%\acom{For example, we will frequently make use of the lattice $L_\lambda$ corresponding to the point $t^\lambda\in\Gr$ given $\lambda\in P$. If $e_1,\dots,e_m$ denote the standard basis elements of 
% $L_0(\cO)$ then 
%$\cO^m$ then 
%$$ 
%L_\lambda = \Sp_\cO(t^{\lambda_1}e_1, \dots, t^{\lambda_m}e_m)\,. 
%$$}


We will be particularly interested in $\CC[t]$-lattices in $ \CC(t)^m$. 
% TODO: 
% \acom{Wait, I thought $\CC(t)$ is used in the Ran space version of the BD Gr.}
Given such a lattice $ L $ and a point $ a \in \CC$, the \textbf{specialization} of $ L $ at $ a $ is defined as
$$
% L(a) := L \otimes_{\CC[t]} \CC\xt[t-a] \subset \CC\xT[t-a]^m\,.
L(a) := L \otimes_{\CC[t]} \cO_a, 
% \text{ a lattice in } \cK_a^m.  
$$
a lattice in $\cK_a^m$. 
If 
% $ L(a) = L_0(\cO_a)$ 
$L(a) = \cO_a^m$ then $L$ is said to be \textbf{trivial at $a$}. 
% We say that $ L $ is trivial at $ a $ if $ L(a) = \cO_a^m$. 
% \acom{Do we want to employ the notation $\cO_a$ and $\cK_a$ here?}
For example, since $t-s $ is invertible in $ \cO_a $ if $ a \ne s $, the lattice $(t-s)^{-1}\CC[t] \subset \CC(t)$ is trivial at any $a\ne s$. 
% \acom{it is a little bit uneven to have notation for $\cO^m$ but not for $\cO_a^m$?}

% \acom{Should we also mention our interest in $\cO$-lattices in $\cK^m$?}
% \jcom{I don't think that it is necessary.  I just put the part about $ \CC(t)$ to set up the notation for this specialization.}


% \jcom{I wasn't sure of the notation to use here, but I wanted something which didn't conflict with $ L_0$.}

% \acom{I think that MVy use this notation also: }

%\includegraphics[width=\textwidth]{img/Capture d’écran, le 2021-02-17 à 13.03.10.png}}

\subsection{Affine Grassmannians} %  actors
% To map thick Gr to C is to map a lattice to the points where it isn't trivialzable; doesn't make sense to ask for poles 
% G(C(t))/G(C[t]) for example lives over the Ran space, finitely many eigenvalues; positive part of thick contains this guy as image 
Set $\AA = \AA^1$ viewed as the subset $\PP\setminus \{\infty\}$ of $\PP$. Let $\Vtriv$ denote the trivial rank $ m $ vector bundle. 
% 
We now define various versions of ``the'' affine Grassmannian which will play an important r\^ole in this paper.

\begin{definition}
     The \textbf{ordinary affine Grassmannian} $\Gr = G(\cK)/G(\cO)$.
\end{definition}    
    It is described in modular terms by 
    $$
     \Gr = 
    \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_0 $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_0^\times $} \right\} \,. 
    $$

    We also have a lattice description 
    $$ 
    \Gr = \left\{ L \subset \cK^m : \text{ $L$ is a $\cO$-lattice} \right\}\,.
    $$
    We obtain a lattice from a pair $ (V,\varphi) $ by setting $ L = \Gamma(D_0, V)$ which is embedded into $ \cK^m = \Gamma(D_0^\times, \Vtriv)$ using $ \varphi$.  
    On the other hand, to get a lattice from the group-theoretic description $ G(\cK) / G(\cO) $ we set $ L = g \cO^m$ for $ g \in G(\cK)$.
    % 
\begin{definition}
 For any $ s \in \CC $, 
    the \textbf{ordinary affine Grassmannian} $\Gr_s = G(\cK_s)/G(\cO_s)$ \textbf{at} $ s $. 
\end{definition}    
As above, we have modular and lattice descriptions:
\begin{gather*}
    \Gr_s = 
    \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_s $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D^\times_s $} \right\}\,, \\
    \Gr_s = \left\{ L \subset \cK_s^m : \text{ $L$ is a $\cO_s$-lattice} \right\} \,. 
\end{gather*}
% 
% TODO:
% consider decorating lattice, modular, group-theoretic descriptions? 
% paper too short for it, ok. 
% but it's a little weird how we do 
% $$ X = A \\ X = Y $$ 
% Hmm.. maybe it's alright
% I was going to suggest we do $$ X = A \\ = B $$ instead 
% 
\begin{definition}
The \textbf{thick affine Grassmannian} $\Grth = G(\Kinf)/G(\CC[t])$.
\end{definition}
Again, we have the following modular and lattice descriptions:
\begin{gather*}
        \Grth = 
        \left\{ (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ \PP $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_\infty $ } \right\}\,, \\
\Grth = \left\{ L : L \subset  \Kinf^m \text{ is a $\CC[t]$-lattice} \right\}\,.
    % \CC\xT[t^{-1}]^m 
\end{gather*}
% \jcom{Maybe write $\Kinf$ for $ \CC\xT[t^{-1}] $?  Also maybe change the ``thick'' notation''.} \acom{Since $\sf th$ is also the first two letters of ``thin''?} \jcom{That's a good point!  Maybe we could use some kind of bold  Gr (``thick'' letters) or maybe that would be too confusing.} \acom{Ok, it's tentative.}
% 
\begin{definition} 
    The two point \textbf{Beilinson--Drinfeld Grassmannian} 
    $$\pi : \Grbd\to \AA$$ 
    with one point fixed at 0, and the second point $s\in\AA$ varying.
\end{definition}
It is described in modular terms by 
$$
    \Grbd = 
    \left\{ 
        (V,\varphi,s) : V\text{ is a rank $m$ vector bundle on }\PP, \varphi : V \xrightarrow{\sim} \Vtriv \text{ on } \PP \setminus \{0, s\}  
    \right\}\,. 
$$
The fibre of $\Grbd \to \AA$ over $ s \in \AA $ will be denoted $ \Gr_{0,s} $ and is given by
% \rcom{why do we have the $s\ne 0$ restriction here} \jcom{it would work when $ s = 0$, but it looks a bit strange there.}
% 
$$ 
G(\CC[t, t^{-1}, (t-s)^{-1}]/G(\CC[t])\,. % := \pi^{-1}(s) 
$$
% \acom{What is the modular description of the fibre?} \jcom{That is written above (just fix $ s$).}
% 
% \acom{0 in notation is kinda redundant? But we can't take it away because we already have a $\Gr_s$ and it is something else?}
% \jcom{Yes, that is one reason.  But also, there was a comment of yours before in the file, saying that you sometimes got confused that one point was fixed at 0, so this is a good notation to remind us (and the reader).  For the same reason, maybe we should write $ \Gr_{0, \AA}$ instead of $ \Gr_\AA$.}
% \acom{lol}
We also have the lattice descriptions:
% 
\begin{gather*}
\Grbd = 
\{ (L,s) :
L \subset  \CC(t)^m\text{ is a $\CC[t]$-lattice trivial at any }a \ne 0, s \}\,, \\
% , $L$ is 
    \Gr_{0,s} = 
\{ L : L \subset  \CC[t,t^{-1},(t-s)^{-1}]^m \text{ is a $\CC[t]$-lattice} \}\,.
\end{gather*}
% 
\begin{definition} 
The \textbf{positive parts} of $\Gr $ and $\Grth$, resp., are defined by 
$$
    \Gr^+ = \left(M_m(\cO) \cap G(\cK)\right) / G(\cO) \text{ and }      
    {\Grth}^+ = \left(M_m(\CC[t]) \cap G(\Kinf)\right) / G(\CC[t])\,. 
$$
\end{definition}
In modular terms, $\Gr^+$ (resp.\ ${\Grth}^+$) is the set of those $ (V, \varphi)$ where $ \varphi : V \rightarrow \Vtriv $ extends to an inclusion of coherent sheaves over $ D_0 $ (resp.\ over $ \PP$).

In lattice terms, $ \Gr^+$ (resp.\ ${\Grth}^+$) contains those lattices $L$ which are contained in the appropriate standard lattice. 
% \acom{previously called ``trivial''} \jcom{I changed it so that $ R^m $ is called the standard lattice.  But I would like to leave the terminology ``trivial at $ a$'' is that ok, or should we write ``standard at $a$''.} 
% \acom{Oh, I don't know.. is trivialized at $a$? is standardized at $a$? Maybe standard at $a$ is better.. Man I never thought about this interchangeable lingo.} 
% lattice $ L_0(\cO)$ (resp. $L_0(\CC[t])$). % , i.e. $ L \subseteq L_0$.

\subsection{Relations between different affine Grassmannians}
These different versions of the affine Grassmannian are related as follows.  

\begin{enumerate}
    % TODO: I think it makes more sense to put the first point at the end; it just relates (fibres of) the BD Gr to the ordinary Gr
    % \item There is an isomorphism $ \Gr_s \cong \Gr $ coming from the isomorphism $ \Ks \cong \cK$.
    \item 
    %We have a map from the two point Beilinson--Drinfeld affine Grassmannian to the thick affine Grassmannian $ \Grbd \rightarrow \Grth $.
    % $$ \Grbd \rightarrow \Grth $$ 
    We define a map $ \Grbd \rightarrow \Grth $ by 
    % \acom{Don't we also want to restrict $V$} 
    % Joel says nope, restriction of vector bundle corresponds to specialization of lattice; here the map is inclusion  
    $$
    (V,\varphi,s)\mapsto (V, \varphi \big|_{D_\infty})\,.
    $$
    % 
    % It is given in modular terms by restricting the trivialization to $ D_\infty$. \acom{Does it mean $(V,\varphi,s)\mapsto (V\big|_{D_\infty},\varphi\big|_{D_\infty})$}
    In group-theoretic terms, on the fibre over $ s $, this is the inclusion
    $$
    G(\CC[t, t^{-1}, (t-s)^{-1}])/ G(\CC[t]) \rightarrow G(\Kinf)/G(\CC[t])\,. 
    $$
    % \acom{Permission to update notation and replace $\CC\xT[t^{-1}]$ by $\Kinf$ as per your suggestion Joel?}
    % \jcom{Permission granted!}
    % \acom{replaced}
    % 
    Finally, it is given in lattice terms on the fibre over $s$ as the identity on the lattice $(L,s)\mapsto L$ and using the inclusion $\CC[t, t^{-1}, (t-s)^{-1}]^m \rightarrow \Kinf^m$ on the ambient spaces.
    \item The fibres $ \Gr_{0,s}$ of $ \Grbd \rightarrow \AA$ can be described as % are given as follows
    \begin{equation*}
    % \label{eq:grbdgrs}
    \Gr_{0,s} \cong 
    \begin{cases} 
        \Gr \times \Gr_s & s \ne 0 \\
        \Gr              & s = 0\,.
    \end{cases}
    \end{equation*}
    This isomorphism is given in the modular realization by restricting the vector bundle and trivialization.    
    
    Suppose that $ s \ne 0 $.  In the lattice realization, this is given by forming the specializations 
    $$
    L \mapsto (L(0), L(s))\,.
    $$
    Note that if $ L = g \CC[t]^m$ for $ g \in G([t,t^{-1}, (t-s)^{-1}])$, then 
    $$
        (L(0), L(s)) = (g \cO^m, g\cO^m_s)\,. 
    $$
    % 
    \jcom{I restated this again.  I think that it is simpler and more clear like this.  And I think that this is the fact that we end up using later.}
% 
% 
% in the $ s \ne 0 $ case.
% \jcom{Maybe we should introduce some notation for this isomorphism.}
% \acom{Ok. Question: how is the isomorphism given at $s=0$?}
% \acom{Ok. TODO. Also write this in group theoretic terms. Probably $[g] \mapsto ([g],[g])$ is ok. Since $g\CC[t]\otimes_{\CC[t]}\cO_s \cong g\cO_s $ for some reason, where $g\in\CC[t,t^{-1},(t-s)^{-1}]$ can be viewed as $g\in\cK_s$.}
% 
%    Finally, in group-theoretic terms, it is given (on representatives) by $g\mapsto (g,g)$. \acom{Reinstated the map. Choosing to supress the coset in our notation makes it potentially ambiguous. Maybe we can say inclusion $\times$ inclusion? In any case we should say something. Else, what are we referring to when we say ``This is compatible'' in the next line?}
%    This is compatible with the lattice description since for $ g \in G(\CC[t, t^{-1}, (t-s)^{-1}]) $, we have
%    $$
%    \left( g \CC[t]^m \right) \otimes_{\CC[t]} \cO \cong g \cO^m
%    $$
%    and similarly for $ L(s)$.
% 
    In the $ s = 0 $ case, the isomorphism is described in the same way, except that we just need to form $ L(0)$.
    \item There is an isomorphism $ \Gr_s \cong \Gr $ coming from the isomorphism $ \Ks \cong \cK$. 
    % 
    %    \jcom{I tried to answer your question, but didn't explain it well.  When $ s = 0$, the map is the same except that $ L(0) = L(s) $ , so we only need $L(0)$.}
\end{enumerate}

\subsection{The fusion construction}
The following construction will be very important in this paper.  

Let $ \Gr_{0, \CC^\times} $ denote the preimage of $\AA \setminus \{0\}$ under the map $\Gr_{0,\AA} \rightarrow \AA $.  Composing the isomorphisms relating $\Grbd$ to $\Gr_s$ and $\Gr_s$ to $\Gr$, (items (2) and (3) above), we obtain an isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr$.

These isomorphisms glue together to a projection map
$$
\tau : \Gr_{0, \CC^\times} \rightarrow \Gr \times \Gr \, .
$$ 
(This $\tau$ matches the map $ \tau $ of \cite{mirkovic2007geometric}, except that we are working over $ \{0 \} \times \CC^\times$ instead of the complement of the diagonal in $ \CC^2 $.)


Let $ X_1, X_2 \subset \Gr$ be two subschemes, and consider the subscheme $ \tau^{-1}(X_1 \times X_2) \subset \Gr_{0, \CC^\times} $.
% 
% \jcom{I changed this a bit in order to match Mirkovi\'c--Vilonen better.}
% \acom{We say ``we define the subscheme..'' but what is there to define? Should it say ``we consider'' maybe?}
% Via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $,  for each $ s \ne 0$, we obtain a subscheme $ X_1 \times X_2 \times \AA^\times $  of $\Grbd$.  
% 
We define $ X_1 \ast_{\AA} X_2 $ to be the scheme-theoretic closure of $  \tau^{-1}(X_1 \times X_2) $ inside of $ \Grbd $.  
Next, we consider the projection $ \pi: \Grbd \rightarrow \AA $, and define 
% $ X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) \subset \Gr_{0,0} $, 
\begin{equation*}
    % \label{deservesdisplay}
    X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) 
\end{equation*}
which we call the \textbf{fusion} of $ X_1 $ and $X_2$. It is contained in $\Gr_{0,0}$ but regarded as a subscheme of $ \Gr $ using {the isomorphism} $\Gr_{0,0} \cong \Gr $. 
% 
By construction, 
% \acom{technically, there were two given above, so *these* constructions?} 
$X_1 \ast_{\AA} X_2 $ is a flat family over $\AA$ whose fibre away from $0$ is $X_1 \times X_2$ and whose fibre over $ 0$ is $ X_1 \circ X_2$. 

%\acom{Question abt general fact: why is scheme-theoretic closure of product flat?} \jcom{See Hartshorne Proposition III.9.8.  I just taught this on the last day of my algebraic geometry class!} \acom{Cool! That Proposition is a bit more general too..}

% \acom{Repeat question: what is this iso? Roger says maybe it's in MV?}
% \jcom{Now answered above.}

%\jcom{I thought that it would be good to introduce this operation now, since we will use it often in the paper and it is the most convenient way to define $\Gr^{\lambda_1, \lambda_2}$. I don't know what notation we should use.}

%\acom{Roger and I have been using $\ast$ to denote fusion. Do we need a notation for the scheme-theoretic closure of $X_1\times X_2\times\AA^\times$? If not then I like $X_1\ast X_2 = \overline{X_1 \times X_2 \times \AA^\times}\cap \pi^{-1}(0)$.}
%\jcom{I think that we will need notations for both things, but I'm happy to use $ \ast$ for the central fibre and use something else for the whole family.} \acom{Ok, I have changed it to notation from Roger's thesis/AK. Only now I am confused as to what is being called fusion---why do we not take the top dimensional subscheme?}
%\jcom{I guess that there are two things that one could mean by fusion of MV cycles.  This scheme $ X_1 \circ X_2$ or the expression which appear in Theorem 7.11 of mvbasis, which is a linear combination of the top-dimensional components of this scheme with multiplicites.  In the general context of this section ($X_1, X_2$ are not necessarily MV cycles), then I guess it is best to talk about the subscheme.  Maybe we can call the expression appearing Theorem 7.11 the ``numerical fusion'' of MV cycles.  By the way, somewhere in this paper we should explain Theorem 7.11 and say that this gives us motivation to study fusion of MV cycles.}
%\acom{Sure! It is quoted at the end, in \Cref{s:denouement} as an application. Addendum: for the numerical fusion I suggest simply $[Z_1][Z_2]$. This is what I will use in my talk. }

\subsection{Some subvarieties of affine Grassmannians}
Now fix effective dominant coweights $\lambda', \lambda'' $ of sizes $N',N''$  and their sum $ \lambda = \lambda' + \lambda'' $ of size $ N = N'+N''$. 
% Coweights define elements in the various affine Grassmannians which we consider. Warning: We will write only coset representatives for elements of the various Grassmannians; we hope that it is evident which right cosets they represent. 
% the right cosets which they represent are clear from the context.
We consider the following subschemes of the affine Grassmannians.
% 
\begin{definition}
    The \textbf{spherical Schubert cell} $\Gr^\lambda = G(\cO) t^\lambda$ and its closure 
    $ \overline\Gr^\lambda = \bigcup_{\gamma \le \lambda} \Gr^{\gamma} $ a \textbf{spherical Schubert variety}.  
\end{definition}
    % \acom{do we prefer $\overline{\Gr}^\lambda$ to $\overline{\Gr^\lambda}$? I am for the former! Though it is less accurate??} \jcom{I'm not sure which is better.  I was ``testing'' it out.}

    % 
\begin{definition} 
    The \textbf{fusion of two spherical Schubert varieties} 
    $$ 
    \overline{\Gr}^{\lambda', \lambda''}_{0, \AA} = \overline{\Gr}^{\lambda'} \ast_\AA \overline{\Gr}^{\lambda''} \rightarrow \AA \,.
    $$
\end{definition}
% \acom{I guess that it is good to have the notation here, in place of $\overline{\overline\Gr^{\lambda'}\times\overline\Gr^{\lambda''}\times\AA^\times}$..}
%\acom{should we also subsript this family with $0,\AA$?}
By construction the fibre $ \overline{\Gr}_{0,s}^{\lambda', \lambda''} $ of $\overline{\Gr}^{\lambda', \lambda''}_{0, \AA}$ for $ s \ne 0 $ is $ \overline{\Gr}^{\lambda'} \times \overline{\Gr}^{\lambda''}$.  
% By \cite[Proposition 3.1.14]{zhu2016introduction},
% Zhu's theorem (\jcom{need ref} \acom{pretty sure it's }), 
% the fibre over $ 0 $ is $ \overline{\Gr}^{\lambda' + \lambda''}$.
Over $s=0$ the fibre is reduced and equal to $ \overline{\Gr}^{\lambda' + \lambda''}$ by 
% a theorem of Zhu 
\cite[Proposition 3.1.14]{zhu2016introduction}. 

% Suppose that $ s \ne 0$.  
Moreover, if $s\ne0$, then in the fibre $\overline{\Gr}_{0,s}^{\lambda', \lambda''}$ we have the open locus $ \Gr_{0,s}^{\lambda', \lambda''}$ coming from transporting $ \Gr^{\lambda'} \times \Gr^{\lambda''} $ via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $.  This open locus is a $ G(\CC[t])$-orbit
% .  The lattice description of $ \Gr_{0,s}^{\lambda', \lambda''}$ 
whose lattice description is given in \Cref{le:Grl1l2} below.
% \acom{Why $G(\CC[t])$. Because we are on the left side!}

Because we consider effective dominant weight, the spherical Schubert varieties and their fusions lie in the positive part of the (thick) affine Grassmannian.

\begin{lemma}
    The map $ \Gr_{0, \AA} \rightarrow \Grth$ restricts to a map $ \overline{\Gr}^{\lambda', \lambda''}_{0, \AA} \rightarrow \Grth^+$.
\end{lemma}
\begin{proof}
    Let $ s \ne 0 $. Since $ \lambda', \lambda'' $ are effective, $ t^{\lambda'}, (t-s)^{\lambda''} \in \Grth^+ $.  Since $\Grth^+$ is closed and invariant under the action of $ G(\CC[t])$,  $ \overline{\Gr}_{0,s}^{\lambda', \lambda''} \subset \Grth^+$.
    
    For the $ s = 0$ fibre, a similar reasoning applies (or we can simply conclude by taking closure).
\end{proof}
    % \acom{why ``and so''? bc orbits are connected?}
    % \jcom{I turned this into a Lemma.}

%As above, since $ \lambda', \lambda'' $ are effective, the map $ \acom{What is ``as above''? Also, we have only described the open loci in fibres over $s\ne0$.}

Now fix two effective coweights $ \mu', \mu'' \in \mathbb N^m$ of size $N',N''$ resp. such that $ \mu = \mu' + \mu'' $ is dominant. (We do not require $ \mu', \mu'' $ to be dominant.) 
Using $ \mu, \mu', \mu''$, we define the following subschemes.

\begin{definition}
    The \textbf{Kazhdan--Lusztig slice} $\cW_\mu = G_1(\Oinf) t^\mu \subset \Grth $.  
\end{definition}
Again, since $\mu $ is effective, we see that $ \cW_\mu $ lies inside the positive thick Grassmannian $ \Grth^+$.
% 

In modular terms, $\cW_\mu$ corresponds to the locus of those $ (V, \varphi)$ such that $ V $ is isomorphic to the trivial vector bundle on $ \PP$ and such that $ \varphi$ preserves the {Harder--Narasimhan filtration of $V$ at $ \infty$}.  The lattice description of $ \cW_\mu $ is given in Lemma \ref{le:Wmu} below. 
% \acom{que est-ce?} \jcom{It's not very relevant for this paper.  Here is an explanation: every vector bundle on $\PP$ (or maybe any curve) has a special filtration called the HN--filtration; it is a partial flag of subbundles of type determined by $ \mu$.  We require that $ \varphi$ takes this filtration to the standard partial flag at the fibre over $ \infty$.  Maybe ``preserves'' is not the best word above.}
% A: Ok thanks!
% 
\begin{definition} The \textbf{semi-infinite orbit} 
$$ S^\mu = N_-(\cK)t^\mu \subset \Gr $$ 
and the \textbf{fusion of  semi-infinite orbits} 
$$ S^{\mu', \mu''}_{0,\AA} = S^{\mu'} \ast_\AA S^{\mu''} \to \AA \,.$$  
\end{definition}
If $ s \ne 0 $, then the fibre $S^{\mu', \mu''}_{0,s} $ is given by
$$
% S^{\mu', \mu''}_{0,s} = 
N_-(\CC[t, t^{-1}, (t-s)^{-1}])t^{\mu'} (t-s)^{\mu''} G(\CC[t])\subset G(\CC[t,t^{-1}, (t-s)^{-1}]) / G(\CC[t]) \,. % \G(\CC[t])
$$
% TODO: resolve
% \jcom{Maybe we should introduce the $ L_{\mu', \mu''} $ notation, but I'm not sure it is necessary.}
On the other hand, if $ s = 0$, then the fibre $S^{\mu', \mu''}_{0,0} $ equals $ S^{\mu' + \mu''}$. 
% TODO: resolve
% \acom{changed $\mu$ to $\mu' + \mu''$ since we wrote it out for $\overline{\Gr}_{0,\AA}^{\lambda', \lambda''} $}
% 
\jcom{Still need a reference for why this central fibre is reduced.}
\acom{Two definitions in one. Why does fusion of spherical schubs get preferential treatment \emoji{astonished}}
    

\section{Matrices}

\subsection{Adjoint orbits and their deformations}
 We now consider some subvarieties of the space of $ N\times N$ matrices, again using the $ \lambda, \lambda', \lambda'', \mu, \mu', \mu''$ defined in the previous section.  Recall that $\lambda, \mu $ are partitions of $ N$.

 For $ s \in \CC$ 
    % and $\lambda\in P_{++}$
    we write $ J_{s,\lambda}$ for the Jordan form matrix with eigenvalue $ s$ and Jordan blocks of sizes $ \lambda_1, \dots, \lambda_m$. 
    % \acom{$s\in\AA$?}
    % 
    
\begin{definition}
The nilpotent orbit $ \OO^\lambda \subset M_N(\CC)$ of matrices conjugate to $ J_{0,\lambda}$.
\end{definition}  Its closure is $ \overline{\OO}^\lambda = \bigcup_{\gamma \le \lambda} \OO^{\gamma}$.
    % 
\begin{definition}
    For $ s \in \CC, s \ne 0$, the adjoint orbit $ \OO^{\lambda', \lambda''}_{0,s}$ of matrices conjugate to $ J_{0,\lambda'} \oplus J_{s,\lambda''}$.
\end{definition}  
A linear operator $ T : V \rightarrow V $ on an $N$-dimensional vector space $V$ is said to have Jordan type $((0,\lambda'), (s,\lambda''))$ if its matrix representatives lie in $ \OO^{\lambda', \lambda''}_{0,s}$.
% 

The closure $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ is $\bigcup_{\gamma' \le \lambda', \gamma'' \le \lambda''} \OO^{\gamma', \gamma''}_{0,s}$.     
     
We recall that these adjoint orbit closures are given by rank conditions.  More precisely we have
$$
    \overline{\OO}^\lambda = \{ A \in M_N(\CC) : \rk A^i \le N - \#\text{~boxes in first $i$ columns of }\lambda \}
$$
and
\begin{equation} 
\label{eq:ranks}
\begin{split}
    % 
    \overline{\OO}^{\lambda', \lambda''}_{0,s} = \{ A \in M_N(\CC) : \rk A^i &\le N - 
    \#\text{~boxes in first $i$ columns of }\lambda' \\
    \rk (A-s)^i &\le N - \#\text{~boxes in first $i$ columns of }\lambda'' \}
\end{split}
\end{equation}
%  
\begin{proposition} \label{prop:adjoint}
    There exists a flat family  $\overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow \AA$ whose fibre over $s \in \AA$ is reduced and given by
       $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ if $s \ne 0 $ and
$        \overline{\OO}^\lambda $ if $ s = 0$.
\end{proposition}
% 
In order to prove this result, we will need to recall some results from Eisenbud--Saltman \cite{eisenbud1989rank}.
% 

Let $ r $ be a decreasing, concave, non-negative function with $r(0) = N$ (called a rank function).  Let $k $ be maximal such that $ r(k) \ne 0 $.
\jcom{I don't know if it is really necessary to introduce this $ k$.  Later it will be the number of columns of $ \lambda$, but we could just regard $ \lambda$ to have some columns of size 0.} \acom{And make it $N$? Was $k = m$ replaced because $m$ is already used?} \jcom{Yes and yes} \acom{I like the idea of making it $N$}

Let $ W$ be a subspace of $ \AA^k$. Eisenbud \& Saltman define a flat family $ X_{r,W} \rightarrow W$, whose fibres are reduced and defined by rank conditions. We will now describe these fibres.

Let $ z \in \AA^k$. Some of the coordinates of $z$ may be equal; we introduce some data to keep track of these equalities. There exist $ 0=k_0< k_1< \dots < k_r = k $, distinct $ s_1, \dots, s_r \in \CC$ and a permutation  $ p $ of $ \{1, \dots, k\}$ such that 
$$ s_j = z_{p(k_{j-1}+1)} = \cdots = z_{p(k_j)}\,. $$ We require that $ p $ is of minimal length, given the choice of $ s_1, \dots, s_r$.  The data $s_\bullet, p, k_\bullet$ is unique up to a permutation of $ \{1, \dots, k\}$.

%\jcom{Eisenbud--Saltman don't introduce this $ s_j$ notation, but I find it helpful.}

%\acom{Agreed. Maybe we mention that the point of $p$ is to group the repeated eigenvalues? It wouldn't be the most obvious thing we point out.}
%\jcom{That is what I was trying to say with ``introduce some data \dots''  Maybe it could be more clear.  I also clarified the uniqueness.}
%\acom{Oh, sorry, I see it. Maybe because it was the start of a new paragraph I didn't connect `data' and `ranks' at first glance.}

Next for $ j = 1, \dots r $ and $ i = 1, \dots, k_{j+1} - k_j$, we define
$$
    % r_{z,j}(i):= N - r(p(m_j)) + r(p(m_j + i)) 
    r_{z,j}(i):= N + \sum_{a = 1}^i r(p(k_j + a)) - r(p(k_j +a) - 1)  \, . 
$$
\jcom{This is the same as what ES denote $r(i,j)$ except the roles of $ i, j$ swapped.  We could swap them back.}
\jcom{I fixed it now.  I didn't want to use ES's $a_i$.} 
\acom{\emoji{+1}}

% 
By \cite[Corollary 2.2]{eisenbud1989rank}, the fibre of the flat family $ X_{r,W} $ over $ z$ is given by
\begin{equation} \label{eq:ESfibre}
X_{r,z} = \{ A \in M_N(\CC) :\rk (A-s_j)^i\le r_{z,j}(i) \text{ for all } i , j \text{ as above}\}.
\end{equation}

We will now apply these ideas to prove our Proposition \ref{prop:adjoint}.

\begin{proof}

%\begin{lemma}
%    \label{lem:es}
%    Let $\lambda = \lambda' + \lambda''$ be partitions. There exists a permutation of the columns of $\lambda$ such that the first $\lambda'_1$ columns are precisely the columns of $\lambda'$ and the last $\lambda''_1$ columns are precisely the columns of $\lambda''$. 
%\end{lemma}
%Recall that $\lambda' + \lambda'' = \lambda$ is a partition of $N$. 

Define 
$$r(i) = N - \#\text{~boxes in first $i$ columns of }\lambda\,.$$
It is easy to see that this is a rank function, with $ k = \lambda_1$, the number of columns of $ \lambda$.
%\begin{lemma}
%   The function $r$
%    defined by $r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$ 
%is a rank function. 
%\end{lemma}

Since $\lambda = \lambda' + \lambda''$, there exists a permutation $ p $ of $ \{1, \dots, k\}$ (the columns of $ \lambda$) such that $ p(1), \dots, p(k_1) $ are the columns of $ \lambda'$ and $ p(k_1+1), \dots, p(k)$ are the columns of $ \lambda''$. Here $ k_1 = \lambda'_1$ the number of columns of $ \lambda'$.

For $ s \in \CC$, we define $ z(s) \in \CC^k$, with $ z(s)_{p(j)} = 0 $ for $ j = 1, \dots, k_1 $ and $ z(s)_{p(j)} = s $ for $ j = k_1 + 1, \dots, k$.

For $ s \ne 0$, we see that the equalities in $ z(s) $ give rise to the data of $ r = 2$, $k_1$, $s_1 = 0$, $s_2 = s$, and the permutation $p $.  We also see that
\begin{equation} 
    \label{eq:rcols}
\begin{split}
        r_{z(s), 1}(i) &=  N - \#\text{~boxes in first $i$ columns of }\lambda' \\
        r_{z(s), 2}(i) &=  N - \#\text{~boxes in first $i$ columns of }\lambda'' \,. 
\end{split}
\end{equation}
On the other hand, for $z(0)$, we get that $ r = 1$ and that 
\begin{equation} \label{eq:rcol2}
r_{z(0),1}(i) = r(i) = 
 N - 
     \#\text{~boxes in first $i$ columns of }\lambda\,. 
\end{equation}
%\jcom{Someone should check that I chose the notation correctly to make this work out.}

%\acom{What do we mean by equalities in $z(s)$ reproduce $p$? I thought $z(s)$ was defined by $p$. Also, $r_{z(s),1}(i) = N- r(p(k_1)) + r(p(k_1 + i)) = N - (N-\#\text{boxes in first }p(k_1)\text{ cols} + \cdots)\ne $~\Cref{eq:rcols}. We want to apply $p$ to $\{1,\dots,i\}$ and not just $i$. I guess we could clarify/declare that $r(p(i)) = N - \#$ boxes in columns $p(1),\dots,p(i)$. For a minute I thought it was actually fine, but what we have is: $r_{z(s),j}(i) = N - (\#\text{boxes in cols }1,\dots,p(k_j + i) - \#\text{boxes in cols }1,\dots,p(k_j))$ and what we want is $\dots (\#\text{boxes in cols }p(1),\dots,p(k_j + i) - \#\text{boxes in cols }p(1),\dots,p(k_j + i))$ right?}

%\jcom{I fixed the definition of $ r_{z,j}(i)$, so now it is correct, I believe.  I changed the wording of ``reproduce''. I'm trying to say that $ p$ is used to define $ z(s)$ and the we get $ p $ back from $ z(s)$ as the permutation coming from the equalities.}
%\acom{\emoji{+1}}

% 
Let $ W = \{ z(s) : s \in \CC \}$.  Combining \Cref{eq:ranks,eq:ESfibre,eq:rcols,eq:rcol2} 
% (\ref{eq:ranks}), (\ref{eq:ESfibre}), (\ref{eq:rcols}), (\ref{eq:rcols2}) 
we see that the Eisenbud--Saltman family $ X_{r, W} \rightarrow W $ gives our family $ \overline{\OO}^{\lambda', \lambda''}_{0, \AA}$.
\end{proof}







    
%     The family of adjoint orbits $  whose fibre over $ s \ne 0 $ is $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ and whose \acom{Claim:} fibre over 0 is $\overline{\OO}^\lambda$ (a consequence of Eisenbud and Salten's characterization of ``rank varieties'' as proved in \Cref{cor:es}). \acom{TODO: make into definition to be x-referenced in proof of description of fibres}
    
   % \jcom{We should justify why the 0 fibre is correct scheme-theoretically.} 
    %\acom{does it follow because ``$s$ divides $\det g$'' is a closed condition 
    %in $\AA_s\times \AA_{\det}$? 
    %so the set of $A$ which are conjugate to
    %$J_{0,\lambda'} \oplus J_{s,\lambda''}$
    %by a matrix whose determinant is divisible by $s$ is also closed?}
    %\jcom{I didn't follow your argument. One approach would be to write down the generators for the ideal of $\overline{\OO}^{\lambda', \lambda''}_{0,s} $ and then set $ s =0$.  For example, take $ \lambda' = (1) = \lambda'' $ (I know it is a small example!), then the ideal is generated by $ tr(A) = s$, $det(A) = 0$, so when we take $ s =0$, we get just $ tr(A) = 0, det(A) = 0$.  I am hoping that we can find a reference for this result somewhere.}
    %\acom{I thought that the point would be to show that a dense subset of matrices which are conjugate to $J_{0,\lambda'}\oplus J_{s,\lambda''}$ limit to matrices which are conjugate to $J_\lambda$. In particular, that the conjugating matrix remains invertible in the $s\to0$ limit.}
    
% TODO: delete commented out remark and subcomment!
% \begin{remark}
\begin{comment}
        Base case $m = 1$: by a slight abuse of notation let $\lambda', \lambda_2 \in \NN$. Claim: $J=J_{0,\lambda'} \oplus J_{s,\lambda_2}$ is conjugate to 
        
% $\begin{pNiceArray}{CC|CC}[first-row,last-row=5,first-col,last-col,nullify-dots]
% & C_1 & \Cdots & & C_4 & \\
% L_1 & a_{11} & a_{12} & a_{13} & a_{14} & L_1 \\
% \Vdots & a_{21} & a_{22} & a_{23} & a_{24} & \Vdots \\
% \hline
% & a_{31} & a_{32} & a_{33} & a_{34} & \\
% L_4 & a_{41} & a_{42} & a_{43} & a_{44} & L_4 \\
% & C_1 & \Cdots & & C_4 &
% \end{pNiceArray}$
\[
        A = \begin{bNiceArray}{ccc|ccc}[columns-width = auto,first-row,last-col,code-for-first-row = \color{blue}\scriptstyle\rotate,code-for-last-col = \color{blue}\scriptstyle,nullify-dots,xdots/line-style=loosely dotted]
        1 & \Cdots & \lambda_1 & \lambda_1 + 1 & \Cdots & \lambda_1 + \lambda_2 \\
        0 & 1 & 0 & 0 & 0 & 0 & 1\\
        0 & \Ddots & 1 & 0 & 0 & 0 & \Vdots \\
        0 & 0 & 0 & 1 & 0 & 0 & \lambda_1 \\
        \hline
         & & & s & 1 & 0 & \lambda_1 + 1 \\
         & & & 0 & \Ddots & 1 & \Vdots \\
         & & & 0 & 0 & s & \lambda_1 + \lambda_2 
        \end{bNiceArray} 
\]
\end{comment}
% Denote by $R_i$ the $i$th row of $J$ 
%Let $g = E_{\lambda_2}\cdots E_1$ be the product of elementary matrices $E_i$ which correspond to the elementary row operations 
%$$R_{\lambda_1 + i - 1} \leftarrow R_{\lambda_1 + i-1} + \frac 1s R_{\lambda_1 + i}$$ 
%for $i = 1,\dots,\lambda_2$. Then $gJg^{-1} = A$. For the next part we introduce the notation $A_{\lambda_1+\lambda_2}$ for this matrix $A$. 

%Now let $\lambda_1,\lambda_2$ be arbitrary. Clearly $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to 
%$$\bigoplus_{i=1}^m (J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}})\,.$$ 
%Moreover each block $J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}}$ is conjugate to $A_{\lambda_{1,i}+\lambda_{2,i}}$. Altogether $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to $\bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$ and this latter matrix specializes to the Jordan normal form $J_{0,\lambda_1 + \lambda_2}$ at $s=0$.

% By Tanisaki $A\in \OO^\lambda$ iff the coefficients of $z^m$ in $k\times k$ minors of $zI - A$ for any $m\le p_\lambda(k) - 1$ and for any $k = 1\dots |\lambda|$ vanish. Here $p_\lambda(k) = \lambda_{n-k+1} + \cdots + b_n$ and $k = 1,\dots,n$. So $p_\lambda(k) = \#$ boxes in the last $k$ rows of $\lambda$.

% In particular this result describes the ideal $I_i$ of $N_i\times N_i$ matrices $A_i$ such that $A_i - \delta_{i,2}s$ is conjugate to $J_{\delta_{i,2} s, \lambda_i}$ for $i = 1,2$. Let $A_1,A_2$ be such a pair of matrices. Clearly $A_1 \oplus A_2$ is conjugate to $J_{0,\lambda_1} \oplus J_{s,\lambda_2}$. Let $g$ be such that $g(J_{0,\lambda_1} \oplus J_{s,\lambda_2})g^{-1} = \bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$. 

% If we apply the change of basis $g$ to the ideal $I=I_1 + I_2$ and set $s = 0$ (if possible) do we recover the ideal of $\OO^{\lambda_1 + \lambda_2}$? Let $A^g = g(A_1 \oplus A_2) g^{-1}$...
% \end{remark}
    
\subsection{The Mirkovic--Vybornov slice} 
% 
% Fix the ``$\mu$-numeration'' \((e^1_1,\ldots,e^{\mu_1}_1,\ldots,e^1_m,\ldots,e^{\mu_m}_m)\) of the standard basis of $\CC^N$. 
% \acom{I don't suppose we need this basis now that we're just describing $\TT_\mu$ in words.}
% 
% \begin{definition}
An element of the \textbf{Mirkovi\'c--Vybornov slice} $\TT_\mu$ 
% defined as the set of $A\in M_N(\CC)$ such that 
    % \[
    %     \begin{aligned}
    %         &\text{for all } 1 \le a,s\le m\,,
    %         \text{for all } 1\le b\le \mu_a\,, 1\le t\le \mu_s\,, \\
    %         &\text{if } 1\le t < \mu_s \text{ or } t = \mu_s < b \le \mu_a \\
    %         &\text{then } e^t_s\cdot (A-J_\mu) e^b_a = 0 \,.
    %     \end{aligned}    
    % \]
% \end{definition}
% \jcom{I'm not a huge fan of this description, but I don't know how to write it better.}
% 
% \acom{Agree. Happy to just have the in-words description.}
% 
    % In words, $A$ 
    is a $\mu$ Jordan form matrix plus a $\mu\times\mu$ block matrix with possibly nonzero entries occuring in the first $\min(\mu_i,\mu_j)$ columns of the last row of each $\mu_i\times\mu_j$ block. 
    % $\dim \TT_\mu = N^2 - \dim \OO_\mu$. 
    In pictures, and by example, the elements $A$ for $\mu=(3,2,2,1)$ look like 
    \[
        \left[\begin{BMAT}(e){ccc;cc;c;c}{ccc;cc;c;c}
             & 1 & & & & & \\
             &  & 1 & & & & \\
            A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
              &  & &  & 1 & & \\
              A_{21}^1 & A_{21}^2 & & A_{22}^1 & A_{22}^2 & A_{23}^1 & A_{24}^1 \\
             A_{31}^1 & & & A_{32}^1 & & A_{33}^1 & A_{34}^1 \\
             A_{41}^1 & & & A_{42}^1 & & A_{43}^1 & A_{43}^1
        \end{BMAT}
        \right]\,. 
    \]
    % 

    To each $ A \in \TT_\mu$, we will associate the $m\times m$ matrix 
    of polynomials 
    % $ g(A) = \left( g(A)_{ij} \right)$ 
    $g(A)$
    in $ M_m(\CC[t]) $ whose $(i,j)$th entry is defined as follows.
    \begin{equation}
        \label{eq:mvyofa}
        g(A)_{ij} = 
    \begin{cases} t^{\mu_i} - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i = j \\
            % \text{ if $ i = j$} \\
            - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i \ne j
        %  \text{ if $ i \ne j$ }
    \end{cases}
    \end{equation}
    where $A^k_{ji}$ is the $k$th entry from the left of the last row of the $\mu_j\times\mu_i$ block of $A$. 
    For example,
    \[
        \left[\begin{BMAT}(e){ccc;cc}{ccc;cc} 
            0 & 1 & 0 & 0 & 0\\
            0 & 0 & 1 & 0 & 0\\
            A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2\\
            0 & 0 & 0 & 0 & 1\\
            A_{21}^1 & A_{21}^2 & 0 & A_{22}^1 & A_{22}^2
            \end{BMAT}\right]    
            \mapsto 
            \left[\begin{array}{rr}
                t^{3} - A_{11}^3 t^2 - A_{11}^2 t - A_{11}^1 & -A_{21}^2t - A_{21}^1  \\
                -A_{12}^2 t - A_{12}^1 & t^{2} - A_{22}^2 t - A_{22}^1
            \end{array}\right]\,. 
    \]
    % For example, the $A$ above will define 
    % \acom{the transpose of! TODO. Had script?}
    % \[
    % \begin{bmatrix}
    %     t^3 - A_{11}^1 - A_{11}^2 t - A_{11}^3 t^2 & - A_{12}^1 - A_{12}^2 t & -A_{13}^1 & -A_{14}^1 \\
    %     & t^2 \\
    %     & & t \\
    %     & & & t 
    % \end{bmatrix}    
    % \]
    % 
    % \jcom{I moved the definition here since it will be useful for defining the upper triangular stuff.}
    % \acom{Ok.}
    % 
    \acom{Can add sentence before making next definition? To the effect of ``In $\TT_\mu$ we will be interested in a certain family of block upper-triangular matrices.''}
    \begin{definition} The ``\textbf{upper-triangular}'' \mvy slice $\UU^{\mu', \mu''}_{0,\AA}\rightarrow \AA $, defined by
    $$
    \UU^{\mu', \mu''}_{0,\AA} := \{ (A,s) \in \TT_\mu \times \AA : g(A)_{ii} = t^{\mu'_{i}} (t-s)^{\mu''_{i}}, g(A)_{ij} = 0 \text{ for $ j < i $ }\}\,. 
    $$
    \end{definition}
    So a matrix in $\UU^{\mu', \mu''}_{0,\AA}$ is weakly block upper-triangular and its diagonal blocks are given by the companion matrices for the polynomials $t^{\mu'_{i}} (t-s)^{\mu''_{i}}$ ($i=1,\dots,m$).
    For example, elements of $ \UU_{0,s}^{(1,1,0),(2,1,1)}$ look like 
% 
% $\UU_{0,0}^{(?),(?)}$ entries look like 
% \[
%     \left[\begin{BMAT}(e){ccc;cc}{ccc;cc} 
%         0 & 1 & 0 & 0 & 0\\
%         0 & 0 & 1 & 0 & 0\\
%         0 & 0 & 0 & A_{12}^1 & A_{12}^2\\
%         0 & 0 & 0 & 0 & 1\\
%         0 & 0 & 0 & 0 & 0
%         \end{BMAT}\right]    
%         % \mapsto 
%         % \left[\begin{array}{rr}
%         %     t^{3} & 0 \\
%         %     -A_{12}^2 t - A_{12}^1 & t^{2}
%         % \end{array}\right]
% \]
    \[
        \left[\begin{BMAT}(e){ccc;cc;c}{ccc;cc;c} 
            0 & 1 & 0 & 0 & 0 & 0\\
            0 & 0 & 1 & 0 & 0 & 0\\
            0 & -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 \\
            0 & 0 & 0 & 0 & 1 & 0\\
            0 & 0 & 0 & 0 & s & A_{23}^1\\
            0 & 0 & 0 & 0 & 0 & s
            \end{BMAT}\right] \,.    % \in \UU_{0,s}^{(1,1,0),(2,1,1)}
    \]
    % 
    Note that the fibre $  \UU^{\mu', \mu''}_{0,0}$ is the same as the intersection of $ \TT_\mu $ with the set of strictly upper-triangular matrices. 

    \acom{Mention that any other decomposition $\nu' + \nu''$ of $\mu' + \mu''$ will give a slice $\UU^{\nu',\nu''}_{0,\AA}$ with the same zero fibre.}
    
    % \jcom{Instead of $\TT^+$ maybe a different letter would be best? I'm moved all $\lambda, \mu$ to upper indices for consistency (except for $ \cW_\mu, \TT_\mu$) and also to let us put $ s $ or $ \AA$ in the lower index.  I haven't been very consistent about $ \AA $ vs $ \CC$.  Which do you prefer?}
    % 
    % \acom{Maybe $\UU$ for a different letter? And, I guess we do not need $\AA$ but it is a nice letter to have around. It also serves to distinguish the $\CC$ over which $R,K$ are defined from the $\AA$ over which $\Grbd$ is defined?}
    % \jcom{Sure $\UU$ is good.}
    % \acom{And, sorry, ignore my other comment; it's the same $\CC$ (or $\AA$) nothing to distinguish.}


% TODO: place this better? 
% \acom{The following is a rough note.} 
\begin{comment}
\subsubsection{Detour of rank varieties}
% \begin{theorem}(\cite{eisenbud1989rank})
    % \label{thm:es}
Let $r$ be a decreasing concave non-negative integral function with $r(0) = N$. Such a function is called a rank function. Eisenbud and Saltment associate to a rank function the vareity of $N\times N$ matrices 
% The relevant result from Eisenbud--Saltman seems to be that 
$$X_r = X_{r,0} = \{A:\rk A^i \le r(i) \}$$ 
and show that it is the flat limit of a certain family 
% $$X_{r,\lambda} = \{A : \rk (A-\lambda_i)^j\le r(i,j)\}$$
$$X_{r,z} = \{A : \rk (A-z_i)^j\le r(i,j)\}$$
with $r(i,j)$ determined by $r(i)$.  
% \end{theorem}

\acom{Don't know how much of this setup to recall but here are the parts that will appear in the quoted theorem.}
% 
More precisely, they show that 
\begin{enumerate}
    \item $X_r$ is a normal variety
    \item $X_r$ is Gorenstein with rational singularities
    \item $X_r$ fits into a flat family over $\AA^m$ of normal varieties whose fibre over a point $(z_1,\dots,z_m)$ such that the $z_i$ are all distinct is 
\end{enumerate}
\[
. 
\]
They construct a simultaneous resolution of singularities and use the resolution of $X_r$ to describe the tangent space to $X_r$ at a point corresponding to an endomorphism $A$ such that $\rk A^i = r_i$ for all $i$. As an application of the tangent space computation they show that the general fiber of this family can be written as the scheme-theoretic intersection of varieties like $X_r$. 

To construct the resolution, they take $m$ to be the largest number such that $r(m) \ne r(m+1)$
and consider the affine space $\AA^m$ with coordinates $(x_1,\dots, x_m)$. Given $W\subset \AA^m$ a linear subvariety, 
% TODO: clarify that linear subvariety means to them a (translate of subvectorspace)
they introduce $\mathcal X_{r,W}\subset\End(V) \times W \times F_r$ where $F_r$ is the (paritial) flag variety $F_r = \{(V = V_0 \supset V_1 \supset \cdots \supset V_m \supset 0 ) : \dim V_i = r(i)\}$. 
% 
$\mathcal X_{r,W}\subset\End(V) \times W \times F$ is defined to be the subvariety of triples $(A,\vec z,\{V_i\})$ such that $(A-z_i) V_{i-1} \subset V_i$ for $z_i = x_i(\vec z)$ and $i = 1,\dots,m$. 
The (reduced image of) projection of $\mathcal X_{r,W}$ down to $\End(V) \times W$ is denoted $X_{r,W}$ and it is a closed affine subvariety. 
% 
\begin{theorem}
    (\cite[Theorem~2.1 iii)]{eisenbud1989rank})
    $X_{r,W}$ is the restriction of $X_{r,\AA^m}$ to $W$ and as such it is flat over $W$. 
\end{theorem}

This result is applied to study the fibers of the family $X_{r,\AA^m}$ over points $\vec z \in \AA^m$ for $z_i$ not necessarily distinct. The coordinates $z_i$ of $\vec z$ are partitioned by equality with the help of a permutation $p$ of $\{1,\dots,m\}$ such that 
\begin{align*}
    z_{p(1)} &= \cdots = z_{p(m_1)} , \\
    z_{p(m_1 + 1)} &= \cdots = z_{p(m_2)} , \\ 
    \dots \\
    z_{p(m_s + 1)} &= \cdots = z_{p(m_s + 1)}
\end{align*}
and $p$ preserves the order within each interval $m_i + 1,\dots,m_{i+1}$. \acom{Not sure what this means. Order of coordinates?} With this notation in place Eisenbud and Saltman define the numbers

% 
\begin{corollary}
    \label{cor:es}
    The family $\OO^{\lambda',\lambda''}_{0,\AA}$ defined above has zero fibre $\OO^\lambda$. 
\end{corollary}\begin{gather*}
    r(i,j):= \dim V - a_{p(m_i + 1)} - \cdots - a_{p(m_i + j )} \\
    \qquad \qquad \qquad \qquad \qquad \text{ for } i = 1,\dots,s \text{ and } j = 1,\dots , m_{i + 1 } - m_i 
\end{gather*}
where $a_i = r(i-1) - r(i)$.
It is through the $r(i,j)$ that the rank function $r$ determines the Jordan types of the generalized eigenspaces of the vector of (generalized) eigenvalues $\vec z$. 
\begin{proposition}
    (\cite[Corollary~2.2]{eisenbud1989rank})
    \label{prop:es}
    The scheme-theoretic fibre of $X_{r,W}$ over $\vec z\in W$ is 
    \[
        X_{r,\vec z} = \{A \in \End(V) : \rk (A-z_i)^j\le r(i,j) \text{ for all } i , j \}\,. 
    \]
\end{proposition}
% 
The only work we have to do to apply \Cref{prop:es} is to determine the rank function that will allow us to realize $\OO^\lambda$ as a rank variety, and the family $\OO^{\lambda',\lambda''}_{0,\AA}$ as a deformation of it. 
%  
\begin{lemma}
    \label{lem:es}
    Let $\lambda = \lambda' + \lambda''$ be partitions. There exists a permutation of the columns of $\lambda$ such that the first $\lambda'_1$ columns are precisely the columns of $\lambda'$ and the last $\lambda''_1$ columns are precisely the columns of $\lambda''$. 
\end{lemma}
Recall that $\lambda' + \lambda'' = \lambda$ is a partition of $N$. 
% Let $m_i = \lambda_{i,1}$ ($i = 1,2$), $m = m_1 + m_2$, and define
Let $m = \lambda_1' + \lambda_2'$ and define 
$$r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$$
on $j = 1,\dots,m$. 
\begin{lemma}
   The function $r$
%    defined by $r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$ 
is a rank function. 
\end{lemma}

The case that we are interested in is the case of two generalized eigenvalues, i.e.\ $\vec z = (z_1,z_2) = (0,s)$. 
\acom{Does order matter?} 
% Let me change notation so that $\lambda_i$ are shapes and 
We begin by considering the open loci where the rank conditions are strict. 

Note that $r(j-1) - r(j) = : a_j$ is the length of the $j$th column of $\lambda$. 
% As in \cite{eisenbud1989rank} we will denote this quantity $a_j$. 
% 
% 
Next, take $p$ to be a permutation of the (lengths of) columns $a_j$ of $\lambda$ such that $a_{p(1)},\dots,a_{p(\lambda_1')}$ are the (lengths of) columns of $\lambda'$ and $a_{p(\lambda_1' + 1)},\dots, a_{p(m)}$ are the (lengths of) columns of $\lambda''$. 
Then the functions 
$$
\begin{aligned}
r(1,j) &= N - a_{p(1)} - \cdots - a_{p(j)} \\
% \text{ and } 
r(2,j) &= N - a_{p(\lambda_1' + 1)} - \cdots - a_{p(\lambda_1' + j)}
\end{aligned}
$$
are such that 
\begin{align*}
    \OO^{\lambda',\lambda''}_{0,s} = X_{r,(0,s)} = \{A:\rk A^j &= r(1,j) \text{ for } i = 1,\dots, \lambda_1' \\ \text{ and } \rk (A-s)^j &= r(2,j) \text{ for } i = \lambda_1' + 1 ,\dots, m \}
\end{align*}
and according to \cite[Theorem 2.1(iii)]{eisenbud1989rank} the family $X_{r,(0,s)}$ is flat over $W = \{x_1 = \cdots = x_{\lambda_1'} = 0 , x_{\lambda_1'} = \cdots = x_m\}$ (a linear subvariety of $\AA^m$) with zero fibre $X_{r,(0,0)} = \OO^\lambda$ as desired. \acom{Do you need to permute the indices defining $W$?}
% $X_{r,\AA^m}$ is flat over $\AA^m$ with zero fibre $X_{r,0} = \OO^\lambda$ as desired. I guess we need to carefully choose the $W \subset \AA^m$ defining our family, which is smaller than the family $X_{r,\AA^m}$. Well, $W$ is just cut out by $0 = x_1 = x_2 = \cdots = x_{m_1}$ and $ x_{m_1 + 1} = \cdots = x_m$, or some permutation thereof to match the permutation $p$ above.
 
% On the one hand, we know that $r(i,j)$ should be equal to $N$ minus the number of boxes in the first $j$ columns of $\lambda_i$. On the other hand, ES define $r(i,j)$ to be $N$ minus the $i$th chunk of $j$ columns determined by the rank function $r(i)$. Also, we would like for $r(i)$ to determine the shape $\lambda = \lambda' + \lambda''$. Take for example $\lambda' = \lambda'' = (2,1)$. Then $\lambda = (4,2)$ and so $r\{0,1,2,3,4\}=\{6,4,2,1,0\}$ and 
% $$\{a_1,a_2,a_3,a_4\} = \{6-4,4-2,2-1,1-0\}=\{2,2,1,1\}\,.$$

% For another example take $\lambda' = (1)$ and $\lambda'' = (1,1)$. Then $\lambda = (2,1)$ and $r\{0,1,2\} = \{3,1,0\}$ so $\{a_1,a_2\} = \{2,1\}$.

% Again too small. The problem is I don't think that it is in general true that some permutation of the list $r(i-1) - r(i)$ will be the list of lengths of columns of $\lambda'$ followed by the list of lengths of columns of $\lambda''$ which is what we are looking for. For one thing there's in general more than one way to write a partition as a sum of two partitions. Ok, concretely, a third example. Take $\lambda' = (a,b)$ and $\lambda'' = (c,d)$ then $\lambda = (a + c, b + d)$ and 
% $$r\{0,1,\dots,a + c\} = \{N, N-2, \dots, N-2(b+d), N-2(b+d) - 1,\dots, 0\}$$
% so 
% $$\{a_1,a_2,\dots\} = \{2,2,\dots,2,1,1,\dots,1\}$$
% with $b+d$ 2s and $a+c - b - d$ 1s. Wait a minute, nevermind, of course this is true. We have two partitions $\lambda'$ and $\lambda''$ and one way to form the partition $\lambda = \lambda' + \lambda''$ is to break $\lambda_i$ down into columns, order the columns by length, and recombine. Cool. I think it's fine. 
% \acom{That's it for the note.}
\end{comment}

\section{Exposition}
\label{s:exposition}

Anderson and Kogan conjectured in \cite{anderson2006algebra} \acom{or earlier?} that those MV polynomials which are cluster monomials for a Fomin--Zelevinsky cluster algebra structure on $\CC[N]$ are naturally expressible as determinants\dots
% and they conjectured a formula for many of them.

It's not clear how this work helps with/relates to AK/their conjectures.

The generalized \mvy is interesting in its own right. \acom{is it?}

Computing fusion still hard but at least boiled down to linear algebra. Cf.\ fusion product as it appears in \cite{beilinson1991quantization,feigin2generalized,mirkovic2007geometric,anderson2006algebra,bezrukavnikov2005equivariant}.

Interesting combinatorics? Presumably it is related to Feigin and Loktev's generalized Kostka polynomials? Though Joel says that the product on SSYT that we are witnessing is unlikely to have a combinatorial description. 

Exchange relations only work on cluster modules where one is a mutation of the other (i.e.\ those corresponding to cluster monomials which are related by mutation). Of course this gives me everything. Up to $A_4$ as in type $A_5$ there exist indecomposable modules which are not cluster, so exchange relations do not apply. 

The hope (conjecture) is that this paper gives a way to compute on such modules. Cf.\ counterexample satisfying $\barD(c_Y) = \barD(b_Z) + 2\barD(b)$, where $b$ is a cluster monomial belonging to both bases and $c_Y$ is the square of a cluster monomial ($b'$ say) belonging to both bases. The $\barD$ equation suggests that $(b')^2 = b_Z + 2b$. We can try to check this using Roger's script. Such calculations bring us closer to answering two questions. First, how forgetful is $\barD$. Second, on the level of modules, are the terms appearing in the fusion product determined by $\Ext^1$ of the factors. By Roger's thesis this is true if the factors are cluster modules related by a mutation. 

% No; $x*y=\sum z\Rightarrow y = \frac 1 x \sum z$. 

Representation theory? 

\section{Rising Action} % Problem

\begin{lemma} \label{le:Grl1l2}
    Let $ L \in \Grth^+ $.  Let $ s \in \AA, s \ne 0 $.  The following are equivalent:
    \begin{enumerate}
        \item $ L $ is in the image of the map $ \Gr^{\lambda', \lambda''}_{0,s} \rightarrow \Grth^+$. %\acom{Should we describe this map somewhere? Earlier?}
        \item The linear operator $ t $ on $ \CC[t]^m/L$ has Jordan type $((0,\lambda'), (s,\lambda''))$.
        \item $ L \in G(\CC[t]) t^{\lambda'} (t-s)^{\lambda''}$.
        % \acom{why no $G(\CC[t])$-coset?}
    \end{enumerate}
\end{lemma}

\begin{proof}
First we recall that for $ L \in \Gr^+$, $ L \in \Gr^{\lambda} = G(\cO)t^\lambda $ if and only if $ t |_{\cO^m/L} $ has Jordan type $ \lambda$. 
% \acom{Change $L_0$}

    Now assume that $ (L, s) \in \Gr^{\lambda', \lambda''}_{0,s}$.  By definition, $ L(0) \in \Gr^{\lambda'}$ and $L(s) \in \Gr^{\lambda''} $.  This means that $t $ acting on 
    % $ \CC\xt^m / L(0)$ 
    $\cO^m/L(0)$ has Jordan type $ \lambda'$ and $ t$ acting on 
    % $ \CC\xt[t-s]^m / L(s) $ 
    $\cO_s^m/L(s)$ has Jordan type $ \lambda''$.  For $ a = 0, s$,  we see that $$\CC[t]^m/L \otimes_{\CC[t]} \cO_a \cong \cO_a^m / L(a). $$
    Thus, Lemma \ref{le:linalg} shows that the map
    % $ L_0 / L \rightarrow \CC\xt^m / L(0)$ 
    $\CC[t]^m/L\to \cO^m/L(0)$ induces an isomorphism between the $0$-generalized eigenspace of $ t$ and $ \cO^m / L(0)$.   The same thing holds for the $s$-generalized eigenspace of $t $ and $ \cO_s^m/L(s)$. This shows that (i) implies (ii) and the logic can be reversed to see that (ii) implies (i). 
    % \acom{is this map $v + L \mapsto v + L(0)$?} \jcom{yes} \marginpar[]{*}
    % todo 
    
    On the other hand, if $ L = g t^{\lambda'} (t-s)^{\lambda''} \CC[t]^m$ for some $ g \in G(\CC[t])$, then $ L(0) = g t^{\lambda'}\cO^m $ since 
    % $ (t-s)^{\lambda''} \in G(\CC\xt)$. 
    $ (t-s)^{\lambda''} \in G(\cO)$. In this way, we see that (iii) implies (ii) and the logic can be reversed to get the equivalence.
\end{proof}

\begin{lemma} \label{le:linalg}
    Let $ V $ be a $ \CC[t]$-module which is finite-dimensional as a $ \CC$ vector space.  For any  $ a \in \CC$, the map
    $$ V  \rightarrow V \otimes_{\CC[t]} \cO_a$$
    restricts to an isomorphism between the generalized $ a $-eigenspace of $ t $ and $ V \otimes_{\CC[t]} \cO_a$
    
    \end{lemma}
    
\begin{proof}
    For any $ b \in \CC$, let $ E_b $ denote the generalized $b$-eigenspace of $t$.  Then $ V = \oplus_{b \in \CC} E_b$.  Since $ t - b$ is invertible in $ \cO_a$ and $ t -b $ acts nilpotently on $ E_b$, we see that $ E_b \otimes_{\CC[t]} \cO_a = 0 $.
    
    So it suffices to show that $ E_a \rightarrow E_a \otimes_{\CC[t]} \cO_a$ is an isomorphism.  By the classification of modules over $ \CC[t]$, it suffices to check this when  $ E_a = \CC[t]/(t-a)^k$, where it is clearly true.
\end{proof}

\begin{lemma} \label{le:Wmu}
    Let $ L \in \Grth^+$.  The following are equivalent:
    \begin{enumerate}
        \item $ L \in \cW_\mu$.
                \item $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ for some $ v_i $ of the form $ v_i = t^{\mu_i} e_i + \sum_{j=1}^m p_{ij} e_j $ where $ p_{ij} \in \CC[t] $ has degree less than $ \min(\mu_i, \mu_j)$.
        \item  For all $ i $, 
        $$ t^{\mu_i} e_i \in \Sp_\CC(\{t^k e_j : 0 \le k < \min(\mu_i, \mu_j), 1 \le j \le m \}) + L. $$
    \end{enumerate}
    Moreover, for such $L $, $ \beta_\mu := \{ [t^k e_i] : 0 \le k < \mu_i, 1 \le i \le m\}$ forms a basis for $ \CC[t]^m/L$. 
    % \acom{Change $L_0$}
\end{lemma}
% 
% \jcom{I realized the meaning of the ``extra'' condition from the paper with Sabin and I added it into point 3.}
% \acom{Technically though the extra condition in that paper is weaker? $W_{\mu_i}$ being the span of $e_j,te_j,\dots,t^{\mu_i-1}e_j$ or the complement of $t^{\mu_i}\cO^m$ in $\cO^m$.}
% 
\begin{proof}
    Let $ L \in \cW_\mu$.  Then $ L = \Sp_{\CC[t]}(v_1, \dots, v_m) $ for some $ v_i $ with $ v_i = t^{\mu_i} e_i + \sum_{j=1} q_{ij}t^{\mu_i} e_j $ and $ q_{ij} \in t^{-1} \Oinf$.  Since $ L \in \Grth^+ $, we see that $ v_i \in \CC[t]^m$ which means that $ p_{ij} := q_{ij}t^{\mu_i} $ lies in $ \CC[t]$.  By construction, the polynomial $ p_{ij}$ has degree less than $ \mu_i$.
    
    Fix $ i$ and suppose that for some $ j$, $ \mu_j < \mu_i$.  In this case, we can alter our basis to $ v'_i = v_i - r v_j$ for some polynomial $r \in \CC[t]$.  This gives us new polynomials $ p'_{ij} = p_{ij} - r (t^{\mu_j} + p_{jj}) $.  In this way, we can ensure that $ p_{ij} $ has degree less than $ \min(\mu_i, \mu_j)$.  Thus (i) implies (ii).
    
    Suppose that $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ as in (ii).  Then
    $$t^{\mu_i} e_i - v_i \in \Sp_\CC(\{t^k e_j :  k < \min(\mu_i, \mu_j), 1 \le j \le m \})  \,. $$
    Hence (ii) implies (iii).  

    Finally, given (iii), then we can see $ v_i := t^{\mu_i} e_i - \sum_{j=1}^m p_{ij} e_j \in L $ for some $ p_{ij} \in \CC[t]$ of degree less than $ \min(\mu_i,\mu_j) $.  It is easy to see that $ L = \Sp_\cO(v_1, \dots, v_m) $ and so $ L \in \cW_\mu$.  
    
    Finally to show that $ \beta_\mu$ forms a basis for $ \CC[t]^m/L$, it suffices to show that for each $ i$, $ t^{\mu_i} e_i  \in \Sp_\CC(  t^k e_i : 0 \le k < \mu_i, 1 \le i \le m) + L$. 

%    \rcom{Preimage of $\beta_\mu$}  
    % 
 %   \acom{I guess that this is the ``$\mu$-numeration'' of the standard basis of $\CC^N$ that I commented out.}

    This follows immediately from (iii).
    % 
\end{proof}
% \acom{note-to-self, skipped this; return to it.}
% 
\begin{lemma}
    Let $\mu',\mu'' \in P$ and $\mu = \mu' + \mu''$. Under the map $ \Grbd \rightarrow \Grth$, the image of $ S^{\mu', \mu''}_{0,\AA}$ lands in $ \cW_\mu$.
\end{lemma}
%\acom{Probably it is worth writing down the map $ \Grbd \rightarrow \Grth$ just before this first occurence? \textit{is it $(L,s)\mapsto L$?} Update: added this above.}
% Resolved? the map $ \Grbd \rightarrow \Grth$ caused some confusion in my talk as on the left we have a family and on the right we have the thick affine Grassmannian --- Jiuzu kept thinking aloud that we can make $\Grth$ a family too.. Anyway I think that you address this below Joel. 

\begin{proof}
    Let $ L $ 
    % lie in this image.  
    be a lattice in the image of $S^{\mu',\mu''}_{0,\AA}$. Then $ L = g t^{\mu'} (t-s)^{\mu''}\CC[t]^m$ for some $ g \in N_-(\CC[t, t^{-1}, (t-s)^{-1}]) $.  Let $ h =(t-s)^{\mu''} t^{-\mu''}  $.  Note that $ h \in T_1(\Oinf)$. 
    Moreover,
    $$ L = h (h^{-1} g h) t^{\mu}\CC[t]^m\,. $$
    Note that 
    % $ h^{-1} g h \in N(\CC\xT[t^{-1}])$ 
    $h^{-1} g h \in N_-(\cK_\infty)$ and so we can factor $ h^{-1} g h$ as $n_1 n_2$ for some $ n_1 \in {(N_-)}_1(\Oinf), n_2 \in {N_-}(\CC[t]) $. 
    
    Since $ \mu $ is dominant, we see that $ t^{-\mu} n_2 t^\mu \in N_-(\CC[t]) $. % \CC\xt[t^{-1}]s
% 
% \rcom{I think we want $\mu$ antidominant here or should the $N$'s be $N_-$'s?} % 
% \acom{Si.}
% 
    Thus, $ L = h n_1 t^\mu \CC[t]^m$.  Since $ hn_1 \in G_1(\Oinf)$, the result follows. % \CC\xt[t^{-1}]
    \end{proof}

\section{Climax}
Given $ A \in \TT_\mu$, recall the definition of $ g(A)$ given in \Cref{eq:mvyofa}.  Note that $g(A)t^{-\mu} \in G_1(\Oinf)$.
% section ??.  
Since $ g(A) \in M_m(\CC[t]) \cap G_1(\Oinf)t^\mu$, we will regard $ g(A)$ as giving an element of $ \Grth^+ \cap \cW_\mu$.  (Alternatively, we can see that $ g(A) \CC[t]^m$ satisfies the condition (ii) from Lemma \ref{le:Wmu}.)

The following result is \cite[Theorem 3.2]{cautis2018categorical} up to transpose.
% 
\begin{theorem} \label{th:TmuWmu}
The map $ \TT_\mu \rightarrow \Grth^+ \cap \cW_\mu $ given by $ A \mapsto g(A)\CC[t]^m $ is an isomorphism with inverse given by
$$ L \mapsto [t|_{\CC[t]^m/L} ]^{\tr}_{\beta_\mu} \,. $$
%\acom{Up to transpose?!}
\end{theorem}
% 

For the next result, we will consider the ``intersection'' of $ \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ with $\cW_\mu$.  
As $  \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ is not a subscheme of $ \Grth$, by this intersection, we really mean the preimage of $ \cW_\mu$ under the composition
$$ 
\overline{\Gr}^{\lambda', \lambda''}_{0,\AA}  \hookrightarrow \Grbd \rightarrow \Grth\,.
$$
This is not a very serious abuse of notation, since the map $ \Grbd \rightarrow \Grth $ is almost injective. 
% Joel: The only non-injectivity comes from the fact that we forget the data
% of s.  So the map is non-injective over the locus where s is not
% actually an eigenvalue (in lattice terms where L(s) is the standard
% lattice).
In a similar way, we will write $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu$ using the non-injective map $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow M_N(\CC)$ (for example the fibre of this map over $ 0 $ is $ \CC $). 
% \acom{Grammar of `write using the map' is iffy. Also, is the fibre over 0 just a copy of the zero matrix in every $s$-fibre?}

\begin{theorem} \label{th:OGrl}
    There is an isomorphism
    $$\overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \cong \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu $$
    given by $ (A,s) \mapsto (g(A)\CC[t]^m, s)$.
\end{theorem}
% \acom{Joel, you stopped writing $X_{0,\AA}$ in favour of $X_\AA$ --- should we do this everywhere?}
% \jcom{Oops, no I think that we should stick with $ X_{0, \AA}$.}
% TODO: comb the doc at the end for this notation 
\begin{proof}
Since we already have the isomorphism from Theorem \ref{th:TmuWmu}, it suffices to show that for any $ A \in \TT_\mu$, 
$$ 
(A,s) \in \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \text{ if and only if } (g(A)\CC[t]^m, s) \in \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu \,. 
$$
This follows immediately from Lemma \ref{le:Grl1l2}.
\end{proof}
% 
\begin{theorem}
    The isomorphism from Theorem \ref{th:OGrl} restricts to an isomorphism
    $$ 
    \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \UU^{\mu', \mu''}_{0,\AA} \cong \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} \cap S^{\mu', \mu''}_{0,\AA}\,.
    $$
\end{theorem}

%\jcom{I was thinking about the proof of this result.  It is enough to prove that for $ A \in \OO^{\lambda', \lambda''}_s \cap \TT_\mu $, $g(A) \in S^{\mu', \mu''}_s $ if and only if $ A \in T^+$.  It is easy to show that if $ A \in \UU$, then $ g(A) \in S^{\mu', \mu''}_s $.  But the converse is not so obvious.  (I think Anne's paper/thesis might be incomplete on this point.)  I thought of two approaches: first to write down the lattice interpretation of $S^{\mu', \mu''}$ (similar to what is in my thesis in the Anderson-Kogan comparison section) or think about both $S^{\mu', \mu''}$ and $\UU$ as attracting sets for a $\CC^\times$ action.  Which do you prefer?}
% 
%\acom{Assuming Theorem 2 we think this follows by application of Roger's lemma. Also, I think my thesis is complete on this point.}
% 
%\jcom{I thought about this and I agree with you now.  I will write this up soon.} 
% 
%\acom{But I am also curious about the attracting set approach.}

\begin{proof}
We could prove this by observing the both sides are the attracting locus of an appropriate $ \CC^\times$ action. However, we will give the following more algebraic proof.

Let $ A \in \TT_\mu$ and $ s \in \CC $. We must show that  $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $ if and only if $ (g(A),s) \in S^{\mu', \mu''}_{0,\AA} $.

On the one hand, if $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $, then $ g(A)$ is lower-triangular with diagonal $ t^{\mu'} (t-s)^{\mu''}$, and so $ g(A) \in N_-[t, t^{-1}, (t-s)^{-1}] t^{\mu'} (t-s)^{\mu''}$. 

On the other hand, if $ (g(A), s) \in S^{\mu', \mu''}_{0,\AA}$, then we can write 
$$
g t^\mu r= n t^{\mu'} (t-s)^{\mu''}
$$
for some $ r \in G(\CC[t]), n \in N_-(\CC[t, t^{-1}, (t-s)^{-1}]) $ and $ g = g(A)t^{-\mu}$.  Let $ h = (t-s)^{\mu''} t^{-\mu''}$ which lies in $ T_1(\Oinf) $. % was CC\xt[]
Note that $ h^{-1}n h \in N(\Kinf)$, 
% ((t^{-1}))
so we can factor it as $ h^{-1} n h  = n_1 n_2 $, where $ n_1 \in N_{-,1}(\Oinf), n_2 \in N_-(\CC[t])$.  So then after doing a bit of algebra, we reach
$$
t^\mu r (t^{-\mu} n_2^{-1} t^\mu) t^{-\mu} = g^{-1} h n_1.
$$
Since $ g, h, n_1 \in G_1(\Oinf)$, the right hand side $ g^{-1} h n_1 $ lies in $ G_1(\Oinf) $.  Since $ \mu $ is dominant, $ t^{-\mu} n_2 t^\mu \in N(\CC[t])$, and so the left hand side lies in $t^\mu G(\CC[t]) t^{-\mu}$.

Moreover, since $ \mu $ is dominant, we know that 
$$t^\mu G(\CC[t]) t^{-\mu} \cap G_1(\Oinf) = N_1(\Oinf)\,.$$
% $$t^\mu G(\cO) t^{-\mu} \cap G_1(\CC\xt[t^{-1}]) = N_1(\CC\xt[t^{-1}]\,.$$
Thus, we deduce that $ g^{-1} h n_1 \in N_1(\Oinf)$ 
% $ g^{-1} h n_1 \in N_1(\CC\xt[t^{-1}])$ 
and hence $ g(A) \in t^{\mu'} (t-s)^{\mu''} N_1(\Oinf) $.  
% Given that we know that 
Since $ A \in \TT_\mu $, this implies that $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA}$ as desired.
\end{proof}

\jcom{In this proof, I made some statements about $\mu$ dominant, which are incorrect (as Anne knows). To make them correct we should either switch to having $ \mu$ being antidominant or switch $ N$ to $ N_-$.  In this proof $ g(A)$ generally denotes a group element, rather than an affine Grassmannian element which conflicts a bit with earlier usage.}

%\acom{What was the answer to Roger's question why is $g^{-1}hn_1 \in G_1 (\Oinf)$}

%\rcom{So if we look at $g = g(A)t^{-\mu}$, since $A \in \TT_\mu$, the entries of $g(A)_{ij}$ are polynomials of degree $\min(\mu_i-1,\mu_j-1)$ when $i \neq j$, otherwise of degree $\mu_i$. When we multiply by $t^{-\mu}$, we scale the $i$th column by $t^{-\mu_i}$. So the diagonal entries are $1 + p$ where $p \in t^{-1}\CC[t^{-1}]$ while the off-diagonal entries in column $i$ have highest (in $t$) degree $\min(\mu_i-1, \mu_j-1) - \mu_i < 0$. Hence $g \in G_1(\cO_\infty)$.}

\section{Falling Action} % Resolution
\acom{Recall (from optional \Cref{s:exposition}) the question motivating this work --- Roger's thesis, cluster monomials and the MV basis. Clarify that whereas we have so far only seen spaces related to ``ordinary MV cycles'' it is the ``stable MV cycles'' that are used to define the MV basis of $\CC[N]$ that the motivating question is concerned with. This discrepancy is the reason we might want to discuss maps $B(\infty)\to B(\lambda)$.}

\begin{comment}
\begin{theorem}
    Let $\lambda_i\ge\mu_i$ be dominant ($i=1,2$), $\mu = \mu' +\mu''$, and $\lambda =\lambda'+\lambda''$. 
    % I have an irrational dislike of ending sentences with subscripts --- can we fix this?
    There is an isomorphism 
    \begin{equation}
        \overline{\OO}_{\lambda',\lambda''}\cap\TT_{\mu',\mu''} \to \overline\Gr_\AA^{\lambda',\lambda''}\cap \cW_{\mu',\mu''}
    \end{equation}
    got by taking a $\mu\times\mu$ block matrix $A$ in the $s$-fibre $\overline{\OO_{\lambda',\lambda''}^s}\cap\TT_{\mu',\mu''}^s$ on the left to the representative of the $s$-fibre on the right defined by  
    \begin{equation}
        \begin{split}
            g &= t^{\mu'} (t-s)^{\mu''} + a(t) \\
            a_{ij}(t) &= - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1}
        \end{split}
    \end{equation}
    where $A^k_{ji}$ is the $k$th entry from the left of the last row of the $\mu_j\times\mu_i$ block of $A$. 
\end{theorem}

Let's call this the MVyBD isomorphism.

\begin{proof}
    The proof is fibre by fibre, so fix $s\ne 0$. \acom{Emphasize in the intro later (because this always confuses me) that by the $s$-fibre we really mean the $(0,s)$-fibre; i.e.\ its the BD Grassmannian over the second symmetric power of $C = \AA^1$; better just replace $s$-fibre by $(0,s)$-fibre everywhere it occurs.}
    \begin{enumerate}
        \item The map is well defined. In particular, it defines $\CC[t]$-lattices in $\CC(t)^m$. Moreover, these lattices break down to give pairs of lattices upon inverting $t$ or $t-s$ that have the right properties. [Copy Roger's proof]
        \item The inverse map is got by taking the matrix of multiplication by $t$.  More precisely, let $ L \in Gr^{BD} \cap \cW_\mu$.  We work with the quotient $\CC[t]^m/L$ just as in the ordinary MVy isomorphism---the only difference being $\CC\xt$ is replaced by $\CC[t]$.
\begin{enumerate}
    \item 
    We claim that 
    \begin{equation}
        \{[e_i],[te_i],\dots,[t^{\mu_{i}-1}e_i] : 1\le i \le m\}
    \end{equation}
    is a $\CC$-basis of $\CC[t]^m/L$.
    
    To see this, we use that $ L $ 
 has a $\CC[t]$-basis of the form 
    \begin{equation}
        v_i = t^{\mu_i} + \sum_{j>i} p_{ij}(t) e_j 
    \end{equation}
    with $\deg p_{ij}(t) < \mu_i = \mu_{1,i} + \mu_{2,i}$ ($1\le i\le m$).
    \acom{I don't know why this should be true. We might have to just define fibres of $\cW_{\mu',\mu''}$ in this way?}
    \item $t\big|_{\CC[t]^m/L}$ will have two eigenvalues, 0 and $s$, and its generalized 0-eigenspace will have block type $\le \lambda'$ while its generalized $s$-eigenspace will have block type $\le \lambda''$. 
    To see this, note that there is a natural isomorphism
    $$\CC\xt^m/(L \otimes_{\CC[t]} \CC\xt) = \text{generalized $0$ eigenspace of $t$ on } \CC[t]^m/L$$
    carrying the action of $t $ to the action of $t$.
    
    The left hand side is the same thing as
    $$ \CC\xt^m / (L \otimes_{\CC[t]} \CC\xt) = (\CC[t]^m/L) \otimes_{\CC[t]} \CC\xt $$
    
    the defining fact that lattices satisfying Equation~\ref{eq:defGrBDlambda} equivalently satisfy 
    \begin{equation}
        \begin{split}
            t\big|_{\CC\xt^m/L_1}\text{ has Jordan type} \le \lambda' \\
            t\big|_{\CC\xt^m/L_2}\text{ has Jordan type} \le \lambda'' 
        \end{split}
    \end{equation}
    where recall 
    $L_i = L\otimes \CC\xt$ ??? 
    % $L_i = L\otimes \CC[(t-p_i)^{-1}]$ 
    and $p_1 = s$ while $p_2 = 0$. 
    
    $$\CC\xt^m/(L \otimes_{\CC[t]} \CC\xt) = \text{generalized $0$ eigenspace of $t$ on } \CC[t]^m/L$$
    carrying the action of $t $ to the action of $t$.
    
    The left hand side is the same thing as
    $$ \CC\xt^m / (L \otimes_{\CC[t]} \CC\xt) = (\CC[t]^m/L) \otimes_{\CC[t]} \CC\xt $$
    \acom{Somehow, restricting to an eigenspace is like inverting/forgetting the action of $t$ by any other generalized eigenvalue? Basic linear algebra? Joel?}
\end{enumerate}
    \end{enumerate}
\end{proof}



\begin{theorem}[Theorem 1 version 2]
    Let $\lambda_1,\lambda_2$ and $\mu$ 
    %be dominant
    be arbitrary, such that $\lambda = \lambda_1 + \lambda_2 \ge \mu$. Then there is an isomorphism 
    \begin{equation}
        \overline{\OO_{\lambda_1,\lambda_2}} \cap \TT_\mu \to \overline{\Grbd[2]^{\lambda_2,\lambda_2}}\cap\cW_\mu 
    \end{equation}
    defined by the same map as in Theorem 1.
\end{theorem}

\jcom{This is true as stated with the ``larger'' definition of $ \cW_\mu $.  In fact, for any $\lambda_1, \lambda_2$, it is true $ \overline{\Grbd[2]^{\lambda_2,\lambda_2}}\cap\cW_\mu $ is contained in a subset that we could call $ \cW_\mu^s$ which we could define as
$$
\cW_\mu^s = G_1\xt[t^{-1}]\TT_\mu \cap G[t,t^{-1}, (t-s)^{-1}] / G[t]
$$
where we regard $G[t,t^{-1}, (t-s)^{-1}] / G[t] \subset G\xT[t^{-1}]/G[t] $

The way to think about this is as follows: inside the thick affine Grassmannian we can consider the $G$-bundles trivialized away from just 0, $s$, or equivalently those lattices which become the standard lattice after tensoring with $ \CC[[t-a]] $ for any $ a \ne 0, s$.
}
\begin{corollary}
    The MVyBD isomorphism restricts to an isomorphism of subfamilies 
    \begin{equation}
        \overline{\OO_{\lambda_1,\lambda_2}}\cap\TT_{\mu',\mu''}^+ \to \overline{(\Grbd[2])^{\lambda_1,\lambda_2}}\cap S_{\mu_1,\mu''}\,. 
    \end{equation}
\end{corollary}

\begin{proof}
    Let $ A \in \overline{\OO_{\lambda_1,\lambda_2}}\cap\TT_{\mu',\mu''}^+$ and let $ g $ be the polynomial matrix formed by the Mirkovi\'c-Vybornov isomorphism.  Then the diagonal entries of $ g $ are $ t^{\mu_{1,k}} (t-s)^{\mu_{2,k}}$ and we can factor
    $$ g = (g t^{-\mu_1}(t-s)^{-\mu_2}) t^{\mu_1}(t-s)^{\mu_2} \in N[t, t^{-1}, (t-s)^{-1}] t^{\mu_1} (t-s)^{\mu_2}$$
    So we get containment in one direction.
    
    For the reverse containment, we choose $ [g] \in \overline{(\Grbd[2])^{\lambda_1,\lambda_2}}\cap S_{\mu',\mu''}$.  By the lemma below, $[g] \in \cW_\mu$ and thus it lies in the image of our map and we are done.
\end{proof}

% Define $S_{\mu',\mu''}^s = N_-\xT[t^{-1}] t^{\mu_1} (t-s)^{\mu_2}$. 

\acom{is it a fibre of $S_{\mu',\mu''}$ defined above?}

We could also make the following claim. 


\begin{lemma}[KWWY14]
    Let $\mu$ be dominant. Then 
    \begin{equation}
        N_{-}\xT[t^{-1}] L_\mu = N_1\xt[t^{-1}]L_\mu
    \end{equation}
    \acom{where I am not sure about the double brackets.}
\end{lemma}

\begin{lemma}%[Roger's lemma]
    Let $\mu',\mu''$ be dominant and let $s\in \AA^1 - \{0\}$. Then 
    \begin{equation}
        S_{\mu',\mu''}^s \subset \cW_\mu 
    \end{equation}
    where $\mu = \mu_1 + \mu_2$.
\end{lemma}

\begin{proof}
    % Copy Roger's proof.
    We have
\[
\begin{split}
    S_{\mu_1, \mu_2} & = N\xT[t^{-1}]t^{\mu_1}(t-s)^{\mu_2} \\
     & \subset T_1\xt[t^{-1}] N\xT[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \\
     & = T_1\xt[t^{-1}] N_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \qquad \text{\cite[Lemma 2.3]{kamnitzer2014yangians}}\\
     & = B_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} \\
     & = B_1\xt[t^{-1}] t^{\mu_1 + \mu_2} \\
     & \subset G_1\xt[t^{-1}] t^{\mu_1 + \mu_2} \\
     & = W_{\mu_1 + \mu_2}
\end{split}
\]
where $B_1\xt[t^{-1}] t^{\mu_1} (t-s)^{\mu_2} = B_1\xt[t^{-1}] t^{\mu_1 + \mu_2}$ since 
\[
\frac{t}{t-s} = 
1 + \frac{s}{t} + \frac{s^2}{t^2} + \cdots 
\in B_1\xt[t^{-1}].
\]
\end{proof}
\end{comment}
% 
% In this section we demonstrate how to put the main results of this paper into practice.
% 
Let $m,\lambda',\lambda'',\lambda,\mu',\mu'',\mu,N$ be as before.
% 
In this part we prove that 
% the ideal of 
$Z'\ast_\AA Z''$ can be computed from a ``minimal'' pair $(\tau',\tau'')$ of semistandard Young tableaux for the Lusztig data  $(n_\bullet(Z'),n_\bullet(Z''))$.
% 
Given $\lambda$ dominant effective and $\mu\le\lambda$ effective, let $YT(\lambda)_\mu$ denote the set of semistandard Young tableaux of shape $\lambda$ and weight $\mu$. 
% 

We need the following result from \cite{dthesis}.
\begin{proposition}
    % Generalized orbital varieties are indexed by semistandard Young tableaux. 
    There are bijections 
    % Composing with the ordinary Mirkovi\'c--Vybornov isomorphism gives bijections 
    \[
        YT(\lambda)_\mu \to \irr \overline{\OO}^\lambda\cap\TT_\mu\cap\n \to \irr \overline\Gr^\lambda\cap S^\mu_-\,. 
    \]
    Moreover, there is a \acom{partial?} inverse 
    \[
    \CC[N]\to \bigcup_{\lambda~\text{dominant}\atop \mu\le\lambda} YT(\lambda)_\mu \,. 
    \]
    % $$
    % \begin{tikzcd}
    %     YT(\lambda)_\mu \ar[r] \ar[d] & \irr \overline{\OO}^\lambda\cap\TT_\mu\cap\n \ar[d] \\
    %     GT(\lambda)_\mu \ar[r] & \irr \overline\Gr^\lambda\cap S^\mu_-
    % \end{tikzcd}
    % $$
\end{proposition}
% 
\acom{Refer to Claxton--Tingley multisegments, Hong--Lee marignally large tableaux for comparison.}
\jcom{I don't think that you should mention this partial inverse here.  I think that you should explain here how the generalized orbital varieties are indexed by the tableaux, in other words, how $ X(\tau) $ is defined.}

Denote the image of $\tau$ in $\overline{\OO}^\lambda\cap\TT_\mu\cap\n$ by $X(\tau)$ and in $\overline{\Gr}^\lambda\cap S^\mu_-$ by $Z(\tau)$. Thus $Z(\tau) = \phi(X(\tau))$. \acom{Out of place?} 
\rcom{What is $\phi$?}

% 
% 
% TODO: Not yet, that would be the algo stuff that we are holding off on 
% In this section we explain how to determine the ideal of $Z'\ast_\AA Z''$ from the minimal pair $(\tau',\tau'')$ corresponding to $(n_\bullet(Z'),n_\bullet(Z''))$.
% 
% In particular, we prove that ``the generalized Spaltenstein map'' produces MV cycles. 
% 
% and recall the $\mu$-numeration of the standard basis of $\CC^N$. 
% TODO: can't we do without it^^
Given $A \in M_N(\CC)$ we'll denote by $A\big|_{\CC^i}$ the restriction of $A$ to the subspace spanned by the first $i$ standard basis vectors. Given $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ denote by $\lambda'(i),\lambda''(i)$ the shape of $\tau'(i),\tau''(i)$ resp.\ where $\tau^?(i)$ denotes the tableau got from $\tau^?$ by discarding all but its first $i$ boxes. If the tableau contains repeated entries then first means leftmost.


\begin{proposition}
    Given $Z',Z''$, let $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ be such that\dots If $s\ne 0$, then 
    \[
    \overline{\{A\in\UU_{0,s}^{\mu',\mu''} : A \big|_{\CC^{i}} \in \OO_{0,s}^{\lambda'(i),\lambda''(i)} \text{ all } i=1,\dots,N\}} 
    \cong Z'\times Z'' \,. %\mu_1 + \cdots + \mu_i
    % = Z(\tau')\times Z(\tau'')\,. 
    \]
\end{proposition}
% 
\jcom{Do you want the ``blocky'' or ``boxy'' description here?  I mean, do you want $ i = 1, \dots, N$ or $ i =1, \dots, m$? I'm not so sure that ``boxy'' will work, since with those $s$ in the matrix, it doesn't preserves all the $ \CC^i$.}


\begin{proof}
    Let $(A,s)$ be an element of the set on the left hand side, let $a\in\{0,s\}$ and denote by $E_a$ the generalized $a$-eigenspace of $A$. 
    Let $i\in \{1,\dots,N\}$ and write $m(i)$ for the maximum weight of a box in $\tau'(i)$ or $\tau''(i)$. 
    
    The the Jordan type of $A-a\big|_{E_a\cap\CC^{m(i)}}$ is given by $\lambda'(i)$ if $a = 0$ and $\lambda''(i)$ if $a = s$. 
    Moreover, the lattice $(L,s)$ defined by $g(A)$ is such that $t-a$ has the same Jordan type on 
    % $\cO_a^{m(i)} / L(a)\cap\cO_a^{m(i)} = \CC[t]^{m(i)}/L\cap\CC[t]^{m(i)}\otimes_{\CC[t]}\cO_a $ 
    \begin{equation}
        \label{eq:bcflat}
        \cO_a^{m(i)} / L(a)\cap\cO_a^{m(i)} = \CC[t]^{m(i)}/L\cap\CC[t]^{m(i)}\otimes_{\CC[t]}\cO_a 
    \end{equation}
    as does $(A-a\big|_{E_a\cap\CC^{m(i)}},s)$. 
    \acom{Not sure when to use $(?,s)$ notation and when not in this proof. Here I just didn't want to end the sentence with the bulky subscript.}
    % 
    \Cref{eq:bcflat} follows from e.g.\ \cite[Proposition~10.14]{atiyah2018introduction}:
    \begin{quotation}
        If $A$ is a Noetherian ring, $\fa$ an ideal, $\hat A$ the $\fa$-adic completion of $A$, then $\hat A$ is a flat $A$-algebra.
    \end{quotation}
    applied to $A = \CC[t]$ and $\fa = (t)$. 
    
    \jcom{I think that it is actually a bit simpler.  Again from teaching algebraic geometry, I found the following.  If $ A $ is a principal ideal domain, then $M $ is flat iff it is torsion-free.}

    \acom{Here $A$ is $\CC[t]$ and $M = \cO$ right? Better, agreed. Does the direction: torsion free implies flat, or not flat implies not torsion free, hold even without the PID assumption?}
    
    The second assertion is a consequence of \Cref{lem:} \acom{Cite Joel's lemma}. 
    By \Cref{adthesis} \acom{Cite my thesis} it follows that $(L(0),L(s)) \in Z'\times Z''$. 
    Since all maps are isomorphisms the converse is automatic. 
\end{proof}

\begin{question}
%[Related question] 
    If $\mu',\mu''$ are not dominant, the LHS is smaller so the map $?$ has no chance of being an isomorphism. In general the map $?$ has to do with producing a matrix from a linear operator. 
    \[
    \begin{tikzcd}
        \overline\Gr^{\lambda',\lambda'}_{0,s}\cap S^{\mu',\mu''}_{0,s} \ar[r,equals]\ar[d] & \overline\Gr^{\lambda'} \cap S^{\mu'} \times \overline\Gr^{\lambda'} \cap S^{\mu''} \supset Z' \times Z'' \ar[d]\\
        \overline\OO^{\lambda',\lambda''}_{0,s} \cap \UU^{\mu',\mu''}_{0,s} \ar[r,"?"] & \overline\OO^{\lambda'}\cap \TT_{\mu'}\cap \n \times \overline\OO^{\lambda''}\cap \TT_{\mu''}\cap \n
    \end{tikzcd}    
    \]
\end{question}
% 
\begin{proposition}(\cite{baumann2019mirkovic})
    % Given two MV cycles $Z_\tau$ and $Z_\sigma$ of type\dots 
    Let $Z',Z''$  be MV cycles of coweight $(\lambda',\mu'),(\lambda'',\mu'')$ resp.\ and assume that $\lambda',\lambda'',\mu = \mu' + \mu''$ are dominant. 
    Then 
    \begin{equation}
        \label{eq:tabmult}
        % Z_\tau\ast Z_\sigma 
        m_{\lambda'\lambda''} ([Z']\otimes[Z'']) 
        = 
        % \sum_{Z\in\cZ(\lambda)_\mu} 
        \sum_{\tau\in S(\lambda)_\mu}
        i\left(
            Z(\tau), Z'\circ Z''
            % \pi^{-1}(0)\cdot\overline{Z_1\times Z_2\times U}
        \right) [Z(\tau)]
    \end{equation}
    and the intersection multiplicities on the right are found by\dots\acom{To be completed.} 
    
    % In particular, the multiplicity of $Z(\tau)$
    % in the product of the zero fiber 
    % (divisor class) 
    % and the 
    % (Zariski) 
    % closure of the family $Z_1 \times Z_2 \times U$ 
    % is equal to the multiplicity of its generalized orbital variety $X(\tau)$ 
    % in $ \overline{\OO}^{\lambda',\lambda''}_{0,0}\cap\UU^{\mu',\mu''}_{0,0} $. 
    % among the set 
    % \begin{equation}\label{eq:tabmult}
        % \irr\lim_{s\to 0} 
        % \overline{\OO}^{\lambda',\lambda''}_{0,0}\cap\UU^{\mu',\mu''}_{0,0} \,. 
        % \irr \lim_{s\to 0} \overline\OO^s_{\lambda_1,\lambda_2} \cap \TT_{\mu',\mu''}^{+,s}
    % \end{equation}
    \acom{Some of what was written here (commented out above) was incorrect. We can reiterate for emphasis that $i(Z,Z'\circ Z'')$ is computed in terms of rank varieties.}

    Since stable MV cycles can be represented by ordinary ones the multiplicities in 
    $$
    b_{Z'}b_{Z''} = \sum_{Z\in\cZ(\infty)_{-\nu_1 - \nu_2}} i \left(
        Z, Z' \circ Z''
        % \pi^{-1}(0) \cdot \overline{Z_1 \times Z_2 \times U}
    \right) b_Z 
    $$
    can also be deduced from \Cref{eq:tabmult} for appropriate choices of $\lambda',\lambda''$ and $\mu',\mu''$. These are described in \Cref{lem:mintab,prop:minwts} below.
\end{proposition}
% 
\begin{conjecture}
    % Let $Z_i \subset \overline{S^{\nu_i}\cap S^0_-}$ be an MV cycle of weight $\nu_i$ ($i = 1,2$) and put $\nu = \nu_1 + \nu  _2$. 
    Let $\tau$ be the tableau of shape $\lambda$ and weight $\mu$ whose Lusztig datum is equal to the sum of the Lusztig data of $\tau'$ and $\tau''$. 
    % Then $i(\tau, \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U})$ is equal to 1. 
    Then 
    \begin{equation}
        i(\tau, 
        % \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U}
        % Z_1 \circ Z_2 
        \tau' \circ \tau''
        ) = 1 \,. 
    \end{equation}
\end{conjecture}
% 

\begin{lemma}
    \label{lem:mintab}
    Let $n_\bullet$ be a Lusztig datum of coweight $\nu$. There exist $\lambda,\mu$ smallest such that $n_\bullet$ is the Lusztig datum of a tableau $\tau\in YT(\lambda)_\mu$. They are determined by the following minimization problem. Regard $n_\bullet$ as a strictly lower triangular matrix with $(r,c)$th entry $n_{r,c}$ is equal to the number of boxes of weight $r$ in row $c$ of $\tau$. Note that since $r > c$ this data does not account for the number of boxes of weight $r$ in row $r$. Let $\mu^0$ be the diagonal matrix whose $(r,r)$th entry $\mu^0_r$ is equal to the number of boxes of weight $r$ in row $r$ of $\tau$. Note that the sum of the rows of $n_\bullet + \mu^0$ is the shape $\lambda$ of $\tau$, and the sum of the columns of $n_\bullet + \mu^0$ is the weight $\mu$ of $\tau$. The solution to the minimization problem 
    \[
        \begin{gathered}
        % 
        \text{minimize }\mu^0\text{ such that} \\
        % \text{the sum of the rows of }n_\bullet + \mu^0\text{ is non-increasing},
        % \text{the sum of the columns of }n_\bullet + \mu^0\text{ is non-increasing}
        \text{both }\lambda,\mu\text{ are non-increasing}
        \end{gathered} 
    \]
    is got recursively from % setting $\mu^0_m = 0$, 
    \[
        \begin{gathered}
            \mu^0_{m} = 0 \\ 
            \mu^0_{m-1} = \max\{0 , \sum n_{m,c} - \sum n_{m-1,c}\} \\ 
            \mu^0_i = \max\{0, \mu^0_{i+1} + \sum n_{i+1,c} - \sum n_{i,c} , \mu_{i+1}^0 + \sum n_{r,i+1} - \sum n_{r,i}\}
        \end{gathered}
    \]
    for $i = 1,\dots,m-2$. 
    In words, given a Luszig datum $n_\bullet$, if we wish to produce a minimal SSYT for it, 
    \begin{itemize}
        \item we can always take the number of $m$'s in row $m$ to be zero; 
        \item we can take the number of $m-1$'s in row $m-1$ to be zero, unless $n_\bullet$ tells us that there are more $m$'s the first $m-1$ rows than there are $m-1$'s in the first $m-2$ rows, and in that case we take $\mu^0_{m-1}$ so that it exactly offsets the difference; 
        \item we can take the number of $m-2$'s in row $m-2$ to be zero, unless $n_\bullet$ tells us there are more $m-1$'s in the first $m-1$ rows (building off the last step) than there are $m-2$'s in the first $m-3$ rows, or, that there are more boxes in row $m-1$ than there are in row $m-2$, and then we take $\mu^0_{m-2}$ to offset the minimum of the two differences; 
        \item induct up to $\mu^0_1$. 
    \end{itemize}
\end{lemma}

\begin{proposition}
    \label{prop:minwts}
    Given two Lusztig data $n_\bullet',n_\bullet''$ there is a smallest choice of effective coweights $\mu',\mu''$ and effective dominant coweights $\lambda',\lambda''$ such that $\mu = \mu' + \mu''$ is effective dominant and $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ are semistandard Young tableaux with the prescribed Lusztig data. They are got by choosing the minimum $\lambda,\mu$ for $n_\bullet = n_\bullet ' + n_\bullet ''$ afforded by \Cref{lem:mintab}, and then breaking off $n_{r,c}'$ boxes of weight $r$ from each row $c<r$ of $\tau$ to form a $\tau'_0$ an intermediate tableau. If $\tau'_0$ is of an acceptable shape then we keep it. If it is not, then we correct it by padding it using some boxes of weight $r$ in row $r$ of what remains of $\tau$.

    \acom{To be completed. We require a notion of ``smallest'' pair of factor tableaux associated to a given tableau --- that is if we care to include our $B(\infty)\to B(\lambda)$ map. 
    As the examples show, there may be more than one way to break up a ``minimal'' tableau for $n_\bullet = n_\bullet ' + n_\bullet ''$ into two tableaux with data $n_\bullet' $ and $n_\bullet''$ resp.\ The practical notion of min is such that the generalized nonzero eigenspace has the smallest possible dimension.}
\end{proposition}

% Let's wait for Gerhard before we discuss our computations any further
% \subsection{Pseudocode}
% 

\section{Denouement}
\label{s:denouement}
\subsection{Examples}

% \acom{Plus fusion product of fake MV cycles for a fourth!}

We view MV cycles as semi-standard young tableaux. Suppose we have two tableaux $\tau'$ and $\tau''$, with respective weights $\lambda'$, $\mu'$ and $\lambda''$, $\mu''$, and we wished to take their fusion product. We consider a generic matrix $X \in \UU^{\mu',\mu''}_{0,s}$ and see what relations on the variables in $X$ occur when also asking for it to be in $\OO^{\lambda',\lambda''}_{0,s}$. 

Consider the subtableaux $\tau'(i)$ of $\tau'$ and $\tau''(i)$ of $\tau''$ where we take the boxes of $\tau'$ and $\tau''$ containing the numbers $1,\dots,i$. Let $\lambda'(i)$ and $\lambda''(i)$ be the corresponding sizes and $\mu'(i)$ and $\mu'(i)$ be the corresponding weights of the subtableaux. For each $i$, we obtain a corresponding submatrix $X_i \in \UU^{\mu'(i), \mu''(i)}_{0,s}$ of $X$. These subtableaux impose rank conditions, equivalently vanishing minors, on each $X_i$ when asking them to be in $\OO^{\lambda'(i), \lambda''(i)}_{0,s}$, which allows us to deduce relations on the variables in $X$. The relations must also include the new variables in the latest block and cannot have $s$ as a factor.

To find the MV cycles, equivalently tableaux, in the $0$-fibre of the fusion, we consider the ring generated by all the variables of $X$ and quotient out the ideal generated by the relations from the vanishing minors as well as the minimal polynomial of $X$. We then add $s$ into the ideal. Each piece in the primary decomposition of this ideal will correspond to a tableaux, and subsequently a MV cycle.

% \begin{example}
% Let $G = \GL_3$ and consider the tableaux
% $$\tau' = \young(12,3) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau''=\young(13,2).$$

% Lusztig data: $(010)$ for $\tau'$, $(101)$ for $\tau''$

\acom{If we start with $n_\bullet ' = (0,1,0)$ and $n_\bullet'' = (1,0,1)$ then the smallest $\tau$ for $n_\bullet = (1,1,1)$ is $\young(1123,23)$ and hence the smallest $\mu,\lambda$ are $(2,2,2),(4,2,0)$. So far so good. However, the ``smallest'' ``fission'' of $\tau$ is $\young(112,23) , \young(3)$. We could define ``smallest'' to mean minimizing the dimension of the generalized $s$-eigenspace. What do you guys think? If we agree to do this we should amend this example. For Joel: this is an example where our $B(\infty) \to B(\lambda)$ does not match that of Claxton--Tingley (quoted in my thesis). They would get $\lambda = (3,1)$. I think that their map satisfies $\lambda$ dominant and $\mu$ such that each $\mu_i\ge 1$.}

% Dimensions: $(1,0,-1)$ for both.

% The MV cycles corresponding to $\tau'$ and $\tau''$ are both isomorphic to $\PP^2$.
% The weights are $\lambda' = \lambda'' = (2,1,0)$ and $\mu' = \mu'' = (1,1,1)$. Hence the matrix in consideration is 
% \[
% X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
%     0 & 1 & & & & \\
%      & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 \\
%      & & 0 & 1 & & \\
%      & & & s & A_{23}^1 & A_{23}^2 \\
%      & & & & 0 & 1 \\
%      & & & & & s
% \end{BMAT}
% \right].
% \]

% For the first entry of $\mu'$ and $\mu''$, we have the following subtableaux to consider:
% $$\young(1) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1).$$
% Let $X_1$ be the top-left $2 \times 2$ submatrix of $X$. The subtableaux imposes the conditions $\dim \ker X_1 = 1$ and $\dim \ker (X_1-s) = 1$. We see that we do not obtain any relations from this.

% For the first two entries of $\mu'$ and $\mu''$, we have the following subtableaux:
% $$\young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1,2).$$
% Let $X_2$ be the top-left $4 \times 4$ submatrix of $X$. The left subtableau imposes the conditions $\dim \ker X_2 = 1$ and $\dim \ker X_2^2 = 2$ while the right subtableau imposes the condition that $\dim \ker (X_2 -s) = 2$. These conditions force the relation $$A_{12}^1 + sA_{12}^2 = 0.$$

% The original tableaux for $X$ imposes the conditions that $\dim \ker X = 2$, $\dim \ker X^2 = 3$, $\dim \ker (X-s) = 2$, and $\dim \ker (X-s)^2 = 3$. These force the relation $$A_{23}^1 = 0.$$

% Hence we consider the quotient
% $$\frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{23}^2,s]}{\langle A_{12}^1+sA_{12}^2,A_{23}^1,s \rangle} \cong
% \frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{23}^2]}{\langle A_{12}^1,A_{23}^1 \rangle}.$$
% We see already that the $0$-fibre is irreducible. The tableau corresponding to the ideal $\langle A_{12}^1,A_{23}^1 \rangle$ is 
% $$\young(1123,23)$$
% which corresponds to the MV cycle that is the product of the MV cycles corresponding to $\tau'$ and $\tau''$.

% Lusztig datum: $(111)$

% Dimension: $(2,0,-2)$
% \end{example}

\begin{example}
Let $G = \GL_3$ and consider the tableaux
$$\tau' = \young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(11,23).$$

Lustig data: $(100)$ for $\tau'$, $(001)$ for $\tau''$.

\acom{This time the smallest $\tau$ is actually $\young(12,3)$ and the smallest fission is $\young(1,3),\young(2)$. For Joel: this time the our $\lambda$ matches \cite{claxton2015young}'s.}

Dimensions: $(1,-1,0)$ for $\tau'$, $(0,1,-1)$ for $\tau''$.

The MV cycles corresponding to the above tableaux are each isomorphic to $\PP^1$.
We have the weights to be $\lambda' = (2,0,0)$, $\lambda'' = (2,2,0)$, $\mu' = (1,1,0)$, and $\mu'' = (2,1,1)$. The matrix under consideration is therefore
\[
X = \left[\begin{BMAT}(e){ccc;cc;c}{ccc;cc;c}
    0 & 1 & & & & \\
     & 0 & 1 & & & \\
     & -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 \\
     & & & 0 & 1 & \\
     & & & & s & A_{23}^1 \\
     & & & & & s
\end{BMAT}
\right].
\]
There are no relations from looking at the submatrix $X_1$. From the submatrix $X_2$, we require $$A_{12}^1 + sA_{12}^2 = 0.$$
From $X$, we further obtain the relations $$A_{12}^2A_{23}^1 + sA_{13}^1 = 0, A_{12}^1A_{23}^1 + s^2A_{13}^1 = 0.$$
Then the quotient ring we find is
$$\frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s]}{\langle A_{12}^1+sA_{12}^2, A_{12}^2A_{23}^1+sA_{13}^1, A_{12}^1A_{23}^1+s^2A_{13}^1, s\rangle} \cong \frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,b_3,s]}{\langle A_{12}^1, A_{12}^2\rangle \cap \langle A_{12}^1, A_{23}^1 \rangle}.$$
We see that the 0-fibre is reducible with two components. The tableau corresponding to each ideal is
$$\begin{array}{cccc} \vspace{1mm}
    \young(1113,22) & \text{for } \langle A_{12}^1, A_{12}^2 \rangle & \text{Lusztig: }(101) & \dim = (1,0,-1)\\ 
    \young(1112,23) & \text{for } \langle A_{12}^1, A_{23}^1 \rangle & \text{Lusztig: }(010) & \dim = (1,0,-1).
\end{array}$$
Each of these tableaux corresponds to a MV cycle isomorphic to $\PP^2$ so the generic fibre $\PP^1 \times \PP^1$ degenerates to two copies of $\PP^2$.
\end{example}

\begin{comment}
\begin{example}
Let $G = SL_3$ and consider the following tableaux:
$$\tau' = \young(1122) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(1111,2233).$$

Lusztig data: $(200)$ for $\tau'$, $(002)$ for $\tau''$.

Dimensions: $(2,-2,0)$ for $\tau'$, $(0,2,-2)$ for $\tau''$.

The MV cycles corresponding to these tableaux are each isomorphic to $\PP^1 \times \PP^1$. 
The weights are then $\lambda' = (4,0,0)$, $\lambda'' = (4,4,0)$, $\mu' = (2,2,0)$, and $\mu'' = (4,2,2)$.
Then we are considering the following matrix:
\[
X = \left[\begin{BMAT}(e){cccccc;cccc;cc}{cccccc;cccc;cc}
    0 & 1 & & & & & & & & & & \\
     & 0 & 1 & & & & & & & & & \\
     & & 0 & 1 & & & & & & & & \\
     & & & 0 & 1 & & & & & & & \\
     & & & & 0 & 1 & & & & & & \\
     & & -s^4 & 3s^3 & -6s^2 & 4s & A_{12}^1 & A_{12}^2 & A_{12}^3 & A_{12}^4 & A_{13}^1 & A_{13}^2 \\
     & & & & & & 0 & 1 & & & & \\
     & & & & & & & 0 & 1 & & & \\
     & & & & & & & & 0 & 1 & & \\
     & & & & & & & & -s^2 & 2s & A_{23}^1 & A_{23}^2 \\
     & & & & & & & & & & 0 & 1 \\
     & & & & & & & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
By computer, it can be checked that we obtain the following 3 ideals:
$$
\begin{array}{cl}
    I_1 = & \langle A_{12}^1, A_{12}^2, A_{12}^3, A_{12}^4, s \rangle \\
    I_2 = & \langle A_{12}^1, A_{12}^2, A_{23}^1, A_{23}^2, s \rangle \\
    I_3 = & \langle A_{12}^1, A_{12}^2, (A_{23}^1)^2, A_{12}^4A_{23}^1 + A_{12}^3A_{23}^2, A_{12}^3A_{23}^1, (A_{12}^3)^2, s \rangle.
\end{array}
$$
It can be checked that $I_3$ has multiplicity 2 \rcom{Not sure how to make this precise and how to check it.} so the 0-fibre has 4 components, two of which are the same. The tableau corresponding to each ideal is
$$
\begin{array}{cccc}\vspace{1mm}
    \young(11111133,2222) & \text{for } I_1 & \text{Lusztig: }(202) & \dim = (2,0,-2) \\ \vspace{1mm}
    \young(11111122,2233) & \text{for } I_2 & \text{Lusztig: }(020) & \dim = (2,0,-2) \\ \vspace{1mm}
    \young(11111123,2223) & \text{for } I_3 & \text{Lusztig: }(111) & \dim = (2,0,-2).
\end{array}
$$
The MV cycle for each ideal is isomorphic to $\PP^2 \times \PP^2$.
\end{example}
\end{comment}

\begin{example}
    
    \acom{Here the fission is the smallest if we set out to minimize the dimension of the zero eigenspace. For Joel: this $\lambda(n_\bullet)$ again matches that of \cite{mirkovic2007quiver}.}

Let $G = \GL_3$. Consider the tableaux
$$\tau' = \young(22) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(11,33).$$
Then the weights are $\lambda' = (2,0,0)$, $\lambda'' = (2,2,0)$, $\mu' = (0,2,0)$, and $\mu'' = (2,0,2)$. Our matrix $X$ is now
\[
X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
    0 & 1 & & & & \\
    -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 \\
     & & 0 & 1 & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 \\
     & & & & 0 & 1 \\
     & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
Using a computer, the ideals we get are
$$
\begin{array}{cl}
    I_1 = & \langle A_{12}^1, A_{12}^2 \rangle \\
    I_2 = & \langle A_{23}^1, A_{23}^2 \rangle \\
    I_3 = & \langle (A_{23}^1)^2, A_{12}^2A_{23}^1 + A_{12}^1A_{23}^2, A_{12}^1A_{23}^1, (A_{12}^1)^2 \rangle. 
\end{array}
$$
The tableaux corresponding to each ideal is
$$
\begin{array}{cc}\vspace{1mm}
    \young(1133,22) & \text{for } I_1 \\ \vspace{1mm}
    \young(1122,33) & \text{for } I_2 \\ \vspace{1mm}
    \young(1123,23) & \text{for } I_3
\end{array}
$$
and the MV cycles corresponding to each are isomorphic to $\PP^2 \times \PP^2$.
\end{example}

Let $G = \GL_4$. There is a cluster structure on $\CC[N]$ consisting of 12 cluster variables. We list the corresponding MV cycles and tableaux:
\[\begin{array}{cccccc} \vspace{1mm}
    Z_1 &\leadsto \young(2) & Z_2 &\leadsto \young(1,3) & Z_3 &\leadsto \young(1,2,4) \\ \vspace{1mm}
    Z_{1 \leftarrow 2} &\leadsto \young(3) & Z_{1 \rightarrow 2} &\leadsto \young(2,3) & Z_{2 \leftarrow 3} &\leadsto \young(1,4) \\ \vspace{1mm}
     Z_{2 \rightarrow 3} &\leadsto \young(1,3,4) & Z_{in} &\leadsto \young(2,4) & Z_{out} &\leadsto \young(13,2,4)  \\
    Z_{P_1} &\leadsto \young(2,3,4) & Z_{P_2} &\leadsto \young(3,4) & Z_{P_3} &\leadsto \young(4)
\end{array}\]

We show that the cluster relations in $\CC[N]$ are also true for the fusion product of the corresponding MV cycles.

For each of the 15 exchange relations, we state the associated matrix $X$, the relations acquired from each submatrix $X_i$, the ideal generated by said relations, and the corresponding tableaux from the ideal.

\begin{example}
Consider the product $Z_1 * Z_2$, so the tableaux are
\[
\young(2) \text{ and } \young(1,3).
\]
Our matrix is
\[
X = \left[\begin{BMAT}(e){c;c;c}{c;c;c}
    s & A_{12}^1 & A_{13}^1 \\
     & 0 & A_{23}^1 \\
     & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1+sA_{13}^1
\end{array}.
\]
Hence our ideal is
$$\langle A_{12}^1A_{23}^1+sA_{13}^1,s \rangle = 
\langle A_{12}^1,s \rangle \cap \langle A_{23}^1,s \rangle.$$
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,s \rangle &\leadsto \young(13,2) &\equiv \young(3) &\leadsto Z_{Z_{1 \leftarrow 2}} \\ 
    \langle A_{23}^1,s \rangle &\leadsto \young(12,3) &\equiv \young(2,3) &\leadsto Z_{Z_{1 \rightarrow 2}}
\end{array}
\]
so $Z_1 * Z_2 = Z_{1 \leftarrow 2} + Z_{1 \rightarrow 2}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_3$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(1,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;c;c;c}{cc;c;c;c}
    0 & 1 & & & \\
     & s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & & s & A_{23}^1 & A_{24}^1 \\
     & & & 0 & A_{34}^1 \\
     & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{13}^1 \\
    X_4 & A_{14}^1, A_{23}^1A_{34}^1 + sA_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{14}^1,s \rangle \cap \langle A_{12}^1,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{14}^1,s \rangle &\leadsto \young(11,24,3) &\equiv \young(1,4) &\leadsto Z_{2 \leftarrow 3} \\ 
    \langle A_{12}^1,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(11,23,4) &\equiv \young(1,3,4) &\leadsto Z_{2 \rightarrow 3}
\end{array}
\]
so $Z_2 * Z_3 = Z_{2 \leftarrow 3} + Z_{2 \rightarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_1 * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1 + sA_{13}^1 \\
    X_4 & A_{34}^1, A_{12}^1A_{24}^1 + sA_{14}^1, A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{23}^1,A_{24}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(12,3,4) &\equiv \young(2,3,4) &\leadsto Z_{P_1} \\ 
    \langle A_{12}^1,A_{34}^1,A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1,s \rangle &\leadsto \young(13,2,4) & &\leadsto Z_{out}
\end{array}
\]
so $Z_1 * Z_{2 \rightarrow 3} = Z_{P_1} + Z_{out}$.
\end{example}

\begin{example}
Consider the product $Z_1 * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(2) \text{ and } \young(1,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(2) \text{ and } \young(11,24,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;c;c}{cc;cc;c;c}
    0 & 1 & & & & \\
    -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
     & & 0 & 1 & & \\
     & & & s & A_{23}^1 & A_{24}^1 \\
     & & & & s & A_{34}^1 \\
     & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 + sA_{12}^2 \\
    X_3 & A_{13}^1, A_{23}^1 \\
    X_4 & A_{12}^2A_{24}^1 + sA_{14}^1, A_{12}^1A_{24}^1 - s^2A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s \rangle \cap \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s \rangle &\leadsto \young(114,22,3) &\equiv \young(4) &\leadsto Z_{P_3} \\ 
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle &\leadsto \young(112,24,3) &\equiv \young(2,4) &\leadsto Z_{in}
\end{array}
\]
so $Z_1 * Z_{2 \leftarrow 3} = Z_{P_3} + Z_{in}$.
\end{example}

\begin{example}
Consider the product $Z_3 * Z_{1 \rightarrow 2}$, so the tableaux are 
\[
\young(1,2,4) \text{ and } \young(2,3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(1,2,4) \text{ and } \young(12,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;c;c}{cc;cc;c;c}
    0 & 1 & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
     & & 0 & 1 & & \\
     & & & s & A_{23}^1 & A_{24}^1 \\
     & & & & s & A_{34}^1 \\
     & & & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1 \\
    X_4 & A_{24}^1, A_{13}^1A_{34}^1 - sA_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle &\leadsto \young(112,24,3) &\equiv \young(1,4) &\leadsto Z_{in} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(112,23,4) &\equiv \young(1,3,4) &\leadsto Z_{P_1}
\end{array}
\]
so $Z_3 * Z_{1 \rightarrow 2} = Z_{in} + Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_3 * Z_{1 \leftarrow 2}$, so the tableaux are 
\[
\young(1,2,4) \text{ and } \young(3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    0 & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_4 & A_{23}^1A_{34}^1 - sA_{24}^1, A_{13}^1A_{34}^1 - sA_{14}^1, A_{13}^1A_{24}^1 - A_{23}^1A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,A_{13}^1A_{24}^1-A_{23}^1A_{14}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,s \rangle &\leadsto \young(14,2,3) &\equiv \young(4) &\leadsto Z_{P_3} \\ 
    \langle A_{12}^1,A_{34}^1,A_{13}^1A_{24}^1-A_{23}^1A_{14}^1,s \rangle &\leadsto \young(13,2,4) & &\leadsto Z_{out}
\end{array}
\]
so $Z_3 * Z_{1 \leftarrow 2} = Z_{P_3} + Z_{out}$.
\end{example}

\begin{example}
Consider the product $Z_{1 \leftarrow 2} * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(3) \text{ and } \young(1,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(13,2) \text{ and } \young(1,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;c;c;c}{cc;c;c;c}
    0 & 1 & & & \\
     & s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & & 0 & A_{23}^1 & A_{24}^1 \\
     & & & 0 & A_{34}^1 \\
     & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_4 & A_{13}^1A_{34}^1 + sA_{14}^1
\end{array}.
\]
Hence our ideal is 
\[
\langle A_{12}^1,A_{13}^1A_{34}^1+sA_{14}^1,s \rangle
= \langle A_{12}^1,A_{13}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,s \rangle &\leadsto \young(114,23) &\equiv \young(14,3) &\leadsto Z_2 Z_{P_3} \\ 
    \langle A_{12}^1,A_{34}^1,s \rangle &\leadsto \young(113,24) &\equiv \young(3,4) &\leadsto Z_{P_2}
\end{array}
\]
so $Z_{1 \leftarrow 2} * Z_{2 \leftarrow 3} = Z_2 Z_{P_3} + Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{1 \rightarrow 2} * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2,3) \text{ and } \young(1,3,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(12,23) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;c}{cc;cc;cc;c}
    0 & 1 & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & 0 & 1 & & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & 0 & 1 & \\
     & & & & & s & A_{34}^1 \\
     & & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1, A_{12}^2A_{23}^2 + A_{13}^1 + sA_{13}^2 \\
    X_4 & A_{34}^1, A_{12}^2A_{24}^1 + sA_{14}^1, sA_{23}^2A_{14}^1 - sA_{13}^2A_{24}^1 - A_{13}^1A_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,A_{12}^2A_{23}^2+A_{13}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(113,224,3) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,A_{12}^2A_{23}^2+A_{13}^1,s \rangle &\leadsto \young(112,233,4) &\equiv \young(12,33,4) &\leadsto Z_2 Z_{P_1}
\end{array}
\]
so $Z_{1 \rightarrow 2} * Z_{2 \rightarrow 3} = Z_{P_2} + Z_2 Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_{out}$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(11,23) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;cc;cc;c}{ccc;cc;cc;c}
    0 & 1 & & & & & & \\
     & 0 & 1 & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & & 0 & 1 & & & \\
     & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & & 0 & 1 & \\
     & & & & & & s & A_{34}^1 \\
     & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1, A_{12}^2 \\
    X_3 & A_{13}^1 \\
    X_4 & A_{34}^1, A_{23}^1A_{14}^1 + s(A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1) 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(1113,224,3) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(1113,223,4) &\equiv \young(1,3,4) &\leadsto Z_{1 \leftarrow 2} Z_{2 \rightarrow 3}
\end{array}
\]
so $Z_2 * Z_{out} = Z_{P_2} + Z_{1 \leftarrow 2} Z_{2 \rightarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_{in}$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    0 & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & s & A_{23}^1 & A_{24}^1 \\
     & & 0 & A_{34}^1 \\
     & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1 - sA_{13}^1 \\
    X_4 & A_{23}^1A_{34}^1 + sA_{24}^1, A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{34}^1,s \rangle \cap \langle A_{23}^1,A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{34}^1,s \rangle &\leadsto \young(13,24) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{23}^1,A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1,s \rangle &\leadsto \young(12,34) & &\leadsto Z_{1 \rightarrow 2} Z_{2 \leftarrow 3}
\end{array}
\]
so $Z_2 * Z_{in} = Z_{P_2} + Z_{1 \rightarrow 2} Z_{2 \leftarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_{out} * Z_{1 \rightarrow 2}$, so the tableaux are 
\[
\young(13,2,4) \text{ and } \young(2,3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(13,2,4) \text{ and } \young(12,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;c}{cc;cc;cc;c}
    0 & 1 & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & 0 & 1 & & & \\
     & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & 0 & 1 & \\
     & & & & & s & A_{34}^1 \\
     & & & & & & 0
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1 +sA_{23}^2 \\
    X_4 & A_{34}^1, A_{13}^1A_{24}^1 - A_{23}^1A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(1112,24,3) &\equiv \young(23,4) &\leadsto Z_1 Z_{P_2} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(1123,23,4) &\equiv \young(23,3,4) &\leadsto Z_{1 \leftarrow 2} Z_{P_1}
\end{array}
\]
so $Z_{out} * Z_{1 \rightarrow 2} = Z_1 Z_{P_2} + Z_{1 \leftarrow 2} Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{1 \leftarrow 2}$, so the tableaux are
\[
\young(2,4) \text{ and } \young(3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(2,4) \text{ and } \young(13).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
     s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_4 & A_{23}^1A_{34}^1 - sA_{24}^1
\end{array}.
\]
Hence our ideal is 
\[
\langle A_{23}^1A_{34}^1 - sA_{24}^1,s \rangle
= \langle A_{23}^1,s \rangle \cap \langle A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{23}^1,s \rangle &\leadsto \young(124,3) &\equiv \young(24,3) &\leadsto Z_{1 \rightarrow 2} Z_{P_3} \\ 
    \langle A_{34}^1,s \rangle &\leadsto \young(123,4) &\equiv \young(23,4) &\leadsto Z_1 Z_{P_2}
\end{array}
\]
so $Z_{in} * Z_{1 \leftarrow 2} = Z_{1 \rightarrow 2} Z_{P_3} + Z_1 Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{out} * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(1,4) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(11,24,3) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;cc;cc;cc}{ccc;cc;cc;cc}
    0 & 1 & & & & & & & \\
     & 0 & 1 & & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & & 0 & 1 & & & & \\
     & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & & 0 & 1 & & \\
     & & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & & 0 & 1 \\
     & & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1,A_{12}^2 \\
    X_3 & A_{13}^1, A_{23}^1 \\
    X_4 & \begin{array}{c}
         A_{13}^2A_{34}^1 - sA_{14}^1, A_{34}^1 + sA_{34}^2, A_{13}^2A_{34}^2 + A_{14}^1,  \\
         A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1 + s(A_{13}^2A_{24}^2 - A_{23}^2A_{14}^2), \\ 
         A_{14}^2A_{23}^2A_{34}^1 - A_{14}^1A_{23}^2A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^1A_{24}^2
    \end{array}
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{14}^1,A_{34}^1,s \rangle \cap \begin{array}{c}
     \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2+A_{14}^1,A_{13}^2A_{24}^1-A_{23}^2A_{14}^1,s \rangle. 
\end{array}
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(1114,223,34) &\equiv \young(14,3,4) &\leadsto Z_{2 \rightarrow 3} Z_{P_3} \\ 
    \begin{array}{c}
     \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2+A_{14}^1,A_{13}^2A_{24}^1-A_{23}^2A_{14}^1,s \rangle. 
\end{array}&\leadsto \young(1113,224,34) &\equiv \young(13,24,4) &\leadsto Z_3 Z_{P_2}
\end{array}
\]
so $Z_{out} * Z_{2 \leftarrow 3} = Z_{2 \rightarrow 3} Z_{P_3} + Z_3 Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2,4) \text{ and } \young(1,3,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(12,24,3) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;cc}{cc;cc;cc;cc}
    0 & 1 & & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & 0 & 1 & & & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & 0 & 1 & & \\
     & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & 0 & 1 \\
     & & & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{13}^1, A_{23}^1, A_{12}^2A_{23}^2 + sA_{13}^2 \\
    X_4 & \begin{array}{c}
         A_{23}^2A_{34}^1 - sA_{24}^1, A_{12}^2A_{24}^1 + A_{13}^2A_{34}^1, A_{34}^1 + sA_{34}^2, A_{23}^2A_{34}^2 + A_{24}^1, \\
         A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1 + sA_{14}^2,  \\
         A_{23}^2A_{14}^1 - A_{13}^2A_{24}^1 + s(A_{23}^2A_{14}^2 - A_{13}^2A_{24}^2), \\
         A_{13}^2A_{24}^2A_{34}^1 - A_{13}^2A_{24}^1A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^2A_{24}^1 
    \end{array} 
\end{array}.
\]
Hence our ideal decomposes into 
\[\begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1,s \rangle  
\end{array}
 \cap \begin{array}{c}
      \langle A_{12}^1, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1,  \\
      A_{34}^1, A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1,s \rangle. 
 \end{array} 
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1,s \rangle  
\end{array} &\leadsto \young(113,224,34) &\equiv \young(13,24,4) &\leadsto Z_3 Z_{P_2} \\ 
    \begin{array}{c}
      \langle A_{12}^1, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1,  \\
      A_{34}^1, A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1,s \rangle. 
 \end{array} &\leadsto \young(112,234,34) &\equiv \young(12,34,4) &\leadsto Z_{2 \leftarrow 3} Z_{P_1}
\end{array}
\]
so $Z_{in} * Z_{2 \rightarrow 3} = Z_3 Z_{P_2} + Z_{2 \leftarrow 3} Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{out}$, so the tableaux are 
\[
\young(2,4) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(112,24,3) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;ccc;cc;cc}{ccc;ccc;cc;cc}
    0 & 1 & & & & & & & & \\
     & 0 & 1 & & & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{12}^3 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & & 0 & 1 & & & & & \\
     & & & & 0 & 1 & & & & \\
     & & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & & & 0 & 1 & & \\
     & & & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & & & 0 & 1 \\
     & & & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1, A_{12}^2+sA_{12}^3 \\
    X_3 & A_{13}^1,A_{23}^1 \\
    X_4 & \begin{array}{c}
         A_{23}^2A_{34}^1 - sA_{24}^1, A_{34}^1 + sA_{34}^2, A_{23}^2A_{34}^2 + A_{24}^1, \\
         A_{12}^3A_{34}^1 - A_{12}^2A_{34}^2, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1 + s(A_{13}^2A_{24}^2 - A_{23}^2A_{14}^2), \\
         A_{13}^2A_{24}^2A_{34}^1 - A_{13}^2A_{24}^1A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^2A_{24}^1, \\
         A_{12}^3A_{23}^2A_{14}^1 - A_{12}^2A_{23}^2A_{14}^2 - A_{12}^3A_{13}^2A_{24}^1 + A_{12}^2A_{13}^2A_{24}^2,  \\
         A_{12}^3A_{13}^2A_{24}^1A_{34}^2 - A_{12}^2A_{13}^2A_{24}^2A_{34}^2 + A_{12}^3A_{14}^1A_{24}^1 - A_{12}^2A_{14}^2A_{24}^1
    \end{array} 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1, A_{34}^1,s \rangle
\cap \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1, s \rangle. 
\end{array} 
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1, A_{34}^1,s \rangle &\leadsto \young(11124,223,34) &\equiv \young(24,3,4) &\leadsto Z_{P_1} Z_{P_3} \\ 
    \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1, s \rangle. 
\end{array} &\leadsto \young(11123,224,34) &\equiv \young(123,24,4) &\leadsto Z_1 Z_3 Z_{P_2}
\end{array}
\]
so $Z_{in} * Z_{out} = Z_{P_1} Z_{P_3} + Z_1 Z_3 Z_{P_2}$.
\end{example}

\bibliographystyle{alpha}
\bibliography{mvybd}

\end{document}