%!TEX program = lualatex

\documentclass[draft]{article}
\usepackage{basic}
\usepackage{emoji}
% TODO: The matrix is lower triangular and the lower-triangular matrix is
% \setemojifont{Apple Color Emoji}
% Working title: 
\title{How to compute the fusion product of MV cycles in type A}
\author{Roger Bai, Anne Dranowski, Joel Kamnitzer} 
% \date{October 2020}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
\label{s:intro}
% 
% The Mirkovi\'c--Vybornov isomorphism
In \cite{mirkovic2007quiver} (see also the recent sequel \cite{mirkovic2019comparison}) Mirkovi\'c and Vybornov provide a geometric version of symmetric and skew Howe $(\GL_m,\GL_n)$ dualities by relating Kazhdan--Lusztig slices in the affine Grassmannian of $\GL_m$ to slices in $n\times n$ nilpotent orbits on the one hand 
% conjugacy classes of nilpotent matrices 
and to Nakajima quiver varieties on the other hand. 
This paper generalizes the first of these relations (an isomorphism) to families of slices by way of the Beilinson--Drinfeld Grassmannian. 
Essential to \cite{mirkovic2007geometric}, this space is used to define a fusion (aka convolution) product on sheaves (representations) but can also be used to define a fusion product of MV cycles (bases). 
% 
This product structure makes the vector space spanned by the MV cycles into a commutative algebra isomorphic to the ring of functions on the unipotent subgroup (conjectured in \cite{anderson2003polytope} and proved in \cite{baumann2019mirkovic}). % conjectured by JEA and proved by BKK
% 
To quote from Mirkovi\'c and Vilonen:
\begin{quotation}
    However, we make crucial use of an idea of Drinfeld, going back to around 1990. He discovered an elegant way of obtaining the commutativity constraint by interpreting the convolution product of sheaves as a ``fusion'' product.
\end{quotation}
% 
[Liaison]

The purpose of this paper is to give a conceptually elementary way to compute this product in type $A$. We do not know how fruitful this work might be in the larger landscape of geometric representation theory as it is plagued by two major limitations: it does not readily generalize outside of type $A$, and it is as yet computationally expensive. 
% 
Moreover, it is not clear why one should care about the fusion product on bases. \emoji{imp} \emoji{grin} \emoji{worried} \emoji{monocle-face} \emoji{skull}

\section{Players}
\label{s:players}
\subsection{Rings and discs}
\label{ss:rings}
Set $\cO = \cO_0 = \CC\xt$ and $\cK = \cK_0 = \CC\xT$.
% TODO: I rearranged this sentence, because starting it with 'for any other' presupposes we have introduced s 
%  For any other 
We will also consider $\cO_s = \CC\xt[t-s]$ and its fraction field $\cK_s=\CC\xT[t-s]$, for any 
$ s \in \CC\setminus\{0\} $, as well as $ \Oinf = \CC\xt[t^{-1}] $ and $\Kinf = \CC\xT[t^{-1}]$. For any $ s \in \PP = \PP^1$, 
% TODO: Identifying $s\in\CC$ and the line $s\in\PP$ containing it right?
$ \cO_s$ is the completion of the local ring $ \mathcal O_{\PP, s} $ and thus the formal spectrum of $ \cO_s$ is the formal neighbourhood $ D_s$ of $ s $ (also called the formal disc centered at $ s$).  Similarly the formal spectrum of the field $\cK_s$ is the deleted formal neighbourhood (or punctured disc), denoted $ D_s^\times$.

Note that for $ s \in \CC $, we have an obvious isomorphisms  $\cO_s \cong \cO$ and $\cK_s\cong\cK$ taking $ t-s $ to $ t$. 
% 
\subsection{Groups}
\label{ss:groups}
% 
Let $H $ be an algebraic group over $ \CC $.  We will be interested in $ H(R)$ where $ R $ is a $\CC$-algebra, for example $R = \CC[t], \cO_s$, etc.  
% 
% \acom{random question, I know $H(R)$ is supposed to mean functor of points of the scheme $H$ but can it also be viewed as a base change from $\CC$ to $\RR$?}
% \jcom{Usually $ H_R$ is the notation used for the base change, so using $ H(R)$ here is a bit of an abuse, but it is common and harmless.}
% 
Note that evaluation at $ t = s$ provides a group homomorphism $ H(\cO_s) \rightarrow H$.  We denote the kernel of this map by $ H_1(\cO_s)$, often called the  first congruence subgroup.  We will be particularly interested in this construction in the case $ s = \infty$, which gives us the group $ H_1(\Oinf)$.
% \acom{Add $G_1$}
%\acom{TODO: Algebraic groups over rings, i.e.\ the meaning of $G(K)$. The affine space of $m\times m$ matrices which we denote $ M_m$. The first congruence subgroup $G_1$ and why we take $\Oinf$ points instead of $\CC[t]$ points as some of the lit does.}

Fix $G = \GL_m$ and let $T\subset G$ be the maximal torus of diagonal matrices. 
% are there other possibilities?? :S 
The coweight lattice of $ G $ is $ \ZZ^m = \Hom(\CC^\times,T)$.
% TODO: Thank you - resolved!
% \acom{This identification assumes we are taking $\CC$ points of $G$ right? i.e.\ it's really $P(T(\CC)) = \Hom(\CC^\times,T(\CC))$ that's $\cong \ZZ^m$ and for instance $P(T(\CC[x]/(x^2))$ will give sth else?} 
% \jcom{No, the coweight lattice is the set of homomorphisms of algebraic groups from $ \mathbb G_m$ to $ GL_m$.  It is independent of the ring/field of definition.}
% 
The \textbf{dominant coweights} of $ G $ 
% is
are the set of $ \lambda = (\lambda_1, \dots, \lambda_m) \in \ZZ^m $ such that $ \lambda_1 \ge \cdots \ge \lambda_m$. 
% 
A coweight $ \mu = (\mu_1, \dots, \mu_m) \in \ZZ^m$ is called \textbf{effective}, if $ \mu_j \ge 0 $ for all $ j$.
% We also need the notion of effective dominant coweight, 
% \acom{can we conflate or should we be precise and say coweight?} \jcom{Corrected, let's stick to coweights.}
%which means those $ \lambda $ as above with $ \lambda_m \ge 0$.  
% \acom{1. No boldface on this new term. Proceed with selective boldfacing? 2. Since we sometimes need $\Peff$ which contains $P_{++} = \Peff\cap\Pdom$, how about just using the notation $\Peff,\Pdom$ instead of $P_+, P_{++}$ for a change?}
% \jcom{I added the definition of effective coweight.  I actually don't think that we need any special notation $ P, P_+, P_{dom}$ etc !  Later we can just write ``$ \lambda \in \ZZ^m$ a dominant or effective coweight'' etc}
% TODO: it's settled then, no special notation fro coweights!
% 
So an effective dominant coweight $ \lambda $ is a partition of $|\lambda| = \lambda_1 + \cdots + \lambda_m\in\NN$, which we call the size of $ \lambda$.
%We write $P_+$ for the set of dominant coweights, and $P_{++}$ for the set of effective dominant coweights. 
% 
% nicematrix test
% $\begin{bNiceArray}{ccc|c}[margin]
%     \Block{3-3}<\Large>{A} & & & 0 \\
%     & \hspace*{1cm} & & \Vdots \\
%     & & & 0 \\
%     \hline
%     0 & \Cdots& 0 & 0
%     \end{bNiceArray}$
% 
% $\begin{pNiceMatrix}
%     \frac12 & -\frac12 \\
%     \frac13 & \frac14 \\
%     \end{pNiceMatrix}$
% 
% Set $\AA = \AA^1_x$ (the subscript denoting the chosen local coordinate) and fix $s\in \AA - \{0\}$. 
% 

Given a coweight $\mu \in \ZZ^m$ and a point $ s\in \CC$, we define $ (t-s)^\mu$ to be the diagonal matrix 
% with entries $ (t-s)^{\mu_1}, \dots, (t-s)^{\mu_m}$
% \[
%     \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
%         (t-s)^{\mu_1} & 0             & \Cdots & 0 \\
%         0             & (t-s)^{\mu_2} & \Ddots & \Vdots \\
%         % 0 & b & a & \Ddots & & \\
%         \Vdots              & \Ddots        & \Ddots & 0 \\
%         % \Vdots & & & & \\
%         0             & \Cdots        &   0    & (t-s)^{\mu_m}
%     \end{pNiceMatrix}
% \]
\[
\begin{bmatrix}
    (t-s)^{\mu_1} \\
    & (t-s)^{\mu_2} \\ 
    & & \ddots \\
    & & & (t-s)^{\mu_m}
\end{bmatrix} 
\]
which we will view in 
% $\GL_m(K) $ 
$G(K)$ for any ring $ K $ containing $\CC[t]$ in which $t-s$ is invertible (for example $ K = \Ks, \CC(t)$). % A: Think R sb K; changed it 
\acom{Add warning that we will also sometimes write $(t-s)^\mu$ for its image in 
% $\GL_m(K)/\GL_m(R)$ 
$G(K)/G(R)$ and we hope it will be clear? Only we haven't introduced this space yet.}
% Resolved
% 2. Change notation $\mu = (\mu_1,\dots,\mu_m)$ since we also have $\mu = \mu_1 + \mu_2$. We could do $\mu = \mu' + \mu''$ or $\mu = a + b$? The notation $\mu^{(i)}$ is already taken to mean sth. Ofc all can be changed. Can also do $\mu = {}_{1}\mu + {}_{2}\mu$. \emoji{sweat-smile}}

We will also be interested in the affine space $ M_m$ of $m\times m$ matrices.  Note that for any $ \CC$-algebra $ R $, 
% $\GL_m(R) $ 
$G(R)$ consists of those matrices $ g \in M_m(R) $ whose determinant is invertible in $ R$. Thus for example $ (t-s)^\mu\in M_m(\CC[t])$ for all effective $ \mu \in \ZZ^m$ but 
% $ t^\lambda \in \GL_m(\CC[t]) $ 
$(t-s)^\mu\in G(\CC[t])$ if and only if $ \mu = 0 $.
% 
\acom{maybe add that it's the same for $G(\cO)$.}

\subsection{Lattices}
\label{ss:lat}
We will use the lattice model for the affine Grassmannian, so it is useful to recall the following definition. Let $ R \subset K$ be two $\CC$-algebras (usually, but not always, $K$ will be a field). 
% \acom{When is $K$ not a field?}). Let $ m \in \mathbb N$ \acom{we have already fixed $m$?} and; e.g. K = C[t,t^{-1}] is not a field!!! 
Consider $ K^m $ as a $K$-module. 
% \acom{left or right, doesn't matter? I think we are in a situation where they are the same, i.e.\ our rings are commutative.} 
By restriction $ K^m$ can be viewed as an $R$-module.  An \textbf{$R$-lattice} in $K^m$ 
% \acom{what is $M$? should be $K^m$?}; bc Joel was thinking about arbitrary free modules 
is an $R$-submodule $ L \subset K^m$ 
% such that $ L $ 
which is a free $R$-module of rank $ m $ and satisfies $ L \otimes_R K = K^m $. Equivalently, $ L = \Sp_R(v_1, \dots, v_m)$ where $v_1, \dots, v_m$ are free generators of $K^m$. 
% 
\begin{comment}
\jcom{Maybe need that both $K$ and $R$ are integral domains and have the same field of fractions?} 
\acom{Roger has proved that what is written is ok.}
\rcom{Here's my argument: If $L \subset K^m$ is a free rank $m$ $R$-module and $L \otimes K = K^m$, then it's clear we that $L$ is generated by $m$ generators $v_1,\dots,v_m$ over $R$. Since $L\otimes K = K^m$, these also generate $K^m$ over $K$. Then we have a surjective map $K^m \twoheadrightarrow K^m$ given by sending a basis of $K^m$ to the generators $v_1,\dots,v_m$. Since any surjective endomorphism of finite $K$-modules is injective, the $v_1,\dots,v_m$ form a basis. I think the only thing that is needed is that the rings are commutative.}
\jcom{Great.}
\end{comment} 

% The trivial lattice is 
% No more: L_0(R) = 
$ R^m \subset K^m $ is called the standard lattice. 
% We will abuse notation and denote $L_0(R)$ simply $L_0$ in the hope that the ring $R$ will be understood from the context. 
The group $\GL_m(K) $ acts transitively on the set of $R$-lattices in $K^m$, thus giving a bijection between 
% the set of such lattices 
this set and $\GL_m(K)/\GL_m(R)$, since $ \GL_m(R) $ is the stabilizer of the standard lattice. 
%\acom{For example, we will frequently make use of the lattice $L_\lambda$ corresponding to the point $t^\lambda\in\Gr$ given $\lambda\in P$. If $e_1,\dots,e_m$ denote the standard basis elements of 
% $L_0(\cO)$ then 
%$\cO^m$ then 
%$$ 
%L_\lambda = \Sp_\cO(t^{\lambda_1}e_1, \dots, t^{\lambda_m}e_m)\,. 
%$$}


We will be particularly interested in $\CC[t]$-lattices in $ \CC(t)^m$. 
% TODO: 
% \acom{Wait, I thought $\CC(t)$ is used in the Ran space version of the BD Gr.}
Given such a lattice $ L $ and a point $ a \in \CC$, the \textbf{specialization} of $ L $ at $ a $ is defined as
$$
% L(a) := L \otimes_{\CC[t]} \CC\xt[t-a] \subset \CC\xT[t-a]^m\,.
L(a) := L \otimes_{\CC[t]} \cO_a, 
% \text{ a lattice in } \cK_a^m.  
$$
a lattice in $\cK_a^m$. 
If 
% $ L(a) = L_0(\cO_a)$ 
$L(a) = \cO_a^m$ then $L$ is said to be \textbf{trivial at $a$}. 
% We say that $ L $ is trivial at $ a $ if $ L(a) = \cO_a^m$. 
% \acom{Do we want to employ the notation $\cO_a$ and $\cK_a$ here?}
For example, the lattice $(t-s)^{-1}\CC[t] \subset \CC(t)$ is trivial at any $a\ne s$, since $ t-s $ is invertible in $ \cO_a$. 
% \acom{it is a little bit uneven to have notation for $\cO^m$ but not for $\cO_a^m$?}

% \acom{Should we also mention our interest in $\cO$-lattices in $\cK^m$?}
% \jcom{I don't think that it is necessary.  I just put the part about $ \CC(t)$ to set up the notation for this specialization.}


% \jcom{I wasn't sure of the notation to use here, but I wanted something which didn't conflict with $ L_0$.}

% \acom{I think that MVy use this notation also: }

%\includegraphics[width=\textwidth]{img/Capture d’écran, le 2021-02-17 à 13.03.10.png}}

\subsection{Affine Grassmannians} 
\label{ss:affgrs}
% To map thick Gr to C is to map a lattice to the points where it isn't trivialzable; doesn't make sense to ask for poles 
% G(C(t))/G(C[t]) for example lives over the Ran space, finitely many eigenvalues; positive part of thick contains this guy as image 
Set $\AA = \AA^1$ viewed as the subset $\PP\setminus \{\infty\}$ of $\PP$. Let $\Vtriv$ denote the trivial rank $ m $ vector bundle. 
% 
We now define various versions of ``the'' affine Grassmannian which will play an important r\^ole in this paper.

\begin{definition}
\label{def:gr}
     The \textbf{ordinary affine Grassmannian} $\Gr = G(\cK)/G(\cO)$.
\end{definition}    
It is described in modular terms by 
$$
\Gr = 
    \left\{ 
        (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_0 $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_0^\times $} 
    \right\} \,. 
$$

We also have a lattice description 
$$ 
\Gr = 
    \left\{ L \subset \cK^m : \text{ $L$ is a $\cO$-lattice} \right\}\,.
$$
% 
We obtain a lattice from a pair $ (V,\varphi) $ by setting $ L = \Gamma(D_0, V)$ which is embedded into $ \cK^m = \Gamma(D_0^\times, \Vtriv)$ using $ \varphi$.  
On the other hand, to get a lattice from the group-theoretic description $ G(\cK) / G(\cO) $ we set $ L = g \cO^m$ for $ g \in G(\cK)$.
% 
\begin{definition}
\label{def:grs}
 For any $ s \in \CC $, 
    the \textbf{ordinary affine Grassmannian} $\Gr_s = G(\cK_s)/G(\cO_s)$ \textbf{at} $ s $. 
\end{definition}    
As above, we have modular and lattice descriptions:
\begin{gather*}
\Gr_s = 
    \left\{ (
        V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ D_s $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D^\times_s $} 
    \right\}\,, \\
\Gr_s = 
    \left\{ 
        L \subset \cK_s^m : \text{ $L$ is a $\cO_s$-lattice} 
    \right\} \,. 
\end{gather*}
% 
% TODO:
% consider decorating lattice, modular, group-theoretic descriptions? 
% paper too short for it, ok. 
% but it's a little weird how we do 
% $$ X = A \\ X = Y $$ 
% Hmm.. maybe it's alright
% I was going to suggest we do $$ X = A \\ = B $$ instead 
% 
\begin{definition}
\label{def:grth}
The \textbf{thick affine Grassmannian} $\Grth = G(\Kinf)/G(\CC[t])$.
\end{definition}
Again, we have the following modular and lattice descriptions:
\begin{gather*}
\Grth = 
    \left\{ 
        (V, \varphi) : \text{$V$ is a rank $m$ vector bundle on $ \PP $, $\varphi : V \xrightarrow{\sim} \Vtriv $ on $ D_\infty $ } 
    \right\}\,, \\
\Grth = 
    \left\{ 
        L : L \subset  \Kinf^m \text{ is a $\CC[t]$-lattice} 
    \right\}\,.
    % \CC\xT[t^{-1}]^m 
\end{gather*}
% \jcom{Maybe write $\Kinf$ for $ \CC\xT[t^{-1}] $?  Also maybe change the ``thick'' notation''.} \acom{Since $\sf th$ is also the first two letters of ``thin''?} \jcom{That's a good point!  Maybe we could use some kind of bold  Gr (``thick'' letters) or maybe that would be too confusing.} \acom{Ok, it's tentative.}
% 
\begin{definition} 
\label{def:bdgr}
The two point \textbf{Beilinson--Drinfeld Grassmannian} $$\pi : \Grbd\to \AA$$ with one point fixed at 0, and the second point $s\in\AA$ varying.
\end{definition}
% 
It is described in modular terms by 
$$
\Grbd = 
    \left\{ 
        (V,\varphi,s) : V\text{ is a rank $m$ vector bundle on }\PP, \varphi : V \xrightarrow{\sim} \Vtriv \text{ on } \PP \setminus \{0, s\}  
    \right\}\,. 
$$
% 
The fibre of $\Grbd \to \AA$ over $ s \in \AA $ will be denoted $ \Gr_{0,s} $ and is given by
% \rcom{why do we have the $s\ne 0$ restriction here} \jcom{it would work when $ s = 0$, but it looks a bit strange there.}
% 
$$ 
    G(\CC[t, t^{-1}, (t-s)^{-1}]/G(\CC[t])\,. % := \pi^{-1}(s) 
$$
% \acom{What is the modular description of the fibre?} \jcom{That is written above (just fix $ s$).}
% 
% \acom{0 in notation is kinda redundant? But we can't take it away because we already have a $\Gr_s$ and it is something else?}
% \jcom{Yes, that is one reason.  But also, there was a comment of yours before in the file, saying that you sometimes got confused that one point was fixed at 0, so this is a good notation to remind us (and the reader).  For the same reason, maybe we should write $ \Gr_{0, \AA}$ instead of $ \Gr_\AA$.}
% \acom{lol}
We also have the lattice descriptions:
% 
\begin{gather*}
\Grbd = 
    \left\{ 
        (L,s) : L \subset  \CC(t)^m\text{ is a $\CC[t]$-lattice trivial at any }a \ne 0, s 
    \right\}\,, \\
% , $L$ is 
\Gr_{0,s} = 
    \left\{ 
        L : L \subset  \CC[t,t^{-1},(t-s)^{-1}]^m \text{ is a $\CC[t]$-lattice} 
    \right\}\,.
\end{gather*}
% 
\begin{definition} 
\label{def:grplus}
The \textbf{positive parts} of $\Gr $ and $\Grth$,  are defined by 
\begin{gather*}
    \Gr^+ = \left(M_m(\cO) \cap G(\cK)\right) / G(\cO) \\
    % \text{ and }      
    {\Grth}^+ = \left(M_m(\CC[t]) \cap G(\Kinf)\right) / G(\CC[t])\,. 
\end{gather*}
\end{definition}
% 
In modular terms, $\Gr^+$ (resp.\ ${\Grth}^+$) is the set of those $ (V, \varphi)$ where $ \varphi : V \rightarrow \Vtriv $ extends to an inclusion of coherent sheaves over $ D_0 $ (resp.\ over $ \PP$).

In lattice terms, $ \Gr^+$ (resp.\ ${\Grth}^+$) contains those lattices $L$ which are contained in $\cO^m$ (resp.\ $\CC[t]^m$).
% \acom{previously called ``trivial''} \jcom{I changed it so that $ R^m $ is called the standard lattice.  But I would like to leave the terminology ``trivial at $ a$'' is that ok, or should we write ``standard at $a$''.} 
% \acom{Oh, I don't know.. is trivialized at $a$? is standardized at $a$? Maybe standard at $a$ is better.. Man I never thought about this interchangeable lingo.} 
% lattice $ L_0(\cO)$ (resp. $L_0(\CC[t])$). % , i.e. $ L \subseteq L_0$.
% 
\subsection{Relations between different affine Grassmannians}
\label{ss:relsbtwgrs}
% 
These different versions of the affine Grassmannian are related as follows.  
% 
\begin{enumerate}
    % TODO: I think it makes more sense to put the first point at the end; it just relates (fibres of) the BD Gr to the ordinary Gr
    % \item There is an isomorphism $ \Gr_s \cong \Gr $ coming from the isomorphism $ \Ks \cong \cK$.
    \item \label{it:bd-th}
    %We have a map from the two point Beilinson--Drinfeld affine Grassmannian to the thick affine Grassmannian $ \Grbd \rightarrow \Grth $.
    % $$ \Grbd \rightarrow \Grth $$ 
    We define a map $ \Grbd \rightarrow \Grth $ by 
    % \acom{Don't we also want to restrict $V$} 
    % Joel says nope, restriction of vector bundle corresponds to specialization of lattice; here the map is inclusion  
    $$
        (V,\varphi,s)\mapsto (V, \varphi \big|_{D_\infty})\,.
    $$
    % 
    % It is given in modular terms by restricting the trivialization to $ D_\infty$. \acom{Does it mean $(V,\varphi,s)\mapsto (V\big|_{D_\infty},\varphi\big|_{D_\infty})$}
    In group-theoretic terms, on the fibre over $ s $, this is the inclusion
    $$
        G(\CC[t, t^{-1}, (t-s)^{-1}])/ G(\CC[t]) \rightarrow G(\Kinf)/G(\CC[t])\,. 
    $$
    % \acom{Permission to update notation and replace $\CC\xT[t^{-1}]$ by $\Kinf$ as per your suggestion Joel?}
    % \jcom{Permission granted!}
    % \acom{replaced}
    % 
    Finally, it is given in lattice terms on the fibre over $s$ as the identity on the lattice $(L,s)\mapsto L$ and using the inclusion $\CC[t, t^{-1}, (t-s)^{-1}]^m \rightarrow \Kinf^m$ on the ambient spaces.
    \item \label{it:polyno-taylor}
    The fibres $ \Gr_{0,s}$ of $ \Grbd \rightarrow \AA$ can be described as % are given as follows
    \begin{equation*}
    % \label{eq:grbdgrs}
    \Gr_{0,s} \cong 
        \begin{cases} 
            \Gr \times \Gr_s & s \ne 0 \\
            \Gr              & s = 0\,.
        \end{cases}
    \end{equation*}
    This isomorphism is given in the modular realization by restricting the vector bundle and trivialization.    
    
    Suppose that $ s \ne 0 $.  In the lattice realization, this is given by forming the specializations 
    $$
        L \mapsto (L(0), L(s))\,.
    $$
    Note that if $ L = g \CC[t]^m$ for $ g \in G([t,t^{-1}, (t-s)^{-1}])$, then 
    $$
        (L(0), L(s)) = (g \cO^m, g\cO^m_s)\,. 
    $$
    % 
    % \jcom{I restated this again.  I think that it is simpler and more clear like this.  And I think that this is the fact that we end up using later.}
    % 
    % in the $ s \ne 0 $ case.
    % \jcom{Maybe we should introduce some notation for this isomorphism.}
    % \acom{Ok. Question: how is the isomorphism given at $s=0$?}
    % \acom{Ok. TODO. Also write this in group theoretic terms. Probably $[g] \mapsto ([g],[g])$ is ok. Since $g\CC[t]\otimes_{\CC[t]}\cO_s \cong g\cO_s $ for some reason, where $g\in\CC[t,t^{-1},(t-s)^{-1}]$ can be viewed as $g\in\cK_s$.}
    % 
    %    Finally, in group-theoretic terms, it is given (on representatives) by $g\mapsto (g,g)$. \acom{Reinstated the map. Choosing to supress the coset in our notation makes it potentially ambiguous. Maybe we can say inclusion $\times$ inclusion? In any case we should say something. Else, what are we referring to when we say ``This is compatible'' in the next line?}
    %    This is compatible with the lattice description since for $ g \in G(\CC[t, t^{-1}, (t-s)^{-1}]) $, we have
    %    $$
    %    \left( g \CC[t]^m \right) \otimes_{\CC[t]} \cO \cong g \cO^m
    %    $$
    %    and similarly for $ L(s)$.
    % 
    In the $ s = 0 $ case, the isomorphism is described in the same way, except that we just need to form $ L(0)$.
    
    \item \label{it:grs-gr} There is an isomorphism $ \Gr_s \cong \Gr $ coming from the isomorphism $ \Ks \cong \cK$. 
    
    Combining with the above point, we obtain an isomorphism
    $$
    \tau_s : \Gr_{0,s} \rightarrow \Gr \times \Gr 
    $$
    % 
    %    \jcom{I tried to answer your question, but didn't explain it well.  When $ s = 0$, the map is the same except that $ L(0) = L(s) $ , so we only need $L(0)$.}
\end{enumerate}

\jcom{I moved the ``composition'' of 2 and 3 into 3 and named it $ \tau_s$, since we will use it later quite a bit.  I'm not that happy with the layout of this section, so feel free to reorganize it (maybe into Propositions?)}

\subsection{The fusion construction}
The following construction will be very important in this paper.  

Let $ \Gr_{0, \CC^\times} $ denote the preimage of $\AA \setminus \{0\}$ under the map $\Gr_{0,\AA} \rightarrow \AA $.  

%Composing the isomorphisms relating $\Grbd$ to $\Gr_s$ and $\Gr_s$ to $\Gr$, (\cref{it:polyno-taylor,it:grs-gr} of \cref{ss:relsbtwgrs} preceding), we obtain an isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr$.
% TODO: cf https://tex.stackexchange.com/questions/256849/cleveref-change-behaviour-of-cref-to-use-the-abbreviated-form

These isomorphisms $ \tau_s $ defined above glue together to a projection map
\begin{equation}
\label{eq:mvitau}
    \tau : \Gr_{0, \CC^\times} \rightarrow \Gr \times \Gr \, .
\end{equation}
(This $\tau$ matches the map $ \tau $ of \cite{mirkovic2007geometric}, except that we are working over $ \{0 \} \times \CC^\times$ instead of the complement of the diagonal in $ \CC^2 $.)


Let $ X_1, X_2 \subset \Gr$ be two subschemes, and consider the subscheme $ \tau^{-1}(X_1 \times X_2) \subset \Gr_{0, \CC^\times} $.
% 
% \jcom{I changed this a bit in order to match Mirkovi\'c--Vilonen better.}
% \acom{We say ``we define the subscheme..'' but what is there to define? Should it say ``we consider'' maybe?}
Via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $,  for each $ s \ne 0$, we obtain a subscheme $ X_1 \times X_2 \times \AA^\times $  of $\Grbd$.  

We define $ X_1 \ast_{\AA} X_2 $ to be the scheme-theoretic closure of $  \tau^{-1}(X_1 \times X_2) $ inside of $ \Grbd $.  By construction, this is a flat family over $ \AA$.  We write $X_1 \ast_s X_2 $ for the scheme-theoretic fibre over a point $ s \in \AA$.  

If $ s \ne 0$, then the map $ \tau_s$ identifies $ X_1 \ast_s X_2 \subset \Gr_{0,s}$ with $ X_1 \times X_2 \subset \Gr \times \Gr$.

On the other hand, the fibre $ X_1 \ast_0 X_2$ is a subscheme of $ \Gr_{0,0} $, but regarded as a subscheme of $ \Gr $ using {the isomorphism} $\Gr_{0,0} \cong \Gr $.  We call this subscheme the \textbf{fusion} of $ X_1 $ and $ X_2$.

% \jcom{I rewrote this to introduce the $ X_1 \ast_s X_2 $ notation that we need for later.  In doing so, $ X_1 \circ X_2 $ became obsolete, since we now have $ X_1 \ast_0 X_2$, but if you like we could just define $X_1 \circ X_2 = X_1 \ast_0 X_2 $.}

%Next, we consider the projection $ \pi: \Grbd \rightarrow \AA $, and define 
% $ X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) \subset \Gr_{0,0} $, 
%\begin{equation*}
    % \label{deservesdisplay}
%    X_1 \circ X_2 = (X_1 \ast_\AA X_2) \cap \pi^{-1}(0) 
%\end{equation*}
%which we call the \textbf{fusion} of $ X_1 $ and $X_2$. It is contained in $\Gr_{0,0}$ but regarded as a subscheme of $ \Gr $ using {the isomorphism} $\Gr_{0,0} \cong \Gr $.  
% 
%By construction, 
% \acom{technically, there were two given above, so *these* constructions?} 
%$X_1 \ast_{\AA} X_2 $ is a flat family over $\AA$ whose fibre away from $0$ is $X_1 \times X_2$ and whose fibre over $ 0$ is $ X_1 \circ X_2$. 

% \acom{Ah, sorry --- so do we introduce a even more general notion of $\ast_\AA$ for arbitrary flat families? Looking at Eisenbud's geoemtry of schemes (where flat iff tor-free actually shows up as Exercise II-33!) there does not seem to be anything quite like $\ast_\AA$ or $\circ$ defined.}

% \jcom{I'm not sure how a general notion would be defined.  In our case, we are using the special fact that the general fibre is a product.}

% \acom{I mean exactly in the case of this special fact, which is not special to the BD Gr.}

% Cf. https://math.stackexchange.com/questions/1681680/flat-family-limit-of-intersection-vs-intersection-of-limits
% 
% 
% Begin OLD
%\acom{Question abt general fact: why is scheme-theoretic closure of product flat?} \jcom{See Hartshorne Proposition III.9.8.  I just taught this on the last day of my algebraic geometry class!} \acom{Cool! That Proposition is a bit more general too..}
% 
% \acom{Repeat question: what is this iso? Roger says maybe it's in MV?}
% \jcom{Now answered above.}
% 
%\jcom{I thought that it would be good to introduce this operation now, since we will use it often in the paper and it is the most convenient way to define $\Gr^{\lambda_1, \lambda_2}$. I don't know what notation we should use.}
% 
%\acom{Roger and I have been using $\ast$ to denote fusion. Do we need a notation for the scheme-theoretic closure of $X_1\times X_2\times\AA^\times$? If not then I like $X_1\ast X_2 = \overline{X_1 \times X_2 \times \AA^\times}\cap \pi^{-1}(0)$.}
%\jcom{I think that we will need notations for both things, but I'm happy to use $ \ast$ for the central fibre and use something else for the whole family.} \acom{Ok, I have changed it to notation from Roger's thesis/AK. Only now I am confused as to what is being called fusion---why do we not take the top dimensional subscheme?}
%\jcom{I guess that there are two things that one could mean by fusion of MV cycles.  This scheme $ X_1 \circ X_2$ or the expression which appear in Theorem 7.11 of mvbasis, which is a linear combination of the top-dimensional components of this scheme with multiplicites.  In the general context of this section ($X_1, X_2$ are not necessarily MV cycles), then I guess it is best to talk about the subscheme.  Maybe we can call the expression appearing Theorem 7.11 the ``numerical fusion'' of MV cycles.  By the way, somewhere in this paper we should explain Theorem 7.11 and say that this gives us motivation to study fusion of MV cycles.}
%\acom{Sure! It is quoted at the end, in \Cref{s:denouement} as an application. Addendum: for the numerical fusion I suggest simply $[Z_1][Z_2]$. This is what I will use in my talk. }
% END OLD
% 
\subsection{Some subvarieties of affine Grassmannians}
\label{ss:subgrs}
Now fix effective dominant coweights $\lambda', \lambda'' $ of sizes $N',N''$  and their sum $ \lambda = \lambda' + \lambda'' $ of size $ N = N'+N''$. 
% Coweights define elements in the various affine Grassmannians which we consider. Warning: We will write only coset representatives for elements of the various Grassmannians; we hope that it is evident which right cosets they represent. 
% the right cosets which they represent are clear from the context.
We consider the following subschemes of the affine Grassmannians.
% 
\begin{definition}
\label{def:sphschub}
    The \textbf{spherical Schubert cell} $\Gr^\lambda = G(\cO) t^\lambda$ and its closure 
    $ \overline\Gr^\lambda = \bigcup_{\gamma \le \lambda} \Gr^{\gamma} $ 
    a \textbf{spherical Schubert variety}.  
\end{definition}
    % \acom{do we prefer $\overline{\Gr}^\lambda$ to $\overline{\Gr^\lambda}$? I am for the former! Though it is less accurate??} \jcom{I'm not sure which is better.  I was ``testing'' it out.}
    % 
\begin{definition}
\label{def:sphfus}
    The \textbf{fusion of two spherical Schubert varieties} 
    $$ 
        \overline{\Gr}^{\lambda', \lambda''}_{0, \AA} = \overline{\Gr}^{\lambda'} \ast_\AA \overline{\Gr}^{\lambda''} \rightarrow \AA \,.
    $$
\end{definition}
% \acom{I guess that it is good to have the notation here, in place of $\overline{\overline\Gr^{\lambda'}\times\overline\Gr^{\lambda''}\times\AA^\times}$..}
%\acom{should we also subsript this family with $0,\AA$?}
%By construction the fibre $ \overline{\Gr}_{0,s}^{\lambda', \lambda''} $ of $\overline{\Gr}^{\lambda', \lambda''}_{0, \AA}$ for $ s \ne 0 $ is $ \overline{\Gr}^{\lambda'} \times \overline{\Gr}^{\lambda''}$.  
% By \cite[Proposition 3.1.14]{zhu2016introduction},
% Zhu's theorem (\jcom{need ref} \acom{pretty sure it's }), 
% the fibre over $ 0 $ is $ \overline{\Gr}^{\lambda' + \lambda''}$.
By a theorem of Zhu 
\cite[Proposition 3.1.14]{zhu2016introduction}, $ \overline{\Gr}^{\lambda'} \ast_0 \overline{\Gr}^{\lambda''} $ is reduced and equal to  $ \overline{\Gr}^{\lambda' + \lambda''}$.

% Suppose that $ s \ne 0$.  
Moreover, if $s\ne0$, then in the fibre $\overline{\Gr}_{0,s}^{\lambda', \lambda''}$ we have the open locus $ \Gr_{0,s}^{\lambda', \lambda''}$ coming from transporting $ \Gr^{\lambda'} \times \Gr^{\lambda''} $ via the isomorphism $ \Gr_{0,s} \cong \Gr \times \Gr_s $. 
% \acom{reference \cref{it:grs-gr}?} 
This open locus is a $ G(\CC[t])$-orbit
% .  The lattice description of $ \Gr_{0,s}^{\lambda', \lambda''}$ 
whose lattice description is given in \Cref{le:Grl1l2} below.
% \acom{Why $G(\CC[t])$. Because we are on the left side!}

Because we consider effective dominant coweights, the spherical Schubert varieties and their fusions lie in the positive part of the (thick) affine Grassmannian.
% 
\begin{lemma}
\label{le:sphfusispos}
    The map $ \Gr_{0, \AA} \rightarrow \Grth$ restricts to a map $ \overline{\Gr}^{\lambda', \lambda''}_{0, \AA} \rightarrow \Grth^+$.
\end{lemma}
\begin{proof}
    Let $ s \ne 0 $. Since $ \lambda', \lambda'' $ are effective, $ t^{\lambda'}, (t-s)^{\lambda''} \in \Grth^+ $.  Since $\Grth^+$ is closed and invariant under the action of $ G(\CC[t])$,  $ \overline{\Gr}_{0,s}^{\lambda', \lambda''} \subset \Grth^+$.
    
    For the $ s = 0$ fibre, a similar reasoning applies (or we can simply conclude by taking closure).
\end{proof}
    % \acom{why ``and so''? bc orbits are connected?}
    % \jcom{I turned this into a Lemma.}

%As above, since $ \lambda', \lambda'' $ are effective, the map $ \acom{What is ``as above''? Also, we have only described the open loci in fibres over $s\ne0$.}

Now fix two effective (possibly non-dominant) coweights $ \mu', \mu''$ of sizes $N',N''$ such that $ \mu = \mu' + \mu'' $ is dominant. 
% (We do not require $ \mu', \mu'' $ to be dominant.) 
Using $ \mu, \mu', \mu''$, we define the following subschemes.

\begin{definition}
\label{def:klslice}
    The \textbf{Kazhdan--Lusztig slice} $\cW_\mu = G_1(\Oinf) t^\mu \subset \Grth $.  
\end{definition}
% 
\acom{we haven't defined $G_1$}

In modular terms, $\cW_\mu$ corresponds to the locus of those $ (V, \varphi)$ such that $ V $ is isomorphic to the trivial vector bundle on $ \PP$ and such that $ \varphi$ preserves the {Harder--Narasimhan filtration of $V$ at $ \infty$}. The lattice description of $ \cW_\mu \cap \Grth$ is given in Lemma \ref{le:Wmu} below. 
\acom{First lattice description that doesn't come for free?}
% 
% \acom{que est-ce?} \jcom{It's not very relevant for this paper.  Here is an explanation: every vector bundle on $\PP$ (or maybe any curve) has a special filtration called the HN--filtration; it is a partial flag of subbundles of type determined by $ \mu$.  We require that $ \varphi$ takes this filtration to the standard partial flag at the fibre over $ \infty$.  Maybe ``preserves'' is not the best word above.}
% A: Ok thanks!
% 
\begin{definition} 
\label{def:inftyorbits}
    The \textbf{semi-infinite orbit} 
$$ S^\mu = N_-(\cK)t^\mu \subset \Gr $$ 
and the \textbf{fusion of  semi-infinite orbits} 
$$ 
    S^{\mu', \mu''}_{0,\AA} = S^{\mu'} \ast_\AA S^{\mu''} \to \AA \,.
$$  
\end{definition}
If $ s \ne 0 $, then the fibre $S^{\mu', \mu''}_{0,s} $ is given by
$$
% S^{\mu', \mu''}_{0,s} = 
    N_-(\CC[t, t^{-1}, (t-s)^{-1}])t^{\mu'} (t-s)^{\mu''} G(\CC[t])\subset G(\CC[t,t^{-1}, (t-s)^{-1}]) / G(\CC[t]) \,. % \G(\CC[t])
$$
% TODO: resolve
% \jcom{Maybe we should introduce the $ L_{\mu', \mu''} $ notation, but I'm not sure it is necessary.}
On the other hand, if $ s = 0$, then the fibre $S^{\mu', \mu''}_{0,0} $ equals $ S^{\mu' + \mu''}$. 
% TODO: resolve
% \acom{changed $\mu$ to $\mu' + \mu''$ since we wrote it out for $\overline{\Gr}_{0,\AA}^{\lambda', \lambda''} $}
% 
\jcom{I realized that this is not correct.  We can't just take the fusion here since we don't want to get the closure of these guys.  I will think about this further.}
\acom{Two definitions in one. Unlike for spherical schubs}

\section{Matrices}
\label{s:mats}
% 
\subsection{Adjoint orbits and their deformations}
\label{ss:familiesofadjointorbits}
% 
We now consider some subvarieties of the space of $ N\times N$ matrices, again using the $ \lambda, \lambda', \lambda'', \mu, \mu', \mu''$ defined in the previous section.  Recall that $\lambda, \mu $ are partitions of $ N$.

For $ s \in \CC$ we write $ J_{s,\lambda}$ for the Jordan form matrix with eigenvalue $ s$ and Jordan blocks of sizes $ \lambda_1, \dots, \lambda_m$.
    
\begin{definition}
\label{def:Olam}
The nilpotent orbit $ \OO^\lambda \subset M_N(\CC)$ of matrices conjugate to $ J_{0,\lambda}$.
\end{definition}  
% 
% Temporarily commenting out, don't use
% Its closure is $ \overline{\OO}^\lambda = \bigcup_{\gamma \le \lambda} \OO^{\gamma}$.
% 
\begin{definition}
\label{def:Olamlam}
    For $ s \in \CC, s \ne 0$, the adjoint orbit $ \OO^{\lambda', \lambda''}_{0,s}$ of matrices conjugate to $ J_{0,\lambda'} \oplus J_{s,\lambda''}$.
\end{definition}  
% 
These matrices and the linear operators they represent will be said to have Jordan type $((0,\lambda'), (s,\lambda''))$.

%A linear operator $ T : V \rightarrow V $ on an $N$-dimensional vector space $V$ is said to have Jordan type $((0,\lambda'), (s,\lambda''))$ if its matrix representatives lie in $ \OO^{\lambda', \lambda''}_{0,s}$.
% \acom{secret definition; add `a matrix is likewise said to have Jordan type...if it lies in'?}
% \jcom{Is that better?}
% \acom{Yay.}
% 

% The closure $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ is $\bigcup_{\gamma' \le \lambda', \gamma'' \le \lambda''} \OO^{\gamma', \gamma''}_{0,s}$.     
% \jcom{Do we need this?}
     
We recall that these adjoint orbit closures are given by rank conditions.  
More precisely we have
$$
    \overline{\OO}^\lambda = \{ A \in M_N(\CC) : \rk A^i \le N - \#\text{~boxes in first $i$ columns of }\lambda \}
$$
and
\begin{equation} 
\label{eq:ranks}
\begin{split}
    % 
    \overline{\OO}^{\lambda', \lambda''}_{0,s} = \{ A \in M_N(\CC) : \rk A^i &\le N - 
    \#\text{~boxes in first $i$ columns of }\lambda' \\
    \rk (A-s)^i &\le N - \#\text{~boxes in first $i$ columns of }\lambda'' \}\,. 
\end{split}
\end{equation}
%  

The following fact seems to be well-known but we could not find this exact statement in the literature.
\begin{proposition} 
    \label{prop:adjoint}
    There exists a flat family $\overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow \AA$ whose fibre over $s \in \AA$ is reduced and given by $\overline{\OO}^{\lambda', \lambda''}_{0,s}$ if $s \ne 0 $ and $\overline{\OO}^\lambda $ if $ s = 0$.
\end{proposition}
% 
In order to prove this result, we will need to recall some results from Eisenbud--Saltman \cite{eisenbud1989rank}.
% 

Let $ r $ be a decreasing, concave, non-negative function with $r(0) = N$ (called a rank function).  Let $k $ be maximal such that $ r(k) \ne 0 $.
% \jcom{I don't know if it is really necessary to introduce this $ k$.  Later it will be the number of columns of $ \lambda$, but we could just regard $ \lambda$ to have some columns of size 0.} \acom{And make it $N$? Was $k = m$ replaced because $m$ is already used?} \jcom{Yes and yes} \acom{I like the idea of making it $N$}

Let $ W$ be a subspace of $ \AA^k$. Eisenbud \& Saltman define a flat family $ X_{r,W} \rightarrow W$, whose fibres are reduced and defined by rank conditions. We will now describe these fibres.

Let $ z \in \AA^k$. Some of the coordinates of $z$ may be equal; we introduce some data to keep track of these equalities. There exist $ 0=k_0< k_1< \dots < k_\ell = k $, distinct $ s_1, \dots, s_\ell \in \CC$ and a permutation  $ p $ of $ \{1, \dots, k\}$ such that 
$$ s_j = z_{p(k_{j-1}+1)} = \cdots = z_{p(k_j)}\,. $$ We require that $ p $ is of minimal length, given the choice of $ s_1, \dots, s_\ell$.  The data $s_\bullet, p, k_\bullet$ is unique up to a permutation of $ \{1, \dots, \ell\}$.
% 
%\jcom{Eisenbud--Saltman don't introduce this $ s_j$ notation, but I find it helpful.}
% 
%\acom{Agreed. Maybe we mention that the point of $p$ is to group the repeated eigenvalues? It wouldn't be the most obvious thing we point out.}
%\jcom{That is what I was trying to say with ``introduce some data \dots''  Maybe it could be more clear.  I also clarified the uniqueness.}
%\acom{Oh, sorry, I see it. Maybe because it was the start of a new paragraph I didn't connect `data' and `ranks' at first glance.}

Next for $ j = 1, \dots \ell $ and $ i = 1, \dots, k_{j+1} - k_j$, we define
$$
    % r_{z,j}(i):= N - r(p(m_j)) + r(p(m_j + i)) 
    r_{z,j}(i):= N + \sum_{a = 1}^i r(p(k_j + a)) - r(p(k_j +a) - 1)  \, . 
$$
\jcom{This is the same as what ES denote $r(i,j)$ except the roles of $ i, j$ swapped.  We could swap them back.}
%\jcom{I fixed it now.  I didn't want to use ES's $a_i$.} 
%\acom{\emoji{+1}}

% 
By \cite[Corollary 2.2]{eisenbud1989rank}, the fibre of the flat family $ X_{r,W} $ over $ z$ is given by
\begin{equation} 
    \label{eq:ESfibre}
    X_{r,z} = \{ A \in M_N(\CC) :\rk (A-s_j)^i\le r_{z,j}(i) \text{ for all } i , j \text{ as above}\}.
\end{equation}
% 
We will now apply these ideas to prove our \Cref{prop:adjoint}.
% 
\begin{proof}
% 
%\begin{lemma}
%    \label{lem:es}
%    Let $\lambda = \lambda' + \lambda''$ be partitions. There exists a permutation of the columns of $\lambda$ such that the first $\lambda'_1$ columns are precisely the columns of $\lambda'$ and the last $\lambda''_1$ columns are precisely the columns of $\lambda''$. 
%\end{lemma}
%Recall that $\lambda' + \lambda'' = \lambda$ is a partition of $N$. 
% 
Define 
$$
    r(i) = N - \#\text{~boxes in first $i$ columns of }\lambda\,.
$$
It is easy to see that this is a rank function, with $ k = \lambda_1$, the number of columns of $ \lambda$.
%\begin{lemma}
%   The function $r$
%    defined by $r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$ 
%is a rank function. 
%\end{lemma}

Since $\lambda = \lambda' + \lambda''$, there exists a permutation $ p $ of $ \{1, \dots, k\}$ (the columns of $ \lambda$) such that $ p(1), \dots, p(k_1) $ are the columns of $ \lambda'$ and $ p(k_1+1), \dots, p(k)$ are the columns of $ \lambda''$. Here $ k_1 = \lambda'_1$ the number of columns of $ \lambda'$.

For $ s \in \CC$, we define $ z(s) \in \CC^k$, with $ z(s)_{p(j)} = 0 $ for $ j = 1, \dots, k_1 $ and $ z(s)_{p(j)} = s $ for $ j = k_1 + 1, \dots, k$.

For $ s \ne 0$, we see that the equalities in $ z(s) $ give rise to the data of $ \ell = 2$, $k_1$, $s_1 = 0$, $s_2 = s$, and the permutation $p $.
We also see that
\begin{equation} 
\label{eq:rcols}
    \begin{split}
            r_{z(s), 1}(i) &=  N - \#\text{~boxes in first $i$ columns of }\lambda' \\
            r_{z(s), 2}(i) &=  N - \#\text{~boxes in first $i$ columns of }\lambda'' \,. 
    \end{split}
\end{equation}
On the other hand, for $z(0)$, we get that $ r = 1$ and that 
\begin{equation} 
\label{eq:rcol2}
    r_{z(0),1}(i) = 
    r(i) = N - \#\text{~boxes in first $i$ columns of }\lambda\,. 
\end{equation}
%\jcom{Someone should check that I chose the notation correctly to make this work out.}
% 
%\acom{What do we mean by equalities in $z(s)$ reproduce $p$? I thought $z(s)$ was defined by $p$. Also, $r_{z(s),1}(i) = N- r(p(k_1)) + r(p(k_1 + i)) = N - (N-\#\text{boxes in first }p(k_1)\text{ cols} + \cdots)\ne $~\Cref{eq:rcols}. We want to apply $p$ to $\{1,\dots,i\}$ and not just $i$. I guess we could clarify/declare that $r(p(i)) = N - \#$ boxes in columns $p(1),\dots,p(i)$. For a minute I thought it was actually fine, but what we have is: $r_{z(s),j}(i) = N - (\#\text{boxes in cols }1,\dots,p(k_j + i) - \#\text{boxes in cols }1,\dots,p(k_j))$ and what we want is $\dots (\#\text{boxes in cols }p(1),\dots,p(k_j + i) - \#\text{boxes in cols }p(1),\dots,p(k_j + i))$ right?}
% 
%\jcom{I fixed the definition of $ r_{z,j}(i)$, so now it is correct, I believe.  I changed the wording of ``reproduce''. I'm trying to say that $ p$ is used to define $ z(s)$ and the we get $ p $ back from $ z(s)$ as the permutation coming from the equalities.}
%\acom{\emoji{+1}}
% 
Let $ W = \{ z(s) : s \in \CC \}$. Combining \Cref{eq:ranks,eq:ESfibre,eq:rcols,eq:rcol2},
% (\ref{eq:ranks}), (\ref{eq:ESfibre}), (\ref{eq:rcols}), (\ref{eq:rcols2}) 
we see that the Eisenbud--Saltman family $ X_{r, W} \rightarrow W $ gives our family $ \overline{\OO}^{\lambda', \lambda''}_{0, \AA}$.
\end{proof}
% 
% 
% The family of adjoint orbits $  whose fibre over $ s \ne 0 $ is $ \overline{\OO}^{\lambda', \lambda''}_{0,s}$ and whose \acom{Claim:} fibre over 0 is $\overline{\OO}^\lambda$ (a consequence of Eisenbud and Salten's characterization of ``rank varieties'' as proved in \Cref{cor:es}). \acom{TODO: make into definition to be x-referenced in proof of description of fibres}
% 
% \jcom{We should justify why the 0 fibre is correct scheme-theoretically.} 
%\acom{does it follow because ``$s$ divides $\det g$'' is a closed condition 
%in $\AA_s\times \AA_{\det}$? 
%so the set of $A$ which are conjugate to
%$J_{0,\lambda'} \oplus J_{s,\lambda''}$
%by a matrix whose determinant is divisible by $s$ is also closed?}
%\jcom{I didn't follow your argument. One approach would be to write down the generators for the ideal of $\overline{\OO}^{\lambda', \lambda''}_{0,s} $ and then set $ s =0$.  For example, take $ \lambda' = (1) = \lambda'' $ (I know it is a small example!), then the ideal is generated by $ tr(A) = s$, $det(A) = 0$, so when we take $ s =0$, we get just $ tr(A) = 0, det(A) = 0$.  I am hoping that we can find a reference for this result somewhere.}
%\acom{I thought that the point would be to show that a dense subset of matrices which are conjugate to $J_{0,\lambda'}\oplus J_{s,\lambda''}$ limit to matrices which are conjugate to $J_\lambda$. In particular, that the conjugating matrix remains invertible in the $s\to0$ limit.}
% 
% TODO: delete commented out remark and subcomment!
% \begin{remark}
\begin{comment}
Base case $m = 1$: by a slight abuse of notation let $\lambda', \lambda_2 \in \NN$. Claim: $J=J_{0,\lambda'} \oplus J_{s,\lambda_2}$ is conjugate to 

% $\begin{pNiceArray}{CC|CC}[first-row,last-row=5,first-col,last-col,nullify-dots]
% & C_1 & \Cdots & & C_4 & \\
% L_1 & a_{11} & a_{12} & a_{13} & a_{14} & L_1 \\
% \Vdots & a_{21} & a_{22} & a_{23} & a_{24} & \Vdots \\
% \hline
% & a_{31} & a_{32} & a_{33} & a_{34} & \\
% L_4 & a_{41} & a_{42} & a_{43} & a_{44} & L_4 \\
% & C_1 & \Cdots & & C_4 &
% \end{pNiceArray}$
\[
A = \begin{bNiceArray}{ccc|ccc}[columns-width = auto,first-row,last-col,code-for-first-row = \color{blue}\scriptstyle\rotate,code-for-last-col = \color{blue}\scriptstyle,nullify-dots,xdots/line-style=loosely dotted]
1 & \Cdots & \lambda_1 & \lambda_1 + 1 & \Cdots & \lambda_1 + \lambda_2 \\
0 & 1 & 0 & 0 & 0 & 0 & 1\\
0 & \Ddots & 1 & 0 & 0 & 0 & \Vdots \\
0 & 0 & 0 & 1 & 0 & 0 & \lambda_1 \\
\hline
    & & & s & 1 & 0 & \lambda_1 + 1 \\
    & & & 0 & \Ddots & 1 & \Vdots \\
    & & & 0 & 0 & s & \lambda_1 + \lambda_2 
\end{bNiceArray} 
\]
\end{comment}
% Denote by $R_i$ the $i$th row of $J$ 
%Let $g = E_{\lambda_2}\cdots E_1$ be the product of elementary matrices $E_i$ which correspond to the elementary row operations 
%$$R_{\lambda_1 + i - 1} \leftarrow R_{\lambda_1 + i-1} + \frac 1s R_{\lambda_1 + i}$$ 
%for $i = 1,\dots,\lambda_2$. Then $gJg^{-1} = A$. For the next part we introduce the notation $A_{\lambda_1+\lambda_2}$ for this matrix $A$. 
% 
%Now let $\lambda_1,\lambda_2$ be arbitrary. Clearly $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to 
%$$\bigoplus_{i=1}^m (J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}})\,.$$ 
%Moreover each block $J_{\lambda_{1,i}}\oplus J_{\lambda_{2,i}}$ is conjugate to $A_{\lambda_{1,i}+\lambda_{2,i}}$. Altogether $J_{0,\lambda_1} \oplus J_{0,\lambda_2}$ is conjugate to $\bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$ and this latter matrix specializes to the Jordan normal form $J_{0,\lambda_1 + \lambda_2}$ at $s=0$.
% 
% By Tanisaki $A\in \OO^\lambda$ iff the coefficients of $z^m$ in $k\times k$ minors of $zI - A$ for any $m\le p_\lambda(k) - 1$ and for any $k = 1\dots |\lambda|$ vanish. Here $p_\lambda(k) = \lambda_{n-k+1} + \cdots + b_n$ and $k = 1,\dots,n$. So $p_\lambda(k) = \#$ boxes in the last $k$ rows of $\lambda$.
% 
% In particular this result describes the ideal $I_i$ of $N_i\times N_i$ matrices $A_i$ such that $A_i - \delta_{i,2}s$ is conjugate to $J_{\delta_{i,2} s, \lambda_i}$ for $i = 1,2$. Let $A_1,A_2$ be such a pair of matrices. Clearly $A_1 \oplus A_2$ is conjugate to $J_{0,\lambda_1} \oplus J_{s,\lambda_2}$. Let $g$ be such that $g(J_{0,\lambda_1} \oplus J_{s,\lambda_2})g^{-1} = \bigoplus_{i = 1}^m A_{\lambda_{1,i}+\lambda_{2,i}}$. 
% 
% If we apply the change of basis $g$ to the ideal $I=I_1 + I_2$ and set $s = 0$ (if possible) do we recover the ideal of $\OO^{\lambda_1 + \lambda_2}$? Let $A^g = g(A_1 \oplus A_2) g^{-1}$...
% \end{remark}
% 
\subsection{The Mirkovic--Vybornov slice}
\label{ss:mvyslice} 
% 
% Fix the ``$\mu$-numeration'' \((e^1_1,\ldots,e^{\mu_1}_1,\ldots,e^1_m,\ldots,e^{\mu_m}_m)\) of the standard basis of $\CC^N$. 
% \acom{I don't suppose we need this basis now that we're just describing $\TT_\mu$ in words.}
% 
% \begin{definition}
An element of the \textbf{Mirkovi\'c--Vybornov slice} $\TT_\mu$ 
% defined as the set of $A\in M_N(\CC)$ such that 
% \[
%     \begin{aligned}
%         &\text{for all } 1 \le a,s\le m\,,
%         \text{for all } 1\le b\le \mu_a\,, 1\le t\le \mu_s\,, \\
%         &\text{if } 1\le t < \mu_s \text{ or } t = \mu_s < b \le \mu_a \\
%         &\text{then } e^t_s\cdot (A-J_\mu) e^b_a = 0 \,.
%     \end{aligned}    
% \]
% \end{definition}
% \jcom{I'm not a huge fan of this description, but I don't know how to write it better.}
% 
% \acom{Agree. Happy to just have the in-words description.}
% 
% In words, $A$ 
is a $\mu$ Jordan form matrix plus a $\mu\times\mu$ block matrix with possibly nonzero entries occuring in the first $\min(\mu_i,\mu_j)$ columns of the last row of each $\mu_i\times\mu_j$ block. 
% $\dim \TT_\mu = N^2 - \dim \OO_\mu$. 
In pictures, and by example, the elements $A$ for $\mu=(3,2,2,1)$ look like 
\[
    \left[
        \begin{BMAT}(e){ccc;cc;c;c}{ccc;cc;c;c}
            & 1 & & & & & \\
            &  & 1 & & & & \\
        A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
            &  & &  & 1 & & \\
            A_{21}^1 & A_{21}^2 & & A_{22}^1 & A_{22}^2 & A_{23}^1 & A_{24}^1 \\
            A_{31}^1 & & & A_{32}^1 & & A_{33}^1 & A_{34}^1 \\
            A_{41}^1 & & & A_{42}^1 & & A_{43}^1 & A_{43}^1
    \end{BMAT}
    \right]\,. 
\]
% 

To each $ A \in \TT_\mu$, we will associate the $m\times m$ matrix 
of polynomials 
% $ g(A) = \left( g(A)_{ij} \right)$ 
$g(A)$
in $ M_m(\CC[t]) $ whose $(i,j)$th entry is defined as follows.
\begin{equation}
    \label{eq:mvyofa}
    g(A)_{ij} = 
\begin{cases} t^{\mu_i} - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i = j \\
        % \text{ if $ i = j$} \\
        - \sum_{k=1}^{\mu_i} A^k_{ji} t^{k-1} & i \ne j
    %  \text{ if $ i \ne j$ }
\end{cases}
\end{equation}
where $A^k_{ji}$ is the $k$th entry from the left of the last row of the $\mu_j\times\mu_i$ block of $A$. 
For example,
\[
    \left[
        \begin{BMAT}(e){ccc;cc}{ccc;cc} 
        0 & 1 & 0 & 0 & 0\\
        0 & 0 & 1 & 0 & 0\\
        A_{11}^1 & A_{11}^2 & A_{11}^3 & A_{12}^1 & A_{12}^2\\
        0 & 0 & 0 & 0 & 1\\
        A_{21}^1 & A_{21}^2 & 0 & A_{22}^1 & A_{22}^2
        \end{BMAT}\right]    
        \mapsto 
        \left[\begin{array}{rr}
            t^{3} - A_{11}^3 t^2 - A_{11}^2 t - A_{11}^1 & -A_{21}^2t - A_{21}^1  \\
            -A_{12}^2 t - A_{12}^1 & t^{2} - A_{22}^2 t - A_{22}^1
        \end{array}
        \right]\,. 
\]
% For example, the $A$ above will define 
% \acom{the transpose of! TODO. Had script?}
% \[
% \begin{bmatrix}
%     t^3 - A_{11}^1 - A_{11}^2 t - A_{11}^3 t^2 & - A_{12}^1 - A_{12}^2 t & -A_{13}^1 & -A_{14}^1 \\
%     & t^2 \\
%     & & t \\
%     & & & t 
% \end{bmatrix}    
% \]
% 
% \jcom{I moved the definition here since it will be useful for defining the upper triangular stuff.}
% \acom{Ok.}
% 
\acom{Can add sentence before making next definition? To the effect of ``In $\TT_\mu$ we will be interested in a certain family of block upper-triangular matrices.''}
\begin{definition} The ``\textbf{upper-triangular}'' \mvy slice $\UU^{\mu', \mu''}_{0,\AA}\rightarrow \AA $, defined by
$$
\UU^{\mu', \mu''}_{0,\AA} := \{ (A,s) \in \TT_\mu \times \AA : g(A)_{ii} = t^{\mu'_{i}} (t-s)^{\mu''_{i}}, g(A)_{ij} = 0 \text{ for $ j < i $ }\}\,. 
$$
\end{definition}
So a matrix in $\UU^{\mu', \mu''}_{0,\AA}$ is weakly block upper-triangular and its diagonal blocks are given by the companion matrices for the polynomials $t^{\mu'_{i}} (t-s)^{\mu''_{i}}$ ($i=1,\dots,m$).
For example, elements of $ \UU_{0,s}^{(1,1,0),(2,1,1)}$ look like 
% 
% $\UU_{0,0}^{(?),(?)}$ entries look like 
% \[
%     \left[\begin{BMAT}(e){ccc;cc}{ccc;cc} 
%         0 & 1 & 0 & 0 & 0\\
%         0 & 0 & 1 & 0 & 0\\
%         0 & 0 & 0 & A_{12}^1 & A_{12}^2\\
%         0 & 0 & 0 & 0 & 1\\
%         0 & 0 & 0 & 0 & 0
%         \end{BMAT}\right]    
%         % \mapsto 
%         % \left[\begin{array}{rr}
%         %     t^{3} & 0 \\
%         %     -A_{12}^2 t - A_{12}^1 & t^{2}
%         % \end{array}\right]
% \]
\[
    \left[
        \begin{BMAT}(e){ccc;cc;c}{ccc;cc;c} 
        0 & 1 & 0 & 0 & 0 & 0\\
        0 & 0 & 1 & 0 & 0 & 0\\
        0 & -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 \\
        0 & 0 & 0 & 0 & 1 & 0\\
        0 & 0 & 0 & 0 & s & A_{23}^1\\
        0 & 0 & 0 & 0 & 0 & s
        \end{BMAT}
    \right] \,.   
    % \in \UU_{0,s}^{(1,1,0),(2,1,1)}
\]
% 
Note that the fibre $  \UU^{\mu', \mu''}_{0,0}$ is the same as the intersection of $ \TT_\mu $ with the set of strictly upper-triangular matrices. 

\acom{Mention that any other decomposition $\nu' + \nu''$ of $\mu' + \mu''$ will give a slice $\UU^{\nu',\nu''}_{0,\AA}$ with the same zero fibre.}
    
% \jcom{Instead of $\TT^+$ maybe a different letter would be best? I'm moved all $\lambda, \mu$ to upper indices for consistency (except for $ \cW_\mu, \TT_\mu$) and also to let us put $ s $ or $ \AA$ in the lower index.  I haven't been very consistent about $ \AA $ vs $ \CC$.  Which do you prefer?}
% 
% \acom{Maybe $\UU$ for a different letter? And, I guess we do not need $\AA$ but it is a nice letter to have around. It also serves to distinguish the $\CC$ over which $R,K$ are defined from the $\AA$ over which $\Grbd$ is defined?}
% \jcom{Sure $\UU$ is good.}
% \acom{And, sorry, ignore my other comment; it's the same $\CC$ (or $\AA$) nothing to distinguish.}
% 
% TODO: place this better? 
% \acom{The following is a rough note.} 
\begin{comment}
\subsubsection{Detour of rank varieties}
% \begin{theorem}(\cite{eisenbud1989rank})
    % \label{thm:es}
Let $r$ be a decreasing concave non-negative integral function with $r(0) = N$. Such a function is called a rank function. Eisenbud and Saltment associate to a rank function the vareity of $N\times N$ matrices 
% The relevant result from Eisenbud--Saltman seems to be that 
$$X_r = X_{r,0} = \{A:\rk A^i \le r(i) \}$$ 
and show that it is the flat limit of a certain family 
% $$X_{r,\lambda} = \{A : \rk (A-\lambda_i)^j\le r(i,j)\}$$
$$X_{r,z} = \{A : \rk (A-z_i)^j\le r(i,j)\}$$
with $r(i,j)$ determined by $r(i)$.  
% \end{theorem}

\acom{Don't know how much of this setup to recall but here are the parts that will appear in the quoted theorem.}
% 
More precisely, they show that 
\begin{enumerate}
    \item $X_r$ is a normal variety
    \item $X_r$ is Gorenstein with rational singularities
    \item $X_r$ fits into a flat family over $\AA^m$ of normal varieties whose fibre over a point $(z_1,\dots,z_m)$ such that the $z_i$ are all distinct is 
\end{enumerate}
\[
. 
\]
They construct a simultaneous resolution of singularities and use the resolution of $X_r$ to describe the tangent space to $X_r$ at a point corresponding to an endomorphism $A$ such that $\rk A^i = r_i$ for all $i$. As an application of the tangent space computation they show that the general fiber of this family can be written as the scheme-theoretic intersection of varieties like $X_r$. 

To construct the resolution, they take $m$ to be the largest number such that $r(m) \ne r(m+1)$
and consider the affine space $\AA^m$ with coordinates $(x_1,\dots, x_m)$. Given $W\subset \AA^m$ a linear subvariety, 
% TODO: clarify that linear subvariety means to them a (translate of subvectorspace)
they introduce $\mathcal X_{r,W}\subset\End(V) \times W \times F_r$ where $F_r$ is the (paritial) flag variety $F_r = \{(V = V_0 \supset V_1 \supset \cdots \supset V_m \supset 0 ) : \dim V_i = r(i)\}$. 
% 
$\mathcal X_{r,W}\subset\End(V) \times W \times F$ is defined to be the subvariety of triples $(A,\vec z,\{V_i\})$ such that $(A-z_i) V_{i-1} \subset V_i$ for $z_i = x_i(\vec z)$ and $i = 1,\dots,m$. 
The (reduced image of) projection of $\mathcal X_{r,W}$ down to $\End(V) \times W$ is denoted $X_{r,W}$ and it is a closed affine subvariety. 
% 
\begin{theorem}
    (\cite[Theorem~2.1 iii)]{eisenbud1989rank})
    $X_{r,W}$ is the restriction of $X_{r,\AA^m}$ to $W$ and as such it is flat over $W$. 
\end{theorem}

This result is applied to study the fibers of the family $X_{r,\AA^m}$ over points $\vec z \in \AA^m$ for $z_i$ not necessarily distinct. The coordinates $z_i$ of $\vec z$ are partitioned by equality with the help of a permutation $p$ of $\{1,\dots,m\}$ such that 
\begin{align*}
    z_{p(1)} &= \cdots = z_{p(m_1)} , \\
    z_{p(m_1 + 1)} &= \cdots = z_{p(m_2)} , \\ 
    \dots \\
    z_{p(m_s + 1)} &= \cdots = z_{p(m_s + 1)}
\end{align*}
and $p$ preserves the order within each interval $m_i + 1,\dots,m_{i+1}$. \acom{Not sure what this means. Order of coordinates?} With this notation in place Eisenbud and Saltman define the numbers

% 
\begin{corollary}
    \label{cor:es}
    The family $\OO^{\lambda',\lambda''}_{0,\AA}$ defined above has zero fibre $\OO^\lambda$. 
\end{corollary}\begin{gather*}
    r(i,j):= \dim V - a_{p(m_i + 1)} - \cdots - a_{p(m_i + j )} \\
    \qquad \qquad \qquad \qquad \qquad \text{ for } i = 1,\dots,s \text{ and } j = 1,\dots , m_{i + 1 } - m_i 
\end{gather*}
where $a_i = r(i-1) - r(i)$.
It is through the $r(i,j)$ that the rank function $r$ determines the Jordan types of the generalized eigenspaces of the vector of (generalized) eigenvalues $\vec z$. 
\begin{proposition}
    (\cite[Corollary~2.2]{eisenbud1989rank})
    \label{prop:es}
    The scheme-theoretic fibre of $X_{r,W}$ over $\vec z\in W$ is 
    \[
        X_{r,\vec z} = \{A \in \End(V) : \rk (A-z_i)^j\le r(i,j) \text{ for all } i , j \}\,. 
    \]
\end{proposition}
% 
The only work we have to do to apply \Cref{prop:es} is to determine the rank function that will allow us to realize $\OO^\lambda$ as a rank variety, and the family $\OO^{\lambda',\lambda''}_{0,\AA}$ as a deformation of it. 
%  
\begin{lemma}
    \label{lem:es}
    Let $\lambda = \lambda' + \lambda''$ be partitions. There exists a permutation of the columns of $\lambda$ such that the first $\lambda'_1$ columns are precisely the columns of $\lambda'$ and the last $\lambda''_1$ columns are precisely the columns of $\lambda''$. 
\end{lemma}
Recall that $\lambda' + \lambda'' = \lambda$ is a partition of $N$. 
% Let $m_i = \lambda_{i,1}$ ($i = 1,2$), $m = m_1 + m_2$, and define
Let $m = \lambda_1' + \lambda_2'$ and define 
$$r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$$
on $j = 1,\dots,m$. 
\begin{lemma}
   The function $r$
%    defined by $r(j) = N - \#\text{~boxes in first $j$ columns of }\lambda$ 
is a rank function. 
\end{lemma}

The case that we are interested in is the case of two generalized eigenvalues, i.e.\ $\vec z = (z_1,z_2) = (0,s)$. 
\acom{Does order matter?} 
% Let me change notation so that $\lambda_i$ are shapes and 
We begin by considering the open loci where the rank conditions are strict. 

Note that $r(j-1) - r(j) = : a_j$ is the length of the $j$th column of $\lambda$. 
% As in \cite{eisenbud1989rank} we will denote this quantity $a_j$. 
% 
% 
Next, take $p$ to be a permutation of the (lengths of) columns $a_j$ of $\lambda$ such that $a_{p(1)},\dots,a_{p(\lambda_1')}$ are the (lengths of) columns of $\lambda'$ and $a_{p(\lambda_1' + 1)},\dots, a_{p(m)}$ are the (lengths of) columns of $\lambda''$. 
Then the functions 
$$
\begin{aligned}
r(1,j) &= N - a_{p(1)} - \cdots - a_{p(j)} \\
% \text{ and } 
r(2,j) &= N - a_{p(\lambda_1' + 1)} - \cdots - a_{p(\lambda_1' + j)}
\end{aligned}
$$
are such that 
\begin{align*}
    \OO^{\lambda',\lambda''}_{0,s} = X_{r,(0,s)} = \{A:\rk A^j &= r(1,j) \text{ for } i = 1,\dots, \lambda_1' \\ \text{ and } \rk (A-s)^j &= r(2,j) \text{ for } i = \lambda_1' + 1 ,\dots, m \}
\end{align*}
and according to \cite[Theorem 2.1(iii)]{eisenbud1989rank} the family $X_{r,(0,s)}$ is flat over $W = \{x_1 = \cdots = x_{\lambda_1'} = 0 , x_{\lambda_1'} = \cdots = x_m\}$ (a linear subvariety of $\AA^m$) with zero fibre $X_{r,(0,0)} = \OO^\lambda$ as desired. \acom{Do you need to permute the indices defining $W$?}
% $X_{r,\AA^m}$ is flat over $\AA^m$ with zero fibre $X_{r,0} = \OO^\lambda$ as desired. I guess we need to carefully choose the $W \subset \AA^m$ defining our family, which is smaller than the family $X_{r,\AA^m}$. Well, $W$ is just cut out by $0 = x_1 = x_2 = \cdots = x_{m_1}$ and $ x_{m_1 + 1} = \cdots = x_m$, or some permutation thereof to match the permutation $p$ above.
 
% On the one hand, we know that $r(i,j)$ should be equal to $N$ minus the number of boxes in the first $j$ columns of $\lambda_i$. On the other hand, ES define $r(i,j)$ to be $N$ minus the $i$th chunk of $j$ columns determined by the rank function $r(i)$. Also, we would like for $r(i)$ to determine the shape $\lambda = \lambda' + \lambda''$. Take for example $\lambda' = \lambda'' = (2,1)$. Then $\lambda = (4,2)$ and so $r\{0,1,2,3,4\}=\{6,4,2,1,0\}$ and 
% $$\{a_1,a_2,a_3,a_4\} = \{6-4,4-2,2-1,1-0\}=\{2,2,1,1\}\,.$$

% For another example take $\lambda' = (1)$ and $\lambda'' = (1,1)$. Then $\lambda = (2,1)$ and $r\{0,1,2\} = \{3,1,0\}$ so $\{a_1,a_2\} = \{2,1\}$.

% Again too small. The problem is I don't think that it is in general true that some permutation of the list $r(i-1) - r(i)$ will be the list of lengths of columns of $\lambda'$ followed by the list of lengths of columns of $\lambda''$ which is what we are looking for. For one thing there's in general more than one way to write a partition as a sum of two partitions. Ok, concretely, a third example. Take $\lambda' = (a,b)$ and $\lambda'' = (c,d)$ then $\lambda = (a + c, b + d)$ and 
% $$r\{0,1,\dots,a + c\} = \{N, N-2, \dots, N-2(b+d), N-2(b+d) - 1,\dots, 0\}$$
% so 
% $$\{a_1,a_2,\dots\} = \{2,2,\dots,2,1,1,\dots,1\}$$
% with $b+d$ 2s and $a+c - b - d$ 1s. Wait a minute, nevermind, of course this is true. We have two partitions $\lambda'$ and $\lambda''$ and one way to form the partition $\lambda = \lambda' + \lambda''$ is to break $\lambda_i$ down into columns, order the columns by length, and recombine. Cool. I think it's fine. 
% \acom{That's it for the note.}
\end{comment}

\section{Exposition}
\label{s:exposition}
% 
Anderson and Kogan conjectured in \cite{anderson2006algebra} \acom{or earlier?} that those MV polynomials which are cluster monomials for a Fomin--Zelevinsky cluster algebra structure on $\CC[N]$ are naturally expressible as determinants\dots
% and they conjectured a formula for many of them.

It's not clear how this work helps with/relates to AK/their conjectures.

The generalized \mvy is interesting in its own right. \acom{is it?}

Computing fusion still hard but at least boiled down to linear algebra. Cf.\ fusion product as it appears in \cite{beilinson1991quantization,feigin2generalized,mirkovic2007geometric,anderson2006algebra,bezrukavnikov2005equivariant}.

Interesting combinatorics? Presumably it is related to Feigin and Loktev's generalized Kostka polynomials? Though Joel says that the product on SSYT that we are witnessing is unlikely to have a combinatorial description. 

Exchange relations only work on cluster modules where one is a mutation of the other (i.e.\ those corresponding to cluster monomials which are related by mutation). Of course this gives me everything. Up to $A_4$ as in type $A_5$ there exist indecomposable modules which are not cluster, so exchange relations do not apply. 

The hope (conjecture) is that this paper gives a way to compute on such modules. Cf.\ counterexample satisfying $\barD(c_Y) = \barD(b_Z) + 2\barD(b)$, where $b$ is a cluster monomial belonging to both bases and $c_Y$ is the square of a cluster monomial ($b'$ say) belonging to both bases. The $\barD$ equation suggests that $(b')^2 = b_Z + 2b$. We can try to check this using Roger's script. Such calculations bring us closer to answering two questions. First, how forgetful is $\barD$. Second, on the level of modules, are the terms appearing in the fusion product determined by $\Ext^1$ of the factors. By Roger's thesis this is true if the factors are cluster modules related by a mutation. 

% No; $x*y=\sum z\Rightarrow y = \frac 1 x \sum z$. 

Representation theory? 

\section{Rising Action}
\label{s:theproblem}
\begin{lemma} 
\label{le:Grl1l2}
    Let $ L \in \Grth^+ $.  Let $ s \in \AA, s \ne 0 $.  The following are equivalent:
    \begin{enumerate}
        \item $ L $ is in the image of the map $ \Gr^{\lambda', \lambda''}_{0,s} \rightarrow \Grth^+$. %\acom{Should we describe this map somewhere? Earlier?}
        \item The linear operator $ t $ on $ \CC[t]^m/L$ has Jordan type $((0,\lambda'), (s,\lambda''))$.
        \item $ L \in G(\CC[t]) t^{\lambda'} (t-s)^{\lambda''}$.
        % \acom{why no $G(\CC[t])$-coset?}
    \end{enumerate}
\end{lemma}
% 
\begin{proof}
First we recall that for $ L \in \Gr^+$, $ L \in \Gr^{\lambda} = G(\cO)t^\lambda $ if and only if $ t |_{\cO^m/L} $ has Jordan type $ \lambda$. 
% \acom{Change $L_0$}

Now assume that $ (L, s) \in \Gr^{\lambda', \lambda''}_{0,s}$.  By definition, $ L(0) \in \Gr^{\lambda'}$ and $L(s) \in \Gr^{\lambda''} $.  This means that $t $ acting on 
% $ \CC\xt^m / L(0)$ 
$\cO^m/L(0)$ has Jordan type $ \lambda'$ and $ t$ acting on 
% $ \CC\xt[t-s]^m / L(s) $ 
$\cO_s^m/L(s)$ has Jordan type $ \lambda''$.  For $ a = 0, s$,  we see that $$\CC[t]^m/L \otimes_{\CC[t]} \cO_a \cong \cO_a^m / L(a). $$
Thus, Lemma \ref{le:linalg} shows that the map
% $ L_0 / L \rightarrow \CC\xt^m / L(0)$ 
$\CC[t]^m/L\to \cO^m/L(0)$ induces an isomorphism between the $0$-generalized eigenspace of $ t$ and $ \cO^m / L(0)$.   The same thing holds for the $s$-generalized eigenspace of $t $ and $ \cO_s^m/L(s)$. This shows that (i) implies (ii) and the logic can be reversed to see that (ii) implies (i). 
% \acom{is this map $v + L \mapsto v + L(0)$?} \jcom{yes} \marginpar[]{*}
% todo 

On the other hand, if $ L = g t^{\lambda'} (t-s)^{\lambda''} \CC[t]^m$ for some $ g \in G(\CC[t])$, then $ L(0) = g t^{\lambda'}\cO^m $ since 
% $ (t-s)^{\lambda''} \in G(\CC\xt)$. 
$ (t-s)^{\lambda''} \in G(\cO)$. In this way, we see that (iii) implies (ii) and the logic can be reversed to get the equivalence.
\end{proof}

\begin{lemma} 
\label{le:linalg} % aka solution 
Let $ V $ be a $ \CC[t]$-module which is finite-dimensional as a $ \CC$ vector space.  For any  $ a \in \CC$, the map
$$ 
    V  \rightarrow V \otimes_{\CC[t]} \cO_a
$$
restricts to an isomorphism between the generalized $ a $-eigenspace of $ t $ and $ V \otimes_{\CC[t]} \cO_a$
\end{lemma}
% 
\begin{proof}
    For any $ b \in \CC$, let $ E_b $ denote the generalized $b$-eigenspace of $t$.  Then $ V = \oplus_{b \in \CC} E_b$.  Since $ t - b$ is invertible in $ \cO_a$ and $ t -b $ acts nilpotently on $ E_b$, we see that $ E_b \otimes_{\CC[t]} \cO_a = 0 $.
    
    So it suffices to show that $ E_a \rightarrow E_a \otimes_{\CC[t]} \cO_a$ is an isomorphism.  By the classification of modules over $ \CC[t]$, it suffices to check this when  $ E_a = \CC[t]/(t-a)^k$, where it is clearly true.
\end{proof}
% 
\begin{lemma} 
\label{le:Wmu}
Let $ L \in \Grth^+$.  The following are equivalent:
\begin{enumerate}
    \item $ L \in \cW_\mu$.
            \item $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ for some $ v_i $ of the form $ v_i = t^{\mu_i} e_i + \sum_{j=1}^m p_{ij} e_j $ where $ p_{ij} \in \CC[t] $ has degree less than $ \min(\mu_i, \mu_j)$.
    \item  For all $ i $, 
    $$ t^{\mu_i} e_i \in \Sp_\CC(\{t^k e_j : 0 \le k < \min(\mu_i, \mu_j), 1 \le j \le m \}) + L. $$
\end{enumerate}
Moreover, for such $L $, $ \beta_\mu := \{ [t^k e_i] : 0 \le k < \mu_i, 1 \le i \le m\}$ forms a basis for $ \CC[t]^m/L$. 
\end{lemma}
% 
% \jcom{I realized the meaning of the ``extra'' condition from the paper with Sabin and I added it into point 3.}
% \acom{Technically though the extra condition in that paper is weaker? $W_{\mu_i}$ being the span of $e_j,te_j,\dots,t^{\mu_i-1}e_j$ or the complement of $t^{\mu_i}\cO^m$ in $\cO^m$.}
% 
\begin{proof}
    Let $ L \in \cW_\mu$.  Then $ L = \Sp_{\CC[t]}(v_1, \dots, v_m) $ for some $ v_i $ with $ v_i = t^{\mu_i} e_i + \sum_{j=1} q_{ij}t^{\mu_i} e_j $ and $ q_{ij} \in t^{-1} \Oinf$.  Since $ L \in \Grth^+ $, we see that $ v_i \in \CC[t]^m$ which means that $ p_{ij} := q_{ij}t^{\mu_i} $ lies in $ \CC[t]$.  By construction, the polynomial $ p_{ij}$ has degree less than $ \mu_i$.
    
    Fix $ i$ and suppose that for some $ j$, $ \mu_j < \mu_i$.  In this case, we can alter our basis to $ v'_i = v_i - r v_j$ for some polynomial $r \in \CC[t]$.  This gives us new polynomials $ p'_{ij} = p_{ij} - r (t^{\mu_j} + p_{jj}) $.  In this way, we can ensure that $ p_{ij} $ has degree less than $ \min(\mu_i, \mu_j)$.  Thus (i) implies (ii).
    
    Suppose that $ L = \Sp_{\CC[t]}(v_1, \dots, v_m)$ as in (ii).  Then
    $$t^{\mu_i} e_i - v_i \in \Sp_\CC(\{t^k e_j :  k < \min(\mu_i, \mu_j), 1 \le j \le m \})  \,. $$
    Hence (ii) implies (iii).  

    Finally, given (iii), then we can see $ v_i := t^{\mu_i} e_i - \sum_{j=1}^m p_{ij} e_j \in L $ for some $ p_{ij} \in \CC[t]$ of degree less than $ \min(\mu_i,\mu_j) $.  It is easy to see that $ L = \Sp_\cO(v_1, \dots, v_m) $ and so $ L \in \cW_\mu$.  
    
    Finally to show that $ \beta_\mu$ forms a basis for $ \CC[t]^m/L$, it suffices to show that for each $ i$, $ t^{\mu_i} e_i  \in \Sp_\CC(  t^k e_i : 0 \le k < \mu_i, 1 \le i \le m) + L$. 
    % 
    % \rcom{Preimage of $\beta_\mu$}  
    % 
    % \acom{I guess that this is the ``$\mu$-numeration'' of the standard basis of $\CC^N$ that I commented out.}
    % 
    This follows immediately from (iii).
    % 
\end{proof}
% \acom{note-to-self, skipped this; return to it.}
% 
\begin{lemma}
\label{le:inftyfusiskl} % as in lands in KL slice 
    Let $\mu',\mu'' \in P$ and $\mu = \mu' + \mu''$. Under the map $ \Grbd \rightarrow \Grth$, the image of $ S^{\mu', \mu''}_{0,\AA}$ lands in $ \cW_\mu$.
\end{lemma}
%\acom{Probably it is worth writing down the map $ \Grbd \rightarrow \Grth$ just before this first occurence? \textit{is it $(L,s)\mapsto L$?} Update: added this above.}
% Resolved? the map $ \Grbd \rightarrow \Grth$ caused some confusion in my talk as on the left we have a family and on the right we have the thick affine Grassmannian --- Jiuzu kept thinking aloud that we can make $\Grth$ a family too.. Anyway I think that you address this below Joel. 

\begin{proof}
    Let $ L $ 
    % lie in this image.  
    be a lattice in the image of $S^{\mu',\mu''}_{0,\AA}$. Then $ L = g t^{\mu'} (t-s)^{\mu''}\CC[t]^m$ for some $ g \in N_-(\CC[t, t^{-1}, (t-s)^{-1}]) $.  Let $ h =(t-s)^{\mu''} t^{-\mu''}  $.  Note that $ h \in T_1(\Oinf)$. 
    Moreover,
    $$ L = h (h^{-1} g h) t^{\mu}\CC[t]^m\,. $$
    Note that 
    % $ h^{-1} g h \in N(\CC\xT[t^{-1}])$ 
    $h^{-1} g h \in N_-(\cK_\infty)$ and so we can factor $ h^{-1} g h$ as $n_1 n_2$ for some $ n_1 \in {(N_-)}_1(\Oinf), n_2 \in {N_-}(\CC[t]) $. 
    
    Since $ \mu $ is dominant, we see that $ t^{-\mu} n_2 t^\mu \in N_-(\CC[t]) $. % \CC\xt[t^{-1}]s
    % 
    % \rcom{I think we want $\mu$ antidominant here or should the $N$'s be $N_-$'s?} % 
    % \acom{Si.}
    % 
    Thus, $ L = h n_1 t^\mu \CC[t]^m$.  Since $ hn_1 \in G_1(\Oinf)$, the result follows. % \CC\xt[t^{-1}]
\end{proof}
% 
\section{Climax}
\label{s:climax}
Given $ A \in \TT_\mu$, recall the definition of $ g(A)$ given in \Cref{eq:mvyofa}.  Note that $g(A)t^{-\mu} \in G_1(\Oinf)$.
% section ??.  
Since $ g(A) \in M_m(\CC[t]) \cap G_1(\Oinf)t^\mu$, we will regard $ g(A)$ as giving an element of $ \Grth^+ \cap \cW_\mu$.  (Alternatively, we can see that $ g(A) \CC[t]^m$ satisfies the condition (ii) from Lemma \ref{le:Wmu}.)

The following result is \cite[Theorem 3.2]{cautis2018categorical} up to transpose.
% 
\begin{theorem} 
\label{th:TmuWmu}
The map $ \TT_\mu \rightarrow \Grth^+ \cap \cW_\mu $ given by $ A \mapsto g(A)\CC[t]^m $ is an isomorphism with inverse given by
$$ L \mapsto [t|_{\CC[t]^m/L} ]^{\tr}_{\beta_\mu} \,. $$
%\acom{Up to transpose?!}
\end{theorem}
% 

For the next result, we will consider the ``intersection'' of $ \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ with $\cW_\mu$.  
As $  \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} $ is not a subscheme of $ \Grth$, by this intersection, we really mean the preimage of $ \cW_\mu$ under the composition
$$ 
    \overline{\Gr}^{\lambda', \lambda''}_{0,\AA}  \hookrightarrow \Grbd \rightarrow \Grth\,.
$$
This is not a very serious abuse of notation, since the map $ \Grbd \rightarrow \Grth $ is almost injective. 
% Joel: The only non-injectivity comes from the fact that we forget the data
% of s.  So the map is non-injective over the locus where s is not
% actually an eigenvalue (in lattice terms where L(s) is the standard
% lattice).
In a similar way, we will write $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu$ using the non-injective map $ \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \rightarrow M_N(\CC)$ (for example the fibre of this map over $ 0 $ is $ \CC $). 
% \acom{Grammar of `write using the map' is iffy. Also, is the fibre over 0 just a copy of the zero matrix in every $s$-fibre?}

\begin{theorem} 
\label{th:OGrl}
There is an isomorphism
$$
    \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \cong \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu 
$$
given by $ (A,s) \mapsto (g(A)\CC[t]^m, s)$.
\end{theorem}
% \acom{Joel, you stopped writing $X_{0,\AA}$ in favour of $X_\AA$ --- should we do this everywhere?}
% \jcom{Oops, no I think that we should stick with $ X_{0, \AA}$.}
% TODO: comb the doc at the end for this notation 
\begin{proof}
Since we already have the isomorphism from \Cref{th:TmuWmu}, it suffices to show that for any $ A \in \TT_\mu$, 
$$ 
    (A,s) \in \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \TT_\mu \text{ if and only if } (g(A)\CC[t]^m, s) \in \overline\Gr^{\lambda', \lambda''}_{0,\AA} \cap \cW_\mu \,. 
$$
This follows immediately from \Cref{le:Grl1l2}.
\end{proof}
% 
\begin{theorem}
\label{th:OTGrW}
The isomorphism from \Cref{th:OGrl} 
% \acom{should ``Theorem'' etc.\ be capitalized when it appears in the middle of a sentence?} Yup.
restricts to an isomorphism
$$ 
    \overline{\OO}^{\lambda', \lambda''}_{0,\AA} \cap \UU^{\mu', \mu''}_{0,\AA} \cong \overline{\Gr}^{\lambda', \lambda''}_{0,\AA} \cap S^{\mu', \mu''}_{0,\AA}\,.
$$
\end{theorem}
% 
%\jcom{I was thinking about the proof of this result.  It is enough to prove that for $ A \in \OO^{\lambda', \lambda''}_s \cap \TT_\mu $, $g(A) \in S^{\mu', \mu''}_s $ if and only if $ A \in T^+$.  It is easy to show that if $ A \in \UU$, then $ g(A) \in S^{\mu', \mu''}_s $.  But the converse is not so obvious.  (I think Anne's paper/thesis might be incomplete on this point.)  I thought of two approaches: first to write down the lattice interpretation of $S^{\mu', \mu''}$ (similar to what is in my thesis in the Anderson-Kogan comparison section) or think about both $S^{\mu', \mu''}$ and $\UU$ as attracting sets for a $\CC^\times$ action.  Which do you prefer?}
% 
%\acom{Assuming Theorem 2 we think this follows by application of Roger's lemma. Also, I think my thesis is complete on this point.}
% 
%\jcom{I thought about this and I agree with you now.  I will write this up soon.} 
% 
%\acom{But I am also curious about the attracting set approach.}
% 
\begin{proof}
We could prove this by observing the both sides are the attracting locus of an appropriate $ \CC^\times$ action. However, we will give the following more algebraic proof.

Let $ A \in \TT_\mu$ and $ s \in \CC $. We must show that  $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $ if and only if $ (g(A)\CC[t]^m,s) \in S^{\mu', \mu''}_{0,\AA} $. 
%\acom{not $g(A)\CC[t]^m$}

On the one hand, if $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA} $, then $ g(A)$ is lower-triangular with diagonal $ t^{\mu'} (t-s)^{\mu''}$, and so $ g(A) \in N_-[t, t^{-1}, (t-s)^{-1}] t^{\mu'} (t-s)^{\mu''}$. 
% \acom{not $g(A)\CC[t]^m$}
% \jcom{This one isn't a point in $ \Gr$, the others I changed.}

On the other hand, if $ (g(A)\CC[t]^m, s) \in S^{\mu', \mu''}_{0,\AA}$, then we can write 
$$
    g t^\mu r= n t^{\mu'} (t-s)^{\mu''}
$$
for some $ r \in G(\CC[t]), n \in N_-(\CC[t, t^{-1}, (t-s)^{-1}]) $ and $ g = g(A)t^{-\mu}$.  Let $ h = (t-s)^{\mu''} t^{-\mu''}$ which lies in $ T_1(\Oinf) $. % was CC\xt[]
Note that $ h^{-1}n h \in N_-(\Kinf)$, 
% ((t^{-1}))
so we can factor it as $ h^{-1} n h  = n_1 n_2 $, where $ n_1 \in N_{-,1}(\Oinf), n_2 \in N_-(\CC[t])$.  So then after doing a bit of algebra, we reach
$$
    t^\mu r (t^{-\mu} n_2^{-1} t^\mu) t^{-\mu} = g^{-1} h n_1.
$$
Since $ g, h, n_1 \in G_1(\Oinf)$, the right hand side $ g^{-1} h n_1 $ lies in $ G_1(\Oinf) $.  Since $ \mu $ is dominant, $ t^{-\mu} n_2 t^\mu \in N_-(\CC[t])$, and so the left hand side lies in $t^\mu G(\CC[t]) t^{-\mu}$.

Moreover, since $ \mu $ is dominant, we know that 
$$
    t^\mu G(\CC[t]) t^{-\mu} \cap G_1(\Oinf) = N_{-,1}(\Oinf)\,.
$$
% $$t^\mu G(\cO) t^{-\mu} \cap G_1(\CC\xt[t^{-1}]) = N_1(\CC\xt[t^{-1}]\,.$$
Thus, we deduce that $ g^{-1} h n_1 \in N_{-,1}(\Oinf)$ 
% $ g^{-1} h n_1 \in N_1(\CC\xt[t^{-1}])$ 
and hence $ g(A) \in t^{\mu'} (t-s)^{\mu''}N_{-,1}(\Oinf) $.  
% Given that we know that 
Since $ A \in \TT_\mu $, this implies that $ (A,s) \in \UU^{\mu', \mu''}_{0,\AA}$ as desired.
\end{proof}
% 
% \jcom{In this proof, I made some statements about $\mu$ dominant, which are incorrect (as Anne knows). To make them correct we should either switch to having $ \mu$ being antidominant or switch $ N$ to $ N_-$.  In this proof $ g(A)$ generally denotes a group element, rather than an affine Grassmannian element which conflicts a bit with earlier usage.}
% 
%\acom{What was the answer to Roger's question why is $g^{-1}hn_1 \in G_1 (\Oinf)$}
% 
%\rcom{So if we look at $g = g(A)t^{-\mu}$, since $A \in \TT_\mu$, the entries of $g(A)_{ij}$ are polynomials of degree $\min(\mu_i-1,\mu_j-1)$ when $i \neq j$, otherwise of degree $\mu_i$. When we multiply by $t^{-\mu}$, we scale the $i$th column by $t^{-\mu_i}$. So the diagonal entries are $1 + p$ where $p \in t^{-1}\CC[t^{-1}]$ while the off-diagonal entries in column $i$ have highest (in $t$) degree $\min(\mu_i-1, \mu_j-1) - \mu_i < 0$. Hence $g \in G_1(\cO_\infty)$.}
% 

Restricting to the zero fibre and applying $\tau_0$ we recover \cite[a main result]{dthesis}, itself a corollary of the ordinary affine Grassmannian version of \Cref{th:OGrl}, \cite[Theorem~4.4.1(c)]{mirkovic2019comparison}. 
% 
\begin{corollary}
    \label{cor:mvy}
    $\overline{\OO}^\lambda \cap \TT_\mu\cap\n \cong\overline\Gr^\lambda\cap S^\mu$ where $\n\subset M_N(\CC)$ is the subalgebra of upper-triangular matrices. 
\end{corollary}
%The closures of the irreducible components of the righthand side are \textbf{MV cycles of type $\lambda$ and weight $\mu$}. By \cite{baumann2019mirkovic} the set of MV cycles of all possible types and weights can be stitched together to form a \textbf{perfect basis} of $\CC[N]$. 

%\acom{We are ready for applications! One of the motivations for this paper is... Abstractly multiplication is $\ast_s$... To compute we now carry this across \Cref{th:OTGrW}.}

\section{Falling Action} % Resolution
\subsection{MV cycles}
% 
% We need to decide on the order in section 7. How about:
% 
% 1. MV cycles and tableaux (7.1)
% 2. GOVs and relation to MV cycles, including fusion (7.2)
% 3. Stable MV cycles and multiplication  in C[N]
% 
\begin{definition}
An irreducible component of $\overline{\Gr}^\lambda \cap S^\mu$ is called an \textbf{Mirkovic--Vilonen cycle} in $\overline{\Gr}^\lambda$ of coweight $\mu$.
\end{definition}

% \jcom{We could put some stuff here about stable MV cycles and multiplication, but maybe we can have this in a later section.}

% \acom{With this definition here what do we do about the sentence after \Cref{cor:mvy} above?}
% \jcom{I just cut it.  I think that it was a bit out of place there.}

% \acom{Recall (from optional \Cref{s:exposition}) the question motivating this work --- Roger's thesis, cluster monomials and the MV basis. Clarify that whereas we have so far only seen spaces related to ``ordinary MV cycles'' it is the ``stable MV cycles'' that are used to define the MV basis of $\CC[N]$ that the motivating question is concerned with. This discrepancy is the reason we might want to discuss maps $B(\infty)\to B(\lambda)$.}
% 

In \cite{kamnitzer2010mirkovic}, the third author gave a combinatorial description of MV cycles for any reductive group, using Mirkovic--Vilonen polytopes, or equivalently Lusztig data, which are sequences in $\NN^{\Delta_+}$ which depend on a choice of reduced expression.  For $\GL_m$ and the standard reduced word, this description can be reworded in terms of Young tableaux. 
% \jcom{Maybe add something here about Lusztig data and the standard reduced word, $\vi = (1,2,\dots,m-1,\dots,1,2,1)$.}

We begin with some new notation. Let $YT(\lambda)_\mu$ denote the set of (possibly semistandard) Young tableaux of shape $\lambda$ and weight $\mu$. 
%Here, the weight of a tableau is a sequence $(\mu_1,\mu_2,\dots)$ recording the number of boxes of weight $i = 1,2,\dots$ that the tableau contains, and the weight of a box is just the number that the box contains.
% For the sake of making a definition, we momentarily put some letters of our alphabet in unexpected roles, and ask you to suspend your disbelief. Given an arbitrary tableau $t$ having weight $w$ and shape $s$, we denote by $s(i)$ (resp.\ $w(i)$) the shape (resp.\ the weight) of $t(i)$, the tableau got from $t$ by discarding all boxes of weight exceeding $i$. 

Given $\tau\in YT(\lambda)_\mu$ and $ i \in \{1, \dots m\} $, denote by $\lambda(i)$ (resp.\ $\mu(i)$) the shape (resp.\ the weight) of $\tau(i)$, the tableau got from $\tau$ by discarding all boxes of weight exceeding $i$. (Note that $ \mu(i)$ only depends on $ \mu$, while $ \lambda(i)$ depends on the tableau $ \tau$.)  We will regard $ \lambda(i)$ (resp.\ $\mu(i)$) as an effective dominant coweight (resp.\ effective coweight) for $\GL_i$. 
% 

The Lusztig datum $n_\bullet(\tau)$ of the tableau $\tau$ is a list of $ m(m-1)/2$ non-negative integers defined from its \textbf{Gelfand--Tsetlin pattern} $\gt(\tau) = (\lambda(i)_j)_{1\le j\le i\le m}$ by the formula 
$$
\begin{aligned}
n_\bullet(\tau)_{i,j} &= \lambda(i+1)_j - \lambda(i)_j \\
(i,j) &=  (1,1),(2,1),\dots,(m-1,1),%\\
(2,2),\dots,(m-1,2),%\\
\dots,%\\
(m-1,m-1)
\end{aligned}
% (i,j) = \begin{smallmatrix}
% \\
% (1,1) \\
% (2,1) & (2,2) \\
% \vdots & & \ddots \\
% (m-1,1) & (m-1,2) & \dots & (m-1,m-1) 
% \end{smallmatrix}
$$
% for 
% $(i,j) = (1,1),(2,1),\dots,(m-1,1),(2,2),\dots,(m-1,2),\dots,(m-1,m-1)$. 
% 
The pattern $\gt(\tau)$ is recorded as a lower-triangular matrix (the array of shapes of subtableaux $\tau(i)$) and the datum $n_\bullet(\tau)$ is recorded as a sequence. See \cite[\S 4]{berenstein1988tensor}.

\jcom{Something is wrong with the indexing of the Lusztig data, there should only be $ m(m-1)/2 $ entries.}

\acom{Yes $i = m$ doesn't make sense. Should be $i = 1,\dots,m-1$ and $j = 1\dots i$ or $(1,1),(2,1),\dots,(m-1,1),(2,2),\dots,(m-1,2,\dots,(m-1,m-1)$ which is $(m-1) + (m-1-1) + \cdots + 1$ things. Right?}

For example, if 
\[
    \tau = \young(1112,23) 
\]
then
\begin{gather*}
%    \text{if } \tau = \young(1112,23) \text{ then } 
\gt(\tau) = \begin{matrix}
       3 \\
       4 & 1 \\ 
       4 & 2 & 0 
   \end{matrix} \text{ and }
   n_\bullet(\tau) = (1,0,1)\,. 
\end{gather*}
% 
%This combinatorics is relevant because by work of the third author the MV cycles are parametrized by Lusztig data. (This holds much more generally than the present scope.)
% 
We can associate to $\tau$ an MV cycle. 
%  
\begin{equation*}
    Z(\tau) = 
     \overline{
        \{
            L\in S^\mu : L\cap\cO^i\in\Gr^{\lambda(i)}\text{ for } i = 1, \dots m 
        \}\,. 
     }
\end{equation*}
% 

The following result is closely related to Theorem 4 from \cite{dranowski2020generalized}.  It is also closely related to the description of MV cycles in terms of Kostant data obtained by Anderson--Kogan \cite{anderson2004mirkovic} (see Section 9 in \cite{kamnitzer2010mirkovic}).  We also note that a different map from Young tableaux to MV cycles was obtained by Gaussent--Littelmann--Nguyen \cite[Theorem 2]{gaussent2013knuth}.
\begin{proposition}
    \label{pr:newmvdes}
    $Z(\tau)$ is the MV cycle whose Lusztig datum with respect to $ \vi $ is $n_\bullet(\tau)$. 
\end{proposition}
\begin{proof}
    The proof of this theorem follows the same strategy as in \cite{dranowski2020generalized}.
\jcom{Maybe you would prefer to cite your thesis, Anne, but I think that it is better to cite the paper, since people can access it easier.}    
\acom{I've been meaning to update the paper. It's peppered with ``typos''.}

Recall that $ S^\mu = N_-(\cK) t^\mu$.  For the purposes of this proof, we also introduce the notation $S^\mu_+ := N(\cK)t^\mu$.

\jcom{Actually we will need this notation later when we talk about stable MV cycles.  Maybe we should change $ S^\mu $ to $S^\mu_-$.}
\acom{For it.}

First, we consider
\begin{equation*}
Z(\tau)_1 := \{ L \in S^\mu : L \cap \cO^i \in S_+^{\lambda(i)} \text{ for } i = 1, \dots m\}  
%Z(\tau)_2 := \{ L \in S^\mu : L \cap \cO^i \in S^{\lambda(i)} \cap \Gr^{\lambda(i)} \text{ for } i = 1, \dots m\}
\end{equation*}
From \cite[Corollary 9.7]{kamnitzer2010mirkovic}, we see that $ Z(\tau)_1$ is equal to $ t^\mu A(n_\bullet(\tau))^T$, where $ A(n_\bullet)$ is defined in section 4.3 of \cite{kamnitzer2010mirkovic}.  Thus, $ \overline{Z(\tau_1)}$ is the MV cycle whose Lusztig datum is $ n_\bullet(\tau) $.  We also note that $Z(\tau_1) \cap \Gr^\lambda$ is dense in $ Z(\tau_1)$.  
\jcom{Need to justify this last sentence.}

Fix $ i \in \{1, \dots, m\}$. If $ L \in S^\mu$, then $ L \cap \cO^i \in S^{\mu(i)}$.  Thus, we get a map $$ f_i : Z(\tau)_1 \rightarrow S^{\mu(i)} \cap S_+^{\lambda(i)} $$
From the definition of $ Z(\tau)_1 $, we see that $ f_i(Z(\tau_1) = Z(\tau(i)_1)$.  From above $ \Gr^{\lambda(i)} \cap f_i(Z(\tau(i)_1))$ is a dense constructible subset of $Z(\tau(i)_1)$.  Since $ Z(\tau)_1$ is irreducible, this implies that $ f_i^{-1}(S^{\mu(i)} \cap S_+^{\lambda(i)} \cap \Gr^{\lambda(i)}) $ is a dense constructible subset of $ Z(\tau_1) $.  

Working with all $ i $ at once, we conclude that
$$ Z(\tau)_2 := \bigcap_{i=1}^m f_i^{-1}(S^{\mu(i)} \cap S_+^{\lambda(i)} \cap \Gr^{\lambda(i)}) $$
is a dense constructible subset of $ Z(\tau)_1$.  Thus $\overline{Z(\tau)_2} = \overline{Z(\tau)_1}$.

On the other hand, by definition 
$$Z(\tau)_2 \subset \{ L \in S^\mu : L \in \Gr^{\lambda(i)} \text{ for } i = 1, \dots, m\}$$

\jcom{Now, we need to show density here.  But maybe that won't be possible and we will need to take the ``top''.}

    %See computation 5.3.9 and Lemma 5.4.2 (ignore its proof) of \url{http://www.math.toronto.edu/adranows/athesis.pdf}. 
\end{proof}
% This gives us the dimension of Gr^\lambda \cap Gr_\mu \cap S^\mu.
% Given $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ denote by $\lambda'(i),\lambda''(i)$ (resp.\ $\mu'(i),\mu''(i)$) the shape (resp.\ the weight) of $\tau'(i),\tau''(i)$ where $\tau^?(i)$ denotes the tableau got from $\tau^?$ by discarding all 
% % but its first $i$ boxes
% boxes of weight exceeding $i$. 
% % If the tableau contains repeated entries then first means leftmost.
% 
% \jcom{I don't think that you should mention this partial inverse here.  I think that you should explain here how the generalized orbital varieties are indexed by the tableaux, in other words, how $ X(\tau) $ is defined.}
% \acom{Ok, I moved it.}

% Denote the image of $\tau$ in $\overline{\OO}^\lambda\cap\TT_\mu\cap\n$ by $X(\tau)$ and in $\overline{\Gr}^\lambda\cap S^\mu_-$ by $Z(\tau)$. Thus $Z(\tau) = \phi(X(\tau))$. \acom{Out of place?} 
% \rcom{What is $\phi$?}
% 
% 
% TODO: Not yet, that would be the algo stuff that we are holding off on 
% In this section we explain how to determine the ideal of $Z'\ast_\AA Z''$ from the minimal pair $(\tau',\tau'')$ corresponding to $(n_\bullet(Z'),n_\bullet(Z''))$.
% 
% In particular, we prove that ``the generalized Spaltenstein map'' produces MV cycles. 
% 
% and recall the $\mu$-numeration of the standard basis of $\CC^N$. 
% TODO: can't we do without it^^

% 
% In this section we demonstrate how to put the main results of this paper into practice.
% 
% 
\subsection{Fusion of MVCs via BD MVy}
Let $m,\lambda',\lambda'',\lambda,\mu',\mu'',\mu,N$ be as before.
% 
Given $A \in M_N(\CC)$ we denote by 
% $A\big|_{\CC^i}$ 
$A\big|_{\CC^p}$ 
the restriction of $A$ to the subspace spanned by the first $p$ standard basis vectors. If $A\big|_{\CC^p}(\CC^p)\subset\CC^p$ then we identify it with the $p\times p$ upper-left submatrix of $A$. 
% \acom{We may want to introduce $\ast_s$ in general. For now we make a definition.} 

Given $s\in\CC\setminus \{0\}$, and a pair of tableaux $\tau'\in YT(\lambda')_{\mu'}$ and $\tau''\in YT(\lambda'')_{\mu''}$ we define
\begin{equation*}
    X(\tau')\ast_s X(\tau'') = \left\{A\in\UU_{0,s}^{\mu',\mu''} : A \big|_{\CC^{|\mu(i)|}} \in \OO_{0,s}^{\lambda'(i),\lambda''(i)} \text{ all } i=1,\dots,N\right\}\,.
\end{equation*}
% 
% and let $Z'\ast_s Z'' = (Z'\ast_\AA Z'')_s$.
% If $s\ne 0$ identify $Z'\ast_s Z''= Z'\times Z''$ via $\Gr_{0,s}\cong \Gr\times\Gr$.
% \acom{or make the identification using the map the map $\tau$ of \Cref{eq:mvitau}. Can similarly identify $X'\ast_s X'' = X'\times X''$ by performing the obvious change of basis and block-diagonalizing. May make sense to introduce $\ast_s$ in greater generality, earlier; or at least on MV cycles earlier, since here I should actually make the definition for $Z(\tau)$'s.. which is ok, as in not a loss of generality. Still..}

% \jcom{Now that we have $ \ast_s$ defined in the fusion section, we can refer to that here.}

\begin{proposition}
    % Given $b_{Z'},b_{Z''}\in\CC[N]$, 
    % Let $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ 
    % be such that $t^{\lambda'}Z' = X(\tau')$ and $t^{\lambda''}Z'' = X(\tau'')$. 
    % and set $\mu = \mu' + \mu''$. If $s\ne 0$, then 
    The image of $X(\tau')\ast_s X(\tau'')$ under the isomorphism of \Cref{th:OTGrW} is dense in $Z(\tau')\ast_s Z(\tau'')$. 
    % \[
    % \overline{\phi(X(\tau',\tau''))} 
    % = Z'\ast_s Z'' \,. 
    % \]
\end{proposition}
% 
% \jcom{Do you want the ``blocky'' or ``boxy'' description here?  I mean, do you want $ i = 1, \dots, N$ or $ i =1, \dots, m$? I'm not so sure that ``boxy'' will work, since with those $s$ in the matrix, it doesn't preserves all the $ \CC^i$.}

% \acom{Fixed.}

\begin{proof}
    % We wish to apply \Cref{prop:dth}. Nope
    % We will also need \Cref{le:Grl1l2}. Nope 
    % Consider the following diagram.
    % \[
    % \begin{tikzcd}
    %     X'\ast_s X'' \ar[r,dashed,"?"] \ar[d,"\bar g"] & X'\times X'' \ar[d," \phi\times \phi"] \\ 
    %     \Gr_{0,s}\ar[r,"\tau_s"] & \Gr\times\Gr
    % \end{tikzcd}
    % \]
    % \acom{Not sure what's a good notation for the vertical arrows. Don't want to introduce $\phi$. $\bar g(A) = g(A)\CC[t]^m$ temporarily.}
    % We show that the image of $\tau_s\circ \bar g$ coincides with the image of $\phi\times\phi$ which is dense in $Z'\times Z''$ by \Cref{prop:dth}.
    % % 
    Let $A\in X(\tau')\ast_s X(\tau'')$ and $i = 1,\dots, m$.  
    % Denote by 
    % $E_a$ the generalized $a$-eigenspace of $A$, and by 
    The lattices $L=g(A)\CC[t]^m$ and $l=g(A\big|_{\CC^{|\mu(i)|}})\CC[t]^i$ are related by
    $$
    L\cap \CC[t]^i = l
    $$
    where $\CC[t]^i\subset\CC[t]^m = \CC^m\otimes_\CC\CC[t]$ denotes the $\CC[t]$-submodule generated by the first $i$ standard basis vectors.
    %  
    
By definition of $\UU^{\mu',\mu''}$ we have $A\big|_{\CC^{|\mu(i)|}}\in\UU^{\mu'(i),\mu''(i)}_{0,s}$ and by definition of $X(\tau')\ast_s X(\tau'')$, $A\big|_{\CC^{|\mu(i)|}}$ has Jordan type $((\lambda'(i),0),(\lambda''(i),s))$. So, by \Cref{th:OTGrW}, $l\in\Gr_{0,s}^{\lambda'(i),\lambda''(i)}$ and
% \Gr^{\lambda'(i)}\times \Gr^{\lambda''(i)}$ so 
$(l(0),l(s))\in Z(\tau')\times Z(\tau'')$.

So $A\in\dots$ iff $g(A)\in\dots$ 
\end{proof}
% 
% 
In analogy with the Beilinson--Drinfeld construction \acom{add loc cit} we define $X(\tau')\ast_0 X(\tau'')$ to be the zero fibre of the family $\overline{\bigcup_{s\in\CC\setminus\{0\}} X(\tau')\ast_s X(\tau'')}$ in $M_N(\CC)$. 
% 
\begin{corollary}
    $X'\ast_0 X'' \cong Z'\ast_0 Z''$ 
\end{corollary}
% 
Let $X(\tau)$ be the top-dimensional \acom{speak to this?} irreducible component of
% from $\tau\in YT(\lambda)_\mu$ by 
\[
% X(\tau) = 
% \overline{
    \{
        A \in  \TT_\mu\cap\n : A\big|_{\CC^{|\mu(i)|}} \in \OO^{\lambda(i)} \text{ for each } i = 1,\dots,m
    \}\,. 
% }    
\]
Then $\overline{X(\tau)}$ is called a \textbf{generalized orbital variety}. 
% 

\begin{theorem}
    \label{pr:govsasirrecs}
    The irreducible components of $\overline{\OO}^\lambda\cap\TT_\mu\cap\n$ are $\{\overline{X(\tau)} : \tau\in YT(\lambda)_\mu\}$. 
\end{theorem}
\acom{$\overline{X(\tau)}$ vs $Z(\tau) = \overline{al;sdj}$}
\begin{corollary}
    $X'\ast_0 X''$ is a union of GOVs.
\end{corollary}
\begin{proof}
    Since $X'\ast_0 X''\subset \overline{\OO}^\lambda\cap\TT_\mu\cap\n$\dots 
\end{proof}

\begin{proposition}(\cite{baumann2019mirkovic})
    % Given two MV cycles $Z_\tau$ and $Z_\sigma$ of type\dots 
    Let $Z',Z''$  be MV cycles of type $\lambda',\lambda''$ and weight $\mu',\mu''$ resp.\ and assume that $\lambda',\lambda'',\mu = \mu' + \mu''$ are dominant. 
    Then $Z'\ast_0 Z''$ is a union of MVCs
    % \begin{equation}
        % Z'\ast_0 Z'' = \bigcup Z(\tau)
        % \label{eq:tabmult}
        % % Z_\tau\ast Z_\sigma 
        % m_{\lambda'\lambda''} ([Z']\otimes[Z'']) 
        % = 
        % % \sum_{Z\in\cZ(\lambda)_\mu} 
        % \sum_{\tau\in S(\lambda)_\mu}
        % i\left(
        %     X(\tau), X(\tau')\circ X(\tau'')
        % \right) [Z(\tau)]\,. 
    % \end{equation}
    with $i\left(X(\tau), X(\tau')\circ X(\tau'')\right)$ copies of $Z(\tau)$ showing up. 
\end{proposition}
% 
\acom{Closures missing? Here and elsewhere?}
% 
\begin{proof}
    Since $X'\ast_0 X'' \cong Z'\ast_0 Z''$ and GOVs go to MVCs 
\end{proof}

\begin{conjecture}
    % Let $Z_i \subset \overline{S^{\nu_i}\cap S^0_-}$ be an MV cycle of weight $\nu_i$ ($i = 1,2$) and put $\nu = \nu_1 + \nu  _2$. 
    Let $\tau$ be the tableau of shape $\lambda$ and weight $\mu$ whose Lusztig datum is equal to the sum of the Lusztig data of $\tau'$ and $\tau''$. 
    % Then $i(\tau, \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U})$ is equal to 1. 
    Then 
    \begin{equation}
        i(\tau, 
        % \pi^{-1}(0) \cdot \overline{\tau_1 \times \tau_2 \times U}
        % Z_1 \circ Z_2 
        \tau' \circ \tau''
        ) = 1 \,. 
    \end{equation}
\end{conjecture}
\jcom{Rather than adding the Lusztig data, how can you produce $\tau$ directly from $\tau', \tau''$?  I guess you can just add the GT patterns, so for tableaux you just take the union of the rows, is that right?}


\acom{Pierre said he hasn't thought about this since he last thought about this and remarked that KKKO would distinguish two terms, a leading and a tailing one.} 
% 
\subsection{Connection to cluster structure on $\CC[N]$}
% 
Goal: 
$$
b_{Z'}b_{Z''} = \sum_{Z\in\cZ(\infty)_{-\nu_1 - \nu_2}} i \left(
    Z, Z' \ast_0 Z''
    % \pi^{-1}(0) \cdot \overline{Z_1 \times Z_2 \times U}
\right) b_Z 
$$

\begin{lemma}
    \label{lem:mintab}
    Let $n_\bullet$ be a Lusztig datum of coweight $\nu$. There exist $\lambda,\mu$ smallest such that $n_\bullet$ is the Lusztig datum of a tableau $\tau\in YT(\lambda)_\mu$. They are determined by the following minimization problem. Regard $n_\bullet$ as a strictly lower triangular matrix with $(r,c)$th entry $n_{r,c}$ is equal to the number of boxes of weight $r$ in row $c$ of $\tau$. Note that since $r > c$ this data does not account for the number of boxes of weight $r$ in row $r$. Let $\mu^0$ be the diagonal matrix whose $(r,r)$th entry $\mu^0_r$ is equal to the number of boxes of weight $r$ in row $r$ of $\tau$. Note that the sum of the rows of $n_\bullet + \mu^0$ is the shape $\lambda$ of $\tau$, and the sum of the columns of $n_\bullet + \mu^0$ is the weight $\mu$ of $\tau$. The solution to the minimization problem 
    \[
        \begin{gathered}
        % 
        \text{minimize }\mu^0\text{ such that} \\
        % \text{the sum of the rows of }n_\bullet + \mu^0\text{ is non-increasing},
        % \text{the sum of the columns of }n_\bullet + \mu^0\text{ is non-increasing}
        \text{both }\lambda,\mu\text{ are non-increasing}
        \end{gathered} 
    \]
    is got recursively from % setting $\mu^0_m = 0$, 
    \[
        \begin{gathered}
            \mu^0_{m} = 0 \\ 
            \mu^0_{m-1} = \max\{0 , \sum n_{m,c} - \sum n_{m-1,c}\} \\ 
            \mu^0_i = \max\{0, \mu^0_{i+1} + \sum n_{i+1,c} - \sum n_{i,c} , \mu_{i+1}^0 + \sum n_{r,i+1} - \sum n_{r,i}\}
        \end{gathered}
    \]
    for $i = 1,\dots,m-2$. 
    In words, given a Luszig datum $n_\bullet$, if we wish to produce a minimal SSYT for it, 
    \begin{itemize}
        \item we can always take the number of $m$'s in row $m$ to be zero; 
        \item we can take the number of $m-1$'s in row $m-1$ to be zero, unless $n_\bullet$ tells us that there are more $m$'s the first $m-1$ rows than there are $m-1$'s in the first $m-2$ rows, and in that case we take $\mu^0_{m-1}$ so that it exactly offsets the difference; 
        \item we can take the number of $m-2$'s in row $m-2$ to be zero, unless $n_\bullet$ tells us there are more $m-1$'s in the first $m-1$ rows (building off the last step) than there are $m-2$'s in the first $m-3$ rows, or, that there are more boxes in row $m-1$ than there are in row $m-2$, and then we take $\mu^0_{m-2}$ to offset the minimum of the two differences; 
        \item induct up to $\mu^0_1$. 
    \end{itemize}
\end{lemma}

\begin{proposition}
    \label{prop:minwts}
    Given two Lusztig data $n_\bullet',n_\bullet''$ there is a smallest choice of effective coweights $\mu',\mu''$ and effective dominant coweights $\lambda',\lambda''$ such that $\mu = \mu' + \mu''$ is effective dominant and $(\tau',\tau'')\in YT(\lambda')_{\mu'}\times YT(\lambda'')_{\mu''}$ are semistandard Young tableaux with the prescribed Lusztig data. They are got by choosing the minimum $\lambda,\mu$ for $n_\bullet = n_\bullet ' + n_\bullet ''$ afforded by \Cref{lem:mintab}, and then breaking off $n_{r,c}'$ boxes of weight $r$ from each row $c<r$ of $\tau$ to form a $\tau'_0$ an intermediate tableau. If $\tau'_0$ is of an acceptable shape then we keep it. If it is not, then we correct it by padding it using some boxes of weight $r$ in row $r$ of what remains of $\tau$.

    \acom{To be completed. We require a notion of ``smallest'' pair of factor tableaux associated to a given tableau --- that is if we care to include our $B(\infty)\to B(\lambda)$ map. 
    As the examples show, there may be more than one way to break up a ``minimal'' tableau for $n_\bullet = n_\bullet ' + n_\bullet ''$ into two tableaux with data $n_\bullet' $ and $n_\bullet''$ resp.\ The practical notion of min is such that the generalized nonzero eigenspace has the smallest possible dimension.}
    \acom{Reference Claxton--Tingley multisegments, Hong--Lee marignally large tableaux for comparison.}
\end{proposition}

% Let's wait for Gerhard before we discuss our computations any further
% \subsection{Pseudocode}
% 
\section{Cluster Algebras}
One of the main motivations for this paper is computing certain fusion products that are associated to exchange relations in the cluster algebra structure of $\CC[N]$. 

A basis of $\CC[N]$, called the dual semicanonical basis, was originally constructed in \cite{lusztig2000semicanonical} while a cluster algebra structure for $\CC[N]$ was shown in \cite{berenstein2005cluster3} and \cite{geiss2007initial}.
A natural question was whether the cluster monomials were elements of the dual semicanonical basis, proved in the positive in \cite{geiss2006rigid}.

Another basis for $\CC[N]$ was constructed in \cite{mirkovic2007geometric} called the MV basis. This basis is indexed by the MV cycles and the product of basis elements corresponds to the fusion product of the associated MV cycles. While this basis is different from the dual semicanonical basis, as shown in \cite{baumann2019mirkovic}, they seemed to share a lot of common elements, such as various cluster variables, which led to the following conjecture.

\begin{conjecture}\label{conj:cluster in MV}
    The cluster monomials of $\CC[N]$ are elements of the MV basis.
\end{conjecture}

Conjecture \ref{conj:cluster in MV} is known for type $A_n$, $n \leq 3$, as in this case, there is a unique biperfect basis for $\CC[N]$ \cite{baumann2019mirkovic}, so the dual semicanonical basis and MV basis are the same.

It has been shown in \cite{kato2011polytopal} that for initial seeds $(x_1, \dots, x_r)$ obtained from a reduced expression of the longest word in the Weyl group for $G$, each $x_i$ is in the MV basis as these correspond to generalized minors \cite{berenstein2005cluster3}\cite{geiss2007initial}. If the reduced expression satisfies a special condition, then the cluster monomials in the initial seed are also in the MV basis \cite{baumann2020bases}.

Suppose we have a seed $(x_1, \dots, x_r)$ where each $x_i$ is already in the MV basis. To show all the other cluster variables are in the MV basis, it suffices to show that for a mutable variable $x_i$, its mutation $x_i^*$ is also in the MV basis. The exchange relation corresponding to $x_i$ and $x_i^*$ is of the form
$$x_ix_i^* = x_+ + x_-$$
where $x_+$ and $x_-$ are monomials in the $x_j,j\neq i$. Let $Z_i$ be the MV cycle corresponding to $x_i$, so $x_i = b_{Z_i}$. If we are able to find an MV cycle $Z_i^*$ such that
$$b_{Z_i} b_{Z_i^*} = x_+ + x_-,$$
then this means $x_i^* = b_{Z_i^*}$ so it is in the MV basis with associated MV cycle $Z_i^*$. In Section \ref{s:denouement}, we present examples showcasing this is indeed true for type $A_3$. 
\section{Denouement}
\label{s:denouement}
\subsection{Examples}

% \acom{Plus fusion product of fake MV cycles for a fourth!}

We view MV cycles as semi-standard young tableaux. Suppose we have two tableaux $\tau'$ and $\tau''$, with respective weights $\lambda'$, $\mu'$ and $\lambda''$, $\mu''$, and we wished to take their fusion product. We consider a generic matrix $X \in \UU^{\mu',\mu''}_{0,s}$ and see what relations on the variables in $X$ occur when also asking for it to be in $\OO^{\lambda',\lambda''}_{0,s}$. \acom{Relations dictated by $\tau$. Can mostly refer the reader to the previous section for this background, now.}

Consider the subtableaux $\tau'(i)$ of $\tau'$ and $\tau''(i)$ of $\tau''$ where we take the boxes of $\tau'$ and $\tau''$ containing the numbers $1,\dots,i$. Let $\lambda'(i)$ and $\lambda''(i)$ be the corresponding sizes and $\mu'(i)$ and $\mu'(i)$ be the corresponding weights of the subtableaux. For each $i$, we obtain a corresponding submatrix $X_i \in \UU^{\mu'(i), \mu''(i)}_{0,s}$ of $X$. These subtableaux impose rank conditions, equivalently vanishing minors, on each $X_i$ when asking them to be in $\OO^{\lambda'(i), \lambda''(i)}_{0,s}$, which allows us to deduce relations on the variables in $X$. The relations must also include the new variables in the latest block and cannot have $s$ as a factor.

To find the MV cycles, equivalently tableaux, in the $0$-fibre of the fusion, we consider the ring generated by all the variables of $X$ and quotient out the ideal generated by the relations from the vanishing minors as well as the minimal polynomial of $X$. We then add $s$ into the ideal. Each piece in the primary decomposition of this ideal will correspond to a tableaux, and subsequently a MV cycle.

% \begin{example}
% Let $G = \GL_3$ and consider the tableaux
% $$\tau' = \young(12,3) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau''=\young(13,2).$$

% Lusztig data: $(010)$ for $\tau'$, $(101)$ for $\tau''$

\acom{If we start with $n_\bullet ' = (0,1,0)$ and $n_\bullet'' = (1,0,1)$ then the smallest $\tau$ for $n_\bullet = (1,1,1)$ is $\young(1123,23)$ and hence the smallest $\mu,\lambda$ are $(2,2,2),(4,2,0)$. So far so good. However, the ``smallest'' ``fission'' of $\tau$ is $\young(112,23) , \young(3)$. We could define ``smallest'' to mean minimizing the dimension of the generalized $s$-eigenspace. What do you guys think? If we agree to do this we should amend this example. For Joel: this is an example where our $B(\infty) \to B(\lambda)$ does not match that of Claxton--Tingley (quoted in my thesis). They would get $\lambda = (3,1)$. I think that their map satisfies $\lambda$ dominant and $\mu$ such that each $\mu_i\ge 1$.}

% Dimensions: $(1,0,-1)$ for both.

% The MV cycles corresponding to $\tau'$ and $\tau''$ are both isomorphic to $\PP^2$.
% The weights are $\lambda' = \lambda'' = (2,1,0)$ and $\mu' = \mu'' = (1,1,1)$. Hence the matrix in consideration is 
% \[
% X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
%     0 & 1 & & & & \\
%      & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 \\
%      & & 0 & 1 & & \\
%      & & & s & A_{23}^1 & A_{23}^2 \\
%      & & & & 0 & 1 \\
%      & & & & & s
% \end{BMAT}
% \right].
% \]

% For the first entry of $\mu'$ and $\mu''$, we have the following subtableaux to consider:
% $$\young(1) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1).$$
% Let $X_1$ be the top-left $2 \times 2$ submatrix of $X$. The subtableaux imposes the conditions $\dim \ker X_1 = 1$ and $\dim \ker (X_1-s) = 1$. We see that we do not obtain any relations from this.

% For the first two entries of $\mu'$ and $\mu''$, we have the following subtableaux:
% $$\young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \young(1,2).$$
% Let $X_2$ be the top-left $4 \times 4$ submatrix of $X$. The left subtableau imposes the conditions $\dim \ker X_2 = 1$ and $\dim \ker X_2^2 = 2$ while the right subtableau imposes the condition that $\dim \ker (X_2 -s) = 2$. These conditions force the relation $$A_{12}^1 + sA_{12}^2 = 0.$$

% The original tableaux for $X$ imposes the conditions that $\dim \ker X = 2$, $\dim \ker X^2 = 3$, $\dim \ker (X-s) = 2$, and $\dim \ker (X-s)^2 = 3$. These force the relation $$A_{23}^1 = 0.$$

% Hence we consider the quotient
% $$\frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{23}^2,s]}{\langle A_{12}^1+sA_{12}^2,A_{23}^1,s \rangle} \cong
% \frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{23}^2]}{\langle A_{12}^1,A_{23}^1 \rangle}.$$
% We see already that the $0$-fibre is irreducible. The tableau corresponding to the ideal $\langle A_{12}^1,A_{23}^1 \rangle$ is 
% $$\young(1123,23)$$
% which corresponds to the MV cycle that is the product of the MV cycles corresponding to $\tau'$ and $\tau''$.

% Lusztig datum: $(111)$

% Dimension: $(2,0,-2)$
% \end{example}

\begin{example}
Let $G = \GL_3$ and consider the tableaux
$$\tau' = \young(12) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(11,23).$$

Lustig data: $(100)$ for $\tau'$, $(001)$ for $\tau''$.

\acom{This time the smallest $\tau$ is actually $\young(12,3)$ and the smallest fission is $\young(1,3),\young(2)$. For Joel: this time the our $\lambda$ matches \cite{claxton2015young}'s.}

Dimensions: $(1,-1,0)$ for $\tau'$, $(0,1,-1)$ for $\tau''$.

The MV cycles corresponding to the above tableaux are each isomorphic to $\PP^1$.
We have the weights to be $\lambda' = (2,0,0)$, $\lambda'' = (2,2,0)$, $\mu' = (1,1,0)$, and $\mu'' = (2,1,1)$. The matrix under consideration is therefore
\[
X = \left[\begin{BMAT}(e){ccc;cc;c}{ccc;cc;c}
    0 & 1 & & & & \\
     & 0 & 1 & & & \\
     & -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 \\
     & & & 0 & 1 & \\
     & & & & s & A_{23}^1 \\
     & & & & & s
\end{BMAT}
\right].
\]
There are no relations from looking at the submatrix $X_1$. From the submatrix $X_2$, we require $$A_{12}^1 + sA_{12}^2 = 0.$$
From $X$, we further obtain the relations $$A_{12}^2A_{23}^1 + sA_{13}^1 = 0, A_{12}^1A_{23}^1 + s^2A_{13}^1 = 0.$$
Then the quotient ring we find is
$$\frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s]}{\langle A_{12}^1+sA_{12}^2, A_{12}^2A_{23}^1+sA_{13}^1, A_{12}^1A_{23}^1+s^2A_{13}^1, s\rangle} \cong \frac{\CC[A_{12}^1,A_{12}^2,A_{13}^1,b_3,s]}{\langle A_{12}^1, A_{12}^2\rangle \cap \langle A_{12}^1, A_{23}^1 \rangle}.$$
We see that the 0-fibre is reducible with two components. The tableau corresponding to each ideal is
$$\begin{array}{cccc} \vspace{1mm}
    \young(1113,22) & \text{for } \langle A_{12}^1, A_{12}^2 \rangle & \text{Lusztig: }(101) & \dim = (1,0,-1)\\ 
    \young(1112,23) & \text{for } \langle A_{12}^1, A_{23}^1 \rangle & \text{Lusztig: }(010) & \dim = (1,0,-1).
\end{array}$$
Each of these tableaux corresponds to a MV cycle isomorphic to $\PP^2$ so the generic fibre $\PP^1 \times \PP^1$ degenerates to two copies of $\PP^2$.
\end{example}

\begin{comment}
\begin{example}
Let $G = SL_3$ and consider the following tableaux:
$$\tau' = \young(1122) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(1111,2233).$$

Lusztig data: $(200)$ for $\tau'$, $(002)$ for $\tau''$.

Dimensions: $(2,-2,0)$ for $\tau'$, $(0,2,-2)$ for $\tau''$.

The MV cycles corresponding to these tableaux are each isomorphic to $\PP^1 \times \PP^1$. 
The weights are then $\lambda' = (4,0,0)$, $\lambda'' = (4,4,0)$, $\mu' = (2,2,0)$, and $\mu'' = (4,2,2)$.
Then we are considering the following matrix:
\[
X = \left[\begin{BMAT}(e){cccccc;cccc;cc}{cccccc;cccc;cc}
    0 & 1 & & & & & & & & & & \\
     & 0 & 1 & & & & & & & & & \\
     & & 0 & 1 & & & & & & & & \\
     & & & 0 & 1 & & & & & & & \\
     & & & & 0 & 1 & & & & & & \\
     & & -s^4 & 3s^3 & -6s^2 & 4s & A_{12}^1 & A_{12}^2 & A_{12}^3 & A_{12}^4 & A_{13}^1 & A_{13}^2 \\
     & & & & & & 0 & 1 & & & & \\
     & & & & & & & 0 & 1 & & & \\
     & & & & & & & & 0 & 1 & & \\
     & & & & & & & & -s^2 & 2s & A_{23}^1 & A_{23}^2 \\
     & & & & & & & & & & 0 & 1 \\
     & & & & & & & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
By computer, it can be checked that we obtain the following 3 ideals:
$$
\begin{array}{cl}
    I_1 = & \langle A_{12}^1, A_{12}^2, A_{12}^3, A_{12}^4, s \rangle \\
    I_2 = & \langle A_{12}^1, A_{12}^2, A_{23}^1, A_{23}^2, s \rangle \\
    I_3 = & \langle A_{12}^1, A_{12}^2, (A_{23}^1)^2, A_{12}^4A_{23}^1 + A_{12}^3A_{23}^2, A_{12}^3A_{23}^1, (A_{12}^3)^2, s \rangle.
\end{array}
$$
It can be checked that $I_3$ has multiplicity 2 \rcom{Not sure how to make this precise and how to check it.} so the 0-fibre has 4 components, two of which are the same. The tableau corresponding to each ideal is
$$
\begin{array}{cccc}\vspace{1mm}
    \young(11111133,2222) & \text{for } I_1 & \text{Lusztig: }(202) & \dim = (2,0,-2) \\ \vspace{1mm}
    \young(11111122,2233) & \text{for } I_2 & \text{Lusztig: }(020) & \dim = (2,0,-2) \\ \vspace{1mm}
    \young(11111123,2223) & \text{for } I_3 & \text{Lusztig: }(111) & \dim = (2,0,-2).
\end{array}
$$
The MV cycle for each ideal is isomorphic to $\PP^2 \times \PP^2$.
\end{example}
\end{comment}

\begin{example}
    
    \acom{Here the fission is the smallest if we set out to minimize the dimension of the zero eigenspace. For Joel: this $\lambda(n_\bullet)$ again matches that of \cite{mirkovic2007quiver}.}

Let $G = \GL_3$. Consider the tableaux
$$\tau' = \young(22) \hspace{.5cm} \text{ and } \hspace{.5cm} \tau'' = \young(11,33).$$
Then the weights are $\lambda' = (2,0,0)$, $\lambda'' = (2,2,0)$, $\mu' = (0,2,0)$, and $\mu'' = (2,0,2)$. Our matrix $X$ is now
\[
X = \left[\begin{BMAT}(e){cc;cc;cc}{cc;cc;cc}
    0 & 1 & & & & \\
    -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 \\
     & & 0 & 1 & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 \\
     & & & & 0 & 1 \\
     & & & & -s^2 & 2s
\end{BMAT}
\right].
\]
Using a computer, the ideals we get are
$$
\begin{array}{cl}
    I_1 = & \langle A_{12}^1, A_{12}^2 \rangle \\
    I_2 = & \langle A_{23}^1, A_{23}^2 \rangle \\
    I_3 = & \langle (A_{23}^1)^2, A_{12}^2A_{23}^1 + A_{12}^1A_{23}^2, A_{12}^1A_{23}^1, (A_{12}^1)^2 \rangle. 
\end{array}
$$
The tableaux corresponding to each ideal is
$$
\begin{array}{cc}\vspace{1mm}
    \young(1133,22) & \text{for } I_1 \\ \vspace{1mm}
    \young(1122,33) & \text{for } I_2 \\ \vspace{1mm}
    \young(1123,23) & \text{for } I_3
\end{array}
$$
and the MV cycles corresponding to each are isomorphic to $\PP^2 \times \PP^2$.
\end{example}

Let $G = \GL_4$. There is a cluster structure on $\CC[N]$ consisting of 12 cluster variables. We list the corresponding MV cycles and tableaux:
\[\begin{array}{cccccc} \vspace{1mm}
    Z_1 &\leadsto \young(2) & Z_2 &\leadsto \young(1,3) & Z_3 &\leadsto \young(1,2,4) \\ \vspace{1mm}
    Z_{1 \leftarrow 2} &\leadsto \young(3) & Z_{1 \rightarrow 2} &\leadsto \young(2,3) & Z_{2 \leftarrow 3} &\leadsto \young(1,4) \\ \vspace{1mm}
     Z_{2 \rightarrow 3} &\leadsto \young(1,3,4) & Z_{in} &\leadsto \young(2,4) & Z_{out} &\leadsto \young(13,2,4)  \\
    Z_{P_1} &\leadsto \young(2,3,4) & Z_{P_2} &\leadsto \young(3,4) & Z_{P_3} &\leadsto \young(4)
\end{array}\]

We show that the cluster relations in $\CC[N]$ are also true for the fusion product of the corresponding MV cycles.

For each of the 15 exchange relations, we state the associated matrix $X$, the relations acquired from each submatrix $X_i$, the ideal generated by said relations, and the corresponding tableaux from the ideal.

\begin{example}
Consider the product $Z_1 * Z_2$, so the tableaux are
\[
\young(2) \text{ and } \young(1,3).
\]
Our matrix is
\[
X = \left[\begin{BMAT}(e){c;c;c}{c;c;c}
    s & A_{12}^1 & A_{13}^1 \\
     & 0 & A_{23}^1 \\
     & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1+sA_{13}^1
\end{array}.
\]
Hence our ideal is
$$\langle A_{12}^1A_{23}^1+sA_{13}^1,s \rangle = 
\langle A_{12}^1,s \rangle \cap \langle A_{23}^1,s \rangle.$$
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,s \rangle &\leadsto \young(13,2) &\equiv \young(3) &\leadsto {Z_{1 \leftarrow 2}} \\ 
    \langle A_{23}^1,s \rangle &\leadsto \young(12,3) &\equiv \young(2,3) &\leadsto{Z_{1 \rightarrow 2}}
\end{array}
\]
so $Z_1 * Z_2 = Z_{1 \leftarrow 2} + Z_{1 \rightarrow 2}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_3$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(1,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;c;c;c}{cc;c;c;c}
    0 & 1 & & & \\
     & s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & & s & A_{23}^1 & A_{24}^1 \\
     & & & 0 & A_{34}^1 \\
     & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{13}^1 \\
    X_4 & A_{14}^1, A_{23}^1A_{34}^1 + sA_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{14}^1,s \rangle \cap \langle A_{12}^1,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{14}^1,s \rangle &\leadsto \young(11,24,3) &\equiv \young(1,4) &\leadsto Z_{2 \leftarrow 3} \\ 
    \langle A_{12}^1,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(11,23,4) &\equiv \young(1,3,4) &\leadsto Z_{2 \rightarrow 3}
\end{array}
\]
so $Z_2 * Z_3 = Z_{2 \leftarrow 3} + Z_{2 \rightarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_1 * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1 + sA_{13}^1 \\
    X_4 & A_{34}^1, A_{12}^1A_{24}^1 + sA_{14}^1, A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{23}^1,A_{24}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(12,3,4) &\equiv \young(2,3,4) &\leadsto Z_{P_1} \\ 
    \langle A_{12}^1,A_{34}^1,A_{23}^1A_{14}^1 - A_{13}^1A_{24}^1,s \rangle &\leadsto \young(13,2,4) & &\leadsto Z_{out}
\end{array}
\]
so $Z_1 * Z_{2 \rightarrow 3} = Z_{P_1} + Z_{out}$.
\end{example}

\begin{example}
Consider the product $Z_1 * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(2) \text{ and } \young(1,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(2) \text{ and } \young(11,24,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;c;c}{cc;cc;c;c}
    0 & 1 & & & & \\
    -s^2 & 2s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
     & & 0 & 1 & & \\
     & & & s & A_{23}^1 & A_{24}^1 \\
     & & & & s & A_{34}^1 \\
     & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 + sA_{12}^2 \\
    X_3 & A_{13}^1, A_{23}^1 \\
    X_4 & A_{12}^2A_{24}^1 + sA_{14}^1, A_{12}^1A_{24}^1 - s^2A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s \rangle \cap \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,s \rangle &\leadsto \young(114,22,3) &\equiv \young(4) &\leadsto Z_{P_3} \\ 
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle &\leadsto \young(112,24,3) &\equiv \young(2,4) &\leadsto Z_{in}
\end{array}
\]
so $Z_1 * Z_{2 \leftarrow 3} = Z_{P_3} + Z_{in}$.
\end{example}

\begin{example}
Consider the product $Z_3 * Z_{1 \rightarrow 2}$, so the tableaux are 
\[
\young(1,2,4) \text{ and } \young(2,3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(1,2,4) \text{ and } \young(12,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;c;c}{cc;cc;c;c}
    0 & 1 & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{14}^1 \\
     & & 0 & 1 & & \\
     & & & s & A_{23}^1 & A_{24}^1 \\
     & & & & s & A_{34}^1 \\
     & & & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1 \\
    X_4 & A_{24}^1, A_{13}^1A_{34}^1 - sA_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{24}^1,s \rangle &\leadsto \young(112,24,3) &\equiv \young(1,4) &\leadsto Z_{in} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(112,23,4) &\equiv \young(1,3,4) &\leadsto Z_{P_1}
\end{array}
\]
so $Z_3 * Z_{1 \rightarrow 2} = Z_{in} + Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_3 * Z_{1 \leftarrow 2}$, so the tableaux are 
\[
\young(1,2,4) \text{ and } \young(3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    0 & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_4 & A_{23}^1A_{34}^1 - sA_{24}^1, A_{13}^1A_{34}^1 - sA_{14}^1, A_{13}^1A_{24}^1 - A_{23}^1A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,A_{13}^1A_{24}^1-A_{23}^1A_{14}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,s \rangle &\leadsto \young(14,2,3) &\equiv \young(4) &\leadsto Z_{P_3} \\ 
    \langle A_{12}^1,A_{34}^1,A_{13}^1A_{24}^1-A_{23}^1A_{14}^1,s \rangle &\leadsto \young(13,2,4) & &\leadsto Z_{out}
\end{array}
\]
so $Z_3 * Z_{1 \leftarrow 2} = Z_{P_3} + Z_{out}$.
\end{example}

\begin{example}
Consider the product $Z_{1 \leftarrow 2} * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(3) \text{ and } \young(1,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(13,2) \text{ and } \young(1,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;c;c;c}{cc;c;c;c}
    0 & 1 & & & \\
     & s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & & 0 & A_{23}^1 & A_{24}^1 \\
     & & & 0 & A_{34}^1 \\
     & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_4 & A_{13}^1A_{34}^1 + sA_{14}^1
\end{array}.
\]
Hence our ideal is 
\[
\langle A_{12}^1,A_{13}^1A_{34}^1+sA_{14}^1,s \rangle
= \langle A_{12}^1,A_{13}^1,s \rangle \cap \langle A_{12}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,s \rangle &\leadsto \young(114,23) &\equiv \young(14,3) &\leadsto Z_2 Z_{P_3} \\ 
    \langle A_{12}^1,A_{34}^1,s \rangle &\leadsto \young(113,24) &\equiv \young(3,4) &\leadsto Z_{P_2}
\end{array}
\]
so $Z_{1 \leftarrow 2} * Z_{2 \leftarrow 3} = Z_2 Z_{P_3} + Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{1 \rightarrow 2} * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2,3) \text{ and } \young(1,3,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(12,23) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;c}{cc;cc;cc;c}
    0 & 1 & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & 0 & 1 & & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & 0 & 1 & \\
     & & & & & s & A_{34}^1 \\
     & & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1, A_{12}^2A_{23}^2 + A_{13}^1 + sA_{13}^2 \\
    X_4 & A_{34}^1, A_{12}^2A_{24}^1 + sA_{14}^1, sA_{23}^2A_{14}^1 - sA_{13}^2A_{24}^1 - A_{13}^1A_{24}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,A_{12}^2A_{23}^2+A_{13}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(113,224,3) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,A_{12}^2A_{23}^2+A_{13}^1,s \rangle &\leadsto \young(112,233,4) &\equiv \young(12,33,4) &\leadsto Z_2 Z_{P_1}
\end{array}
\]
so $Z_{1 \rightarrow 2} * Z_{2 \rightarrow 3} = Z_{P_2} + Z_2 Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_{out}$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(11,23) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;cc;cc;c}{ccc;cc;cc;c}
    0 & 1 & & & & & & \\
     & 0 & 1 & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & & 0 & 1 & & & \\
     & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & & 0 & 1 & \\
     & & & & & & s & A_{34}^1 \\
     & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1, A_{12}^2 \\
    X_3 & A_{13}^1 \\
    X_4 & A_{34}^1, A_{23}^1A_{14}^1 + s(A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1) 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(1113,224,3) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(1113,223,4) &\equiv \young(1,3,4) &\leadsto Z_{1 \leftarrow 2} Z_{2 \rightarrow 3}
\end{array}
\]
so $Z_2 * Z_{out} = Z_{P_2} + Z_{1 \leftarrow 2} Z_{2 \rightarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_2 * Z_{in}$, so the tableaux are 
\[
\young(1,3) \text{ and } \young(2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
    0 & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & s & A_{23}^1 & A_{24}^1 \\
     & & 0 & A_{34}^1 \\
     & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_3 & A_{12}^1A_{23}^1 - sA_{13}^1 \\
    X_4 & A_{23}^1A_{34}^1 + sA_{24}^1, A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{34}^1,s \rangle \cap \langle A_{23}^1,A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{34}^1,s \rangle &\leadsto \young(13,24) &\equiv \young(3,4) &\leadsto Z_{P_2} \\ 
    \langle A_{23}^1,A_{12}^1A_{24}^1 + A_{13}^1A_{34}^1,s \rangle &\leadsto \young(12,34) & &\leadsto Z_{1 \rightarrow 2} Z_{2 \leftarrow 3}
\end{array}
\]
so $Z_2 * Z_{in} = Z_{P_2} + Z_{1 \rightarrow 2} Z_{2 \leftarrow 3}$.
\end{example}

\begin{example}
Consider the product $Z_{out} * Z_{1 \rightarrow 2}$, so the tableaux are 
\[
\young(13,2,4) \text{ and } \young(2,3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(13,2,4) \text{ and } \young(12,3).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;c}{cc;cc;cc;c}
    0 & 1 & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 \\
     & & 0 & 1 & & & \\
     & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 \\
     & & & & 0 & 1 & \\
     & & & & & s & A_{34}^1 \\
     & & & & & & 0
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{23}^1 +sA_{23}^2 \\
    X_4 & A_{34}^1, A_{13}^1A_{24}^1 - A_{23}^1A_{14}^1 
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle \cap \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{13}^1,A_{23}^1,A_{34}^1,s \rangle &\leadsto \young(1112,24,3) &\equiv \young(23,4) &\leadsto Z_1 Z_{P_2} \\ 
    \langle A_{12}^1,A_{23}^1,A_{24}^1,A_{34}^1,s \rangle &\leadsto \young(1123,23,4) &\equiv \young(23,3,4) &\leadsto Z_{1 \leftarrow 2} Z_{P_1}
\end{array}
\]
so $Z_{out} * Z_{1 \rightarrow 2} = Z_1 Z_{P_2} + Z_{1 \leftarrow 2} Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{1 \leftarrow 2}$, so the tableaux are
\[
\young(2,4) \text{ and } \young(3).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(2,4) \text{ and } \young(13).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){c;c;c;c}{c;c;c;c}
     s & A_{12}^1 & A_{13}^1 & A_{14}^1 \\
     & 0 & A_{23}^1 & A_{24}^1 \\
     & & s & A_{34}^1 \\
     & & & 0
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_4 & A_{23}^1A_{34}^1 - sA_{24}^1
\end{array}.
\]
Hence our ideal is 
\[
\langle A_{23}^1A_{34}^1 - sA_{24}^1,s \rangle
= \langle A_{23}^1,s \rangle \cap \langle A_{34}^1,s \rangle.
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{23}^1,s \rangle &\leadsto \young(124,3) &\equiv \young(24,3) &\leadsto Z_{1 \rightarrow 2} Z_{P_3} \\ 
    \langle A_{34}^1,s \rangle &\leadsto \young(123,4) &\equiv \young(23,4) &\leadsto Z_1 Z_{P_2}
\end{array}
\]
so $Z_{in} * Z_{1 \leftarrow 2} = Z_{1 \rightarrow 2} Z_{P_3} + Z_1 Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{out} * Z_{2 \leftarrow 3}$, so the tableaux are 
\[
\young(1,4) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(11,24,3) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;cc;cc;cc}{ccc;cc;cc;cc}
    0 & 1 & & & & & & & \\
     & 0 & 1 & & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & & 0 & 1 & & & & \\
     & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & & 0 & 1 & & \\
     & & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & & 0 & 1 \\
     & & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1,A_{12}^2 \\
    X_3 & A_{13}^1, A_{23}^1 \\
    X_4 & \begin{array}{c}
         A_{13}^2A_{34}^1 - sA_{14}^1, A_{34}^1 + sA_{34}^2, A_{13}^2A_{34}^2 + A_{14}^1,  \\
         A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1 + s(A_{13}^2A_{24}^2 - A_{23}^2A_{14}^2), \\ 
         A_{14}^2A_{23}^2A_{34}^1 - A_{14}^1A_{23}^2A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^1A_{24}^2
    \end{array}
\end{array}.
\]
Hence our ideal decomposes into 
\[
\langle A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{14}^1,A_{34}^1,s \rangle \cap \begin{array}{c}
     \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2+A_{14}^1,A_{13}^2A_{24}^1-A_{23}^2A_{14}^1,s \rangle. 
\end{array}
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{13}^2,A_{23}^1,A_{14}^1,A_{34}^1,s \rangle &\leadsto \young(1114,223,34) &\equiv \young(14,3,4) &\leadsto Z_{2 \rightarrow 3} Z_{P_3} \\ 
    \begin{array}{c}
     \langle A_{12}^1,A_{12}^2,A_{13}^1,A_{23}^1,A_{34}^1,A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2+A_{14}^1,A_{13}^2A_{24}^1-A_{23}^2A_{14}^1,s \rangle. 
\end{array}&\leadsto \young(1113,224,34) &\equiv \young(13,24,4) &\leadsto Z_3 Z_{P_2}
\end{array}
\]
so $Z_{out} * Z_{2 \leftarrow 3} = Z_{2 \rightarrow 3} Z_{P_3} + Z_3 Z_{P_2}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{2 \rightarrow 3}$, so the tableaux are 
\[
\young(2,4) \text{ and } \young(1,3,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(12,24,3) \text{ and } \young(1,3,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){cc;cc;cc;cc}{cc;cc;cc;cc}
    0 & 1 & & & & & & \\
     & s & A_{12}^1 & A_{12}^2 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & 0 & 1 & & & & \\
     & & & 0 & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & 0 & 1 & & \\
     & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & 0 & 1 \\
     & & & & & & & s
\end{BMAT}
\right].
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1 \\
    X_3 & A_{13}^1, A_{23}^1, A_{12}^2A_{23}^2 + sA_{13}^2 \\
    X_4 & \begin{array}{c}
         A_{23}^2A_{34}^1 - sA_{24}^1, A_{12}^2A_{24}^1 + A_{13}^2A_{34}^1, A_{34}^1 + sA_{34}^2, A_{23}^2A_{34}^2 + A_{24}^1, \\
         A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1 + sA_{14}^2,  \\
         A_{23}^2A_{14}^1 - A_{13}^2A_{24}^1 + s(A_{23}^2A_{14}^2 - A_{13}^2A_{24}^2), \\
         A_{13}^2A_{24}^2A_{34}^1 - A_{13}^2A_{24}^1A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^2A_{24}^1 
    \end{array} 
\end{array}.
\]
Hence our ideal decomposes into 
\[\begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1,s \rangle  
\end{array}
 \cap \begin{array}{c}
      \langle A_{12}^1, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1,  \\
      A_{34}^1, A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1,s \rangle. 
 \end{array} 
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1,s \rangle  
\end{array} &\leadsto \young(113,224,34) &\equiv \young(13,24,4) &\leadsto Z_3 Z_{P_2} \\ 
    \begin{array}{c}
      \langle A_{12}^1, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1,  \\
      A_{34}^1, A_{12}^2A_{24}^2 + A_{13}^2A_{34}^2 + A_{14}^1,s \rangle. 
 \end{array} &\leadsto \young(112,234,34) &\equiv \young(12,34,4) &\leadsto Z_{2 \leftarrow 3} Z_{P_1}
\end{array}
\]
so $Z_{in} * Z_{2 \rightarrow 3} = Z_3 Z_{P_2} + Z_{2 \leftarrow 3} Z_{P_1}$.
\end{example}

\begin{example}
Consider the product $Z_{in} * Z_{out}$, so the tableaux are 
\[
\young(2,4) \text{ and } \young(13,2,4).
\]
As the sum of the weights is not dominant, we will instead be using the tableaux
\[
\young(112,24,3) \text{ and } \young(13,2,4).
\]
Our matrix is 
\[
X = \left[\begin{BMAT}(e){ccc;ccc;cc;cc}{ccc;ccc;cc;cc}
    0 & 1 & & & & & & & & \\
     & 0 & 1 & & & & & & & \\
     & & s & A_{12}^1 & A_{12}^2 & A_{12}^3 & A_{13}^1 & A_{13}^2 & A_{14}^1 & A_{14}^2 \\
     & & & 0 & 1 & & & & & \\
     & & & & 0 & 1 & & & & \\
     & & & & & s & A_{23}^1 & A_{23}^2 & A_{24}^1 & A_{24}^2 \\
     & & & & & & 0 & 1 & & \\
     & & & & & & & s & A_{34}^1 & A_{34}^2 \\
     & & & & & & & & 0 & 1 \\
     & & & & & & & & & s
\end{BMAT}
\right]
\]
The relations from the submatrices are:
\[
\begin{array}{c|c}
    \text{Submatrix} & \text{Relations} \\ \hline
    X_2 & A_{12}^1, A_{12}^2+sA_{12}^3 \\
    X_3 & A_{13}^1,A_{23}^1 \\
    X_4 & \begin{array}{c}
         A_{23}^2A_{34}^1 - sA_{24}^1, A_{34}^1 + sA_{34}^2, A_{23}^2A_{34}^2 + A_{24}^1, \\
         A_{12}^3A_{34}^1 - A_{12}^2A_{34}^2, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1 + s(A_{13}^2A_{24}^2 - A_{23}^2A_{14}^2), \\
         A_{13}^2A_{24}^2A_{34}^1 - A_{13}^2A_{24}^1A_{34}^2 - A_{14}^1A_{24}^1 - sA_{14}^2A_{24}^1, \\
         A_{12}^3A_{23}^2A_{14}^1 - A_{12}^2A_{23}^2A_{14}^2 - A_{12}^3A_{13}^2A_{24}^1 + A_{12}^2A_{13}^2A_{24}^2,  \\
         A_{12}^3A_{13}^2A_{24}^1A_{34}^2 - A_{12}^2A_{13}^2A_{24}^2A_{34}^2 + A_{12}^3A_{14}^1A_{24}^1 - A_{12}^2A_{14}^2A_{24}^1
    \end{array} 
\end{array}.
\]
Hence our ideal decomposes into \acom{formatting funny; can just list irrecs?}
\[
\langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1, A_{34}^1,s \rangle
\cap \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1, s \rangle. 
\end{array} 
\]
The tableau for each ideal is
\[\begin{array}{cccc}\vspace{1mm}
    \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{23}^2, A_{24}^1, A_{34}^1,s \rangle &\leadsto \young(11124,223,34) &\equiv \young(24,3,4) &\leadsto Z_{P_1} Z_{P_3} \\ 
    \begin{array}{c}
     \langle A_{12}^1, A_{12}^2, A_{13}^1, A_{23}^1, A_{34}^1, A_{23}^2A_{34}^2 + A_{24}^1,  \\
     A_{13}^2A_{34}^2 + A_{14}^1, A_{13}^2A_{24}^1 - A_{23}^2A_{14}^1, s \rangle. 
\end{array} &\leadsto \young(11123,224,34) &\equiv \young(123,24,4) &\leadsto Z_1 Z_3 Z_{P_2}
\end{array}
\]
so $Z_{in} * Z_{out} = Z_{P_1} Z_{P_3} + Z_1 Z_3 Z_{P_2}$.
\end{example}

\bibliographystyle{alpha}
\bibliography{mvybd}

\end{document}